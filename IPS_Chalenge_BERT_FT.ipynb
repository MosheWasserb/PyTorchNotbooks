{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IPS Chalenge - BERT FT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MosheWasserb/PyTorchNotbooks/blob/master/IPS_Chalenge_BERT_FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To9ENLU90WGl",
        "colab_type": "code",
        "outputId": "bd7b32fb-34ee-45f6-953d-31c3aa46837a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 52.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=95feb5e208a1ac5a2289d1b959eda8f71b0aee5f46eca54671fde74f46658d70\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvFvBLJV0Dkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from IPython.display import clear_output\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNdAPjA3-exk",
        "colab_type": "text"
      },
      "source": [
        "Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyoj29J24hPX",
        "colab_type": "code",
        "outputId": "c2b21a91-be79-4bc7-9b04-990281430bc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "### read data from your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3GuB5cHv149",
        "colab_type": "code",
        "outputId": "8506b484-ee43-4fa5-a169-6ae0382cd653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "# Import Training, Validation and Test csvs\n",
        "# report1585718073300_v6_test.csv, report1585718073300_v6_train.csv, report1585718073300_v6_val.csv\n",
        "data_training = pd.read_csv(r\"/gdrive/My Drive/IL/report1585718073300_v6_train.csv\")\n",
        "data_test = pd.read_csv(r\"/gdrive/My Drive/IL/report1585718073300_v6_test.csv\")\n",
        "\n",
        "text_col=data_training.columns.values[2] \n",
        "category_col=data_training.columns.values[3]\n",
        "\n",
        "# Filter data with NaN in Case Description or Case Category rows\n",
        "data_training= data_training[data_training[category_col].notnull() & data_training[text_col].notnull()]\n",
        "data_test= data_test[data_test[category_col].notnull() & data_test[text_col].notnull()]\n",
        "\n",
        "data_training=data_training[~data_training[category_col].str.match('Technology/Initiative') & ~data_training[category_col].str.match('Power/Thermal/Mechanical')]\n",
        "data_test=data_test[~data_test[category_col].str.match('Technology/Initiative') & ~data_test[category_col].str.match('Power/Thermal/Mechanical')]\n",
        "\n",
        "data_test[category_col].value_counts().plot.bar()\n",
        "data_test.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>”Id\"</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Description</th>\n",
              "      <th>Category</th>\n",
              "      <th>Subcategory</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NTAwMUowMDAwMGFCeXhl</td>\n",
              "      <td>PMIC workaround not performed and PMIC ID reco...</td>\n",
              "      <td>Hi , The PMIC ID recognized wrong during the B...</td>\n",
              "      <td>Firmware/BIOS</td>\n",
              "      <td>BIOS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NTAwMUowMDAwMFNzNGc1</td>\n",
              "      <td>Bluetooth test fails with HLK of Windows 10 RS 2</td>\n",
              "      <td>The following Bluetooth test fails with HLK of...</td>\n",
              "      <td>Networking/Connectivity</td>\n",
              "      <td>Bluetooth (BT)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NTAwMUowMDAwMFRXRUpV</td>\n",
              "      <td>[ FTS ] During Windows reboot tests the system...</td>\n",
              "      <td>During Windows reboot tests the system with VM...</td>\n",
              "      <td>Software/Driver/OS</td>\n",
              "      <td>Tool</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NTAwMUowMDAwMGNna0F0</td>\n",
              "      <td>Avalon arbitration using multiple SG - DMA</td>\n",
              "      <td>Error Msg : I am attempting to run a Nios II s...</td>\n",
              "      <td>DesignTools</td>\n",
              "      <td>Qsys System Design</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NTAwMUowMDAwMGtEblFB</td>\n",
              "      <td>Is the WAV 654 support SKY 85755 + SKY 85337 ?</td>\n",
              "      <td>Hi Intel , Can we use SKY 85755 and SKY 85337 ...</td>\n",
              "      <td>Hardware/Platform</td>\n",
              "      <td>Design Review</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   ”Id\"  ...         Subcategory\n",
              "0  NTAwMUowMDAwMGFCeXhl  ...                BIOS\n",
              "1  NTAwMUowMDAwMFNzNGc1  ...      Bluetooth (BT)\n",
              "2  NTAwMUowMDAwMFRXRUpV  ...                Tool\n",
              "3  NTAwMUowMDAwMGNna0F0  ...  Qsys System Design\n",
              "4  NTAwMUowMDAwMGtEblFB  ...       Design Review\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAFzCAYAAADc9mULAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydeZgcVdWH3x8JO4REyaesJiCiiIAQNkFlEQg7+IGAAhFREFFR3MANBVEWFQEFjQICIpssoqyRHWRLWAJhkciiAZUoWz5R1vP9cW5lajo9k6lbNZmmc97n6We6bleduT3TXefes8rMCIIgCOZvFhjqCQRBEARDTyiDIAiCIJRBEARBEMogCIIgIJRBEARBAAwf6gnksvTSS9uYMWOGehpBEASvK6ZMmfJPMxvdOv66VQZjxoxh8uTJQz2NIAiC1xWSHm83PlczkaRTJT0l6b7S2LGSHpQ0VdJFkkaWXjtU0nRJD0naqjQ+Po1Nl3RIaXyspNvS+LmSFsp/m0EQBEEOA/EZ/BIY3zI2CVjdzNYA/gQcCiBpNWB34J3pmpMkDZM0DPgJsDWwGrBHOhfgaOA4M3sr8Aywb613FARBEFRmrsrAzG4Anm4Zu8rMXkmHtwLLp+c7AueY2Ytm9igwHVgvPaab2SNm9hJwDrCjJAGbAb9J158O7FTzPQVBEAQVaSKa6GPA5en5csBfS6/NSGN9jb8ReLakWIrxtkjaT9JkSZNnzpzZwNSDIAgCqKkMJH0NeAU4q5np9I+ZTTSzcWY2bvToOZzhQRAEQSbZ0USSPgpsB2xuPdXungBWKJ22fBqjj/F/ASMlDU+7g/L5QRAEwTwia2cgaTzwZWAHM3uh9NIlwO6SFpY0FlgFuB24A1glRQ4thDuZL0lK5Fpgl3T9BOC3eW8lCIIgyGUgoaVnA7cAq0qaIWlf4MfAksAkSXdL+imAmU0DzgPuB64ADjSzV9Oq/9PAlcADwHnpXICvAAdLmo77EE5p9B0GQRAEc0Wv134G48aNs0g6C4IgqIakKWY2rnX8dZuB3I4xh1w613MeO2rbeTCTIAiC1xdRqC4IgiAIZRAEQRCEMgiCIAgIZRAEQRAQyiAIgiAglEEQBEFAKIMgCIKAUAZBEAQBoQyCIAgCQhkEQRAEhDIIgiAICGUQBEEQEMogCIIgIJRBEARBQCiDIAiCgFAGQRAEAaEMgiAIAkIZBEEQBIQyCIIgCAhlEARBEADDh3oCncqYQy4d0HmPHbXtIM8kCIJg8ImdQRAEQRDKIAiCIAhlEARBEBDKIAiCICCUQRAEQcAAlIGkUyU9Jem+0tgbJE2S9HD6OSqNS9IJkqZLmipp7dI1E9L5D0uaUBpfR9K96ZoTJKnpNxkEQRD0z0B2Br8ExreMHQJcbWarAFenY4CtgVXSYz/gZHDlARwGrA+sBxxWKJB0zidK17X+riAIgmCQmasyMLMbgKdbhncETk/PTwd2Ko2fYc6twEhJywBbAZPM7GkzewaYBIxPr40ws1vNzIAzSrKCIAiCeUSuz+BNZva39PzvwJvS8+WAv5bOm5HG+huf0Wa8LZL2kzRZ0uSZM2dmTj0IgiBopbYDOa3orYG5DOR3TTSzcWY2bvTo0fPiVwZBEMwX5CqDfyQTD+nnU2n8CWCF0nnLp7H+xpdvMx4EQRDMQ3KVwSVAERE0AfhtaXzvFFW0AfBcMiddCWwpaVRyHG8JXJlee17SBimKaO+SrCAIgmAeMddCdZLOBjYBlpY0A48KOgo4T9K+wOPAh9LplwHbANOBF4B9AMzsaUlHAHek8w43s8Ip/Sk8YmlR4PL0CIIgCOYhc1UGZrZHHy9t3uZcAw7sQ86pwKltxicDq89tHkEQBMHgERnIQRAEQSiDIAiCIJRBEARBQCiDIAiCgFAGQRAEAaEMgiAIAkIZBEEQBIQyCIIgCAhlEARBEBDKIAiCICCUQRAEQUAogyAIgoBQBkEQBAGhDIIgCAJCGQRBEASEMgiCIAgIZRAEQRAQyiAIgiAglEEQBEFAKIMgCIIAGD7UE5hfGHPIpQM677Gjth3kmQRBEMxJ7AyCIAiCUAZBEARBKIMgCIKAUAZBEAQBoQyCIAgCaioDSZ+XNE3SfZLOlrSIpLGSbpM0XdK5khZK5y6cjqen18eU5Byaxh+StFW9txQEQRBUJVsZSFoO+CwwzsxWB4YBuwNHA8eZ2VuBZ4B90yX7As+k8ePSeUhaLV33TmA8cJKkYbnzCoIgCKpT10w0HFhU0nBgMeBvwGbAb9LrpwM7pec7pmPS65tLUho/x8xeNLNHgenAejXnFQRBEFQgWxmY2RPA94G/4ErgOWAK8KyZvZJOmwEsl54vB/w1XftKOv+N5fE21/RC0n6SJkuaPHPmzNypB0EQBC3UMRONwlf1Y4FlgcVxM8+gYWYTzWycmY0bPXr0YP6qIAiC+Yo6ZqIPAI+a2Uwzexm4ENgIGJnMRgDLA0+k508AKwCk15cC/lUeb3NNEARBMA+oowz+AmwgabFk+98cuB+4FtglnTMB+G16fkk6Jr1+jZlZGt89RRuNBVYBbq8xryAIgqAi2YXqzOw2Sb8B7gReAe4CJgKXAudI+k4aOyVdcgpwpqTpwNN4BBFmNk3SebgieQU40MxezZ1XEARBUJ1aVUvN7DDgsJbhR2gTDWRm/wV27UPOkcCRdeYSBEEQ5BMZyEEQBEEogyAIgiCUQRAEQUAogyAIgoBQBkEQBAGhDIIgCAJqhpYGQ8eYQy4d0HmPHbXtIM8kCIJuIHYGQRAEQSiDIAiCIJRBEARBQCiDIAiCgHAgB4QzOgiC2BkEQRAEhDIIgiAICGUQBEEQEMogCIIgIJRBEARBQCiDIAiCgFAGQRAEAaEMgiAIAkIZBEEQBIQyCIIgCAhlEARBEBDKIAiCICCUQRAEQUAogyAIgoCaykDSSEm/kfSgpAckbSjpDZImSXo4/RyVzpWkEyRNlzRV0tolORPS+Q9LmlD3TQVBEATVqLszOB64wszeDqwJPAAcAlxtZqsAV6djgK2BVdJjP+BkAElvAA4D1gfWAw4rFEgQBEEwb8hWBpKWAt4HnAJgZi+Z2bPAjsDp6bTTgZ3S8x2BM8y5FRgpaRlgK2CSmT1tZs8Ak4DxufMKgiAIqlNnZzAWmAmcJukuSb+QtDjwJjP7Wzrn78Cb0vPlgL+Wrp+RxvoaD4IgCOYRdZTBcGBt4GQzezfwb3pMQgCYmQFW43f0QtJ+kiZLmjxz5symxAZBEMz31FEGM4AZZnZbOv4Nrhz+kcw/pJ9PpdefAFYoXb98GutrfA7MbKKZjTOzcaNHj64x9SAIgqBMtjIws78Df5W0ahraHLgfuAQoIoImAL9Nzy8B9k5RRRsAzyVz0pXAlpJGJcfxlmksCIIgmEcMr3n9Z4CzJC0EPALsgyuY8yTtCzwOfCidexmwDTAdeCGdi5k9LekI4I503uFm9nTNeQVBEAQVqKUMzOxuYFyblzZvc64BB/Yh51Tg1DpzCYIgCPKJDOQgCIIglEEQBEEQyiAIgiAglEEQBEFAKIMgCIKAUAZBEAQBoQyCIAgCQhkEQRAEhDIIgiAICGUQBEEQEMogCIIgIJRBEARBQCiDIAiCgFAGQRAEAaEMgiAIAkIZBEEQBIQyCIIgCAhlEARBEBDKIAiCICCUQRAEQUAogyAIgoBQBkEQBAGhDIIgCAJCGQRBEASEMgiCIAgIZRAEQRAQyiAIgiCgAWUgaZikuyT9Ph2PlXSbpOmSzpW0UBpfOB1PT6+PKck4NI0/JGmrunMKgiAIqtHEzuAg4IHS8dHAcWb2VuAZYN80vi/wTBo/Lp2HpNWA3YF3AuOBkyQNa2BeQRAEwQCppQwkLQ9sC/wiHQvYDPhNOuV0YKf0fMd0THp983T+jsA5ZvaimT0KTAfWqzOvIAiCoBp1dwY/Ar4MvJaO3wg8a2avpOMZwHLp+XLAXwHS68+l82ePt7mmF5L2kzRZ0uSZM2fWnHoQBEFQkK0MJG0HPGVmUxqcT7+Y2UQzG2dm40aPHj2vfm0QBEHXM7zGtRsBO0jaBlgEGAEcD4yUNDyt/pcHnkjnPwGsAMyQNBxYCvhXabygfE0QBEEwD8jeGZjZoWa2vJmNwR3A15jZR4BrgV3SaROA36bnl6Rj0uvXmJml8d1TtNFYYBXg9tx5BUEQBNWpszPoi68A50j6DnAXcEoaPwU4U9J04GlcgWBm0ySdB9wPvAIcaGavDsK8giAIgj5oRBmY2XXAden5I7SJBjKz/wK79nH9kcCRTcwlCIIgqE5kIAdBEAShDIIgCIJQBkEQBAGhDIIgCAJCGQRBEASEMgiCIAgIZRAEQRAwOElnwXzOmEMuHdB5jx217SDPJAiCgRI7gyAIgiCUQRAEQRDKIAiCICCUQRAEQUAogyAIgoBQBkEQBAGhDIIgCAJCGQRBEASEMgiCIAgIZRAEQRAQyiAIgiAglEEQBEFAKIMgCIKAUAZBEAQBoQyCIAgCQhkEQRAEhDIIgiAICGUQBEEQUEMZSFpB0rWS7pc0TdJBafwNkiZJejj9HJXGJekESdMlTZW0dknWhHT+w5Im1H9bQRAEQRXq7AxeAb5gZqsBGwAHSloNOAS42sxWAa5OxwBbA6ukx37AyeDKAzgMWB9YDzisUCBBEATBvCFbGZjZ38zszvR8FvAAsBywI3B6Ou10YKf0fEfgDHNuBUZKWgbYCphkZk+b2TPAJGB87ryCIAiC6jTiM5A0Bng3cBvwJjP7W3rp78Cb0vPlgL+WLpuRxvoaD4IgCOYRtZWBpCWAC4DPmdnz5dfMzACr+ztKv2s/SZMlTZ45c2ZTYoMgCOZ7hte5WNKCuCI4y8wuTMP/kLSMmf0tmYGeSuNPACuULl8+jT0BbNIyfl2732dmE4GJAOPGjWtMyQSdzZhDLh3QeY8dte0gzyQIupc60UQCTgEeMLMfll66BCgigiYAvy2N752iijYAnkvmpCuBLSWNSo7jLdNYEARBMI+oszPYCNgLuFfS3Wnsq8BRwHmS9gUeBz6UXrsM2AaYDrwA7ANgZk9LOgK4I513uJk9XWNeQRAEQUWylYGZ3QSoj5c3b3O+AQf2IetU4NTcuQRBEAT1iAzkIAiCoJ4DOQheb4QzOgjaEzuDIAiCIJRBEARBEGaiIKhF02anMGMFQ0XsDIIgCIJQBkEQBEEogyAIgoBQBkEQBAGhDIIgCAJCGQRBEASEMgiCIAgIZRAEQRAQyiAIgiAgMpCDoKsZSEZzZEcHEMogCIIhIpRLZxHKIAiCrqDJXdD8SPgMgiAIgtgZBEEQtDI/mrBiZxAEQRCEMgiCIAhCGQRBEASEMgiCIAgIB3IQBMGg83pojxo7gyAIgiCUQRAEQRDKIAiCIKCDlIGk8ZIekjRd0iFDPZ8gCIL5iY5QBpKGAT8BtgZWA/aQtNrQzioIgmD+oSOUAbAeMN3MHjGzl4BzgB2HeE5BEATzDTKzoZ4DknYBxpvZx9PxXsD6ZvbplvP2A/ZLh6sCDw1A/NLAPxuaapOyOl1eJ8+taXmdPLem5XXy3DpdXifPrYq8t5jZ6NbB11WegZlNBCZWuUbSZDMb18Tvb1JWp8vr5Lk1La+T59a0vE6eW6fL6+S5NSGvU8xETwArlI6XT2NBEATBPKBTlMEdwCqSxkpaCNgduGSI5xQEQTDf0BFmIjN7RdKngSuBYcCpZjatIfGVzErzUFany+vkuTUtr5Pn1rS8Tp5bp8vr5LnVltcRDuQgCIJgaOkUM1EQBEEwhIQyCIIgCEIZBEEQBKEMgtcJkg6SNELOKZLulLRlpqwFJL2n6TkOFmm+I4Z6HkF305XKQNJ2ku6S9LSk5yXNkvR8RRmfkLRKei5JpyVZUyWt3cAc3yhpZ0nr1JRzTLpJLijpakkzJe2ZKWsjSYun53tK+qGkt9SY2w8kvTP3+hY+ZmbPA1sCo4C9gKNyBJnZa3gtrNpIOlHSCX09asj9dfq/Lg7cB9wv6UuZssam/+WFki4pHrlzSzLfIukD6fmikpbMlLOTpC9K2qrOfFpkNvadSPIaea+DgaT3SPqwpL2LR66srlQGwI+ACcAbzWyEmS1pZlVXVgcBj6XnewBrAGOBg4Hjq05I0u8lrZ6eL4N/wT8GnCnpc1Xlldgy3SS3S/N9K5B10wBOBl6QtCbwBeDPwBk15vYAMFHSbZI+KWmpGrKUfm4DnJlCj9XP+XPjakn/K6mODIDJwBRgEWBt4OH0WAtYqIbc1dL/dSfgcvyzt1emrIvxz8aJwA9KjywkfQL4DfCzNLR8+h1V5ZwEfB54I3CEpG/kzqmFxr4TDb7XWWkx2fqovFAtyTwT+D6wMbBueuRnNJtZ1z2Aa4EFasq4u/T818BBpeM7M+RNKz3/KnBGer4kMLXGPO9LP3+B13cCuCdT1p3p5zeBfXPfaxu5q+Kr+MfT33LTDBmnAVfhN9rF0t9tSo05zQJeA14Gnk/Hz9eQdyswvHS8IHBrDXnTkozzgffX/L/eVvd/2CLvblzR3VUauzdDzn3AsPR8sTr/z9a/XfrZxHeikfc6GA98saWm5HVE0tkg8GXgMknXAy8Wg2b2wwoyXksr+GeAzYEjS68tmjGnl0vPNwd+nuY0S9JrGfIKfi/pQeA/wAGSRgP/zZQ1S9KhwJ7A+yQtgN+QspGXJ397evwTuAc4WNL+ZrZ7BVH74qvtR8zsBUlvBPbJnZeZNb3VHwWMAJ5Ox0uksVx+hq9q7wFuSOa6rBUkcLykw3BlWv4+3Jkp70Uze6nYVEkaDuQkLL1kZq+mubzQwC6t4HcNficaea+S3tAyZMCzlu7qmdwHvBn4Ww0Zs+lWZXAk8H/41j13q/5N3AQwDLjEUka0pPcDj2TI+6ukzwAzcHPCFUneotS44ZrZIZKOAZ4zs1clvUB++e/dgA/ju4K/S1oRODZ3bpKOw7fq1wDfNbPb00tHSxpIxVna+GdWauKekW48HwHGmtkRklYAlinNsSpHAXdJuhY3X70P+Fbu/MzsBKDsc3hc0qaZ4t6Fm5g2w3dD4DejzTLlXS/pq8CikrYAPgX8LkPO2yVNTc8FrJyOBZiZrZE5v8OA1u/EDpmymnqvU/C/efnDu6Sku4GPm9ljGTKXxn1Jt9NbyWe9167MQJZ0n5mt3oCc4cCSZvZMaWxx/O/2fxVl/Q9wOLAM8BMzuyqNbwqsY2bfz5zjYrgfY0Uz20/u9F7VzH6fI69JJO0DnGdm/27z2lJm9twAZFzbz8tmZlk3NEkn4zfGzczsHZJGAVeZ2bo58pLMNwPrp8PbzOzvNWS9CfgusKyZbS1v9rShmZ2SIWs67oN4KXc+LfIWwHdqW+I3tyuBX1Rd5WouwQlm9njm/O40s7XnNjZAWY28137kfxDYz8zGZ1z7/nbjZnZ91ly6VBkcA/yhuOHWkPM/wIFAEREzDTjJzP5RU+4SAFUVSh+yzsVXHXub2epJOfzRzNaqIGMWvbe+omcVY1bd+V7IvdrMNp/b2FBQ3Bwk3WVm705j95jZmpnyip3GSmZ2eNpVvTl3pyHpctxP8jUzWzMtTO4ys3dlyLoYv+E8lTOXeUUy/b0P+IuZTcm4/s3AcsCv8B1usQofAfzUzN7e1FybJFdRpWvfhDuOAW6v8z/uVjPRAcAXJb2I2+or39QkbYQ7O39JT0TNOsBtkj5iZjdXnZSkA4BDgcX9ULOAo83spKqySqxsZrtJ2gPybK9N288lLYI7BJdOK+7yl3K5TJkL4v/X96Wh64CfmdnLfV7UPy8nf4Yl+aPpMaHkcFK6fjN8BzgLuICeL2pVljaz85IPB/Nijq9myhoJPCjpDpowJ0j3Mqfd/DncrPodM/vXAOX8HjjEzO5L/rk7k4yVJU00sx9VnNpWwEfxiJ+yf3AWHrRRmabeaz/ylyAzqlPSh3Az7nX4d+xESV8ys9/kyOs6ZZC2deNzbtYt/ADYyczuKo1dIuki3Lm3fvvL+pzX14H3AJuY2SNpbCXcufcGM/tO5jxfSn6H4qa2MqUvfFXkYaXvTYc3mNnU/s7vg/2BzwHL4l/wgueBH2dO7WTct1Iozr3S2Mcz5Z0AXAT8j6QjgV2Ar2fKAu/Mt7akuwDM7Bl5OfZc/p1WysX/dQP8JpTDYTXm0Y7LgVfxxRJ4yfnFgL/ji6ftByhnrJndl57vA0wys73lcfw34yHiA8bMTgdOl/S/ZnZBlWv7oZH3KungNsOjcF9G7nfia8C6xW4gLWj+gIfCVqbrlIGZvSbpx8C7a4oa0aIICvl3Ky/pZC9gTTObHdVgZo8k7X4PkKsMDsOd0StIOgvYCF8dVUbSQcAngAvT0FlphXZiFTlmdjyu5D5T9dp+WLfFhHONpHtyhZnZWZKm4JFdwhX/AzXm1/RO42C8p8fKkm4GRuMKqxJpTj9r2ETygRazxr0ls1uV5K5GI+wk7WlmvwLGtLv5VowmLGjqvbbeMwxXKHua2b0Z8wIPny+bhf5FjdyxrlMGiasl/S9wYQ1HjySNKjuP0+AbyPuDW1kRlAb/k/PBL10/SdKdwAb4Te0gM8vtq7ovvsL9N4Cko4Fb8GSlASNpMzO7BngiOcha53xhm8vmxquSVjazP6ffsRK+YqvDw/huZXiSuaKZ/SVTVqM7DTO7MzkIV8X/rw/lmMRSNM1DNd9bK8MkrVf4QySti0fdAbxSQU7TEXaLp59LtHkt9z7Q1Ht9Gbi83QKzBldIuhI4Ox3vBlyWK6xblcH++MrqVUn/Ic8RehxwlaQv0mPqWAc4Or1WlSckbW5mV5cHJW1GjThhSTsD15jZpel4pKSdzKxyliT+dyrfYF8lL8v3/Xg4absttNGz86jCl4BrJT2S5vQWauQZpJvQYcA/6HmfhmeaV6bpnYakA4GzrCekeZSkPTL9S6OAafIQxNmRXbk+A9w0d2qydwtXqB+XR9p9r4KcfXH/ygeA3czs2TS+Ae48r8plAGb27dYXJG2XIQ+ae69/Bg5KZth7cPPTVa2LzSqY2ZfSonejNDTRzC7KldeV0URNkT5AX8ajiQy4HzjWzCrHGctr9PwWuAmP/gFPHd8I2NEyO7tJurs1cqgcIVNR1sF4GY+L8A/+jsAvMxx5hbxhlpKKmkDSwvhKGXylXMc3Mh3fBdV1ALYmE/XCzJ7u7/V+5Db5f200BLEkd6kkJ9eX0SjyRLPx1hKzLw9x/rqZrVxDdmPvVdK7gfF4uOow3M5/RW7kWVN0rTKQtAOlyBOrGHefonOuqnuzaJG5CB7yVoSq3o+v/nKzI5E01VqScyTdmxOCmK5dG691YsBNdba1kv6Cb/3PxXcv2R+20kr52XQ8CshdKRf5C1uYWZWtfjs5j9IThrsinrEuPILnL2Y2NlPuvcAaxd8s2f6nmllW4T81GIKY5G2Lf44XKcbM7PAMOROAz+IZ6uAlFk4ws8o1sSRtgzudtzWzh9PYofh3bmszm1FVZpLRyHvtQ/YIYAtgKzPbb4DX3GRmG6uPkPCKFpDZdKWZSNJR+Af/rDR0kKSNzOzQCmJWBM6XhzRejW/rbq9zQ0s3/VNzr++DyZJ+SE8VzgPp2Xnk8Cr+ATPqOUDBv+DbpTmdIg8lPMfMbsqQ9Qkzm11pNEXrfIKe6KIBUXIsPgJcJ+lS8kuWUNzsJf0cuMjMLkvHW+NF5nK5AjhXUlEgbf80Vhk1HIIo6ad4RM2meP2fXYDKq9qkCD6Hm3TvTHNbGzhWkpnZmVXkmdll8nDyyyXthJt41gPel2uOafC9zuE7K2EDVQTp5I3Tz2ZDwrtxZyBPaV/LvFRxsaq6q3UFPUBZS+I2zfH4B+sB/Et5pVVIPmujxWe/RL3ErsWBb6Q5AkzC45/nyPodgKwimuiCNK+dcTtk7YigtJI/HviImQ2b2/ltrm9kpSyv0dMXlrvia7cbq7lDWwBXAEWC3iQ887Wy2U0edbWFtYQgWn6C3VQzW6P0cwncOfreuV7cW86twO5tzDpj8EXDBpnzey9u6vwj8KEmdt4NvNf+fCBmZh/LmNuZZrbX3MYGSlfuDBIj6SkaVqd08sjklLkIQF4WYGs8EW3ANdib1uIluf8GDmlIXCPRRGWSvXo3XJlOBj6UKaqRlXLhXJS0q5md3zLXXTPnBvCkPJfkV+n4I8CTucLSQubk9KhLoyGIeAE48HLnyyZ5y2TIGdGqCADM7DFlNPMpLbgELIwr0qck1VlwFYqk1ns1s+xgh37otRCSZ6ln90fpVmXwPeYsGpZ7w7wML/QFgJndj9v6s+vBA8jLRqwGPGYZoaCSfmRmn5P0O9rsODIjRZqKJnJh0mPAXcB5wJdydislvoIrgAPS8SR8257LoXh56LmNDZQ98OikIprjhjSWhbzG1Pfwz0jZVr1ShrhGQxDxSrkjcdPTnfjnL+d/8Z/M19oySAuu37V5rz+vI7CuDyL5QYrieUUlWwEvAROz59VNZqLkF7g5RZ28gd4Os6yiYZJOB35sZnfUnNsOeCz603j8+U/wsMYxwFfMsyeryFvHzKY0ESki6Zdm9tGWaCJwm3edaKIR5k1GGkGe0bsq/oXMirtPtvxt8B3KuaWXRuDF3NZrYq51kXQTrlyOw0N098FX+N+sIGPhIuIq2aw3Ti/daHVCEHvLXRi/qQ2rGjklryY6vd1LeI2nxdu8NhC5jZhPkqluAzP7YzpeGFikTkRRXz4IM9s3Q9b3KvpB+5fXZcpgipmtoxqFn9rIfBDvlPQ4HqOdVV432W13xU1W1+L270fkxfCurmFb/iBwqdULs5z99ypFE4HfNCpHE0n6spkdI+lE2u9aPpshcxPgdLzGv4AVgAlmdkNFOWvifRGOpifr+xVcMV9X1dHY186sIHOHVv4sz/Y7FGMVZBSZstl25D7kXornUbycjt+MfwYrmSg0j6qWJvPJVDNbLUNWVjhvP/Ia8UGU5I0CVqH3LqPSd6Kg28xEL0uaCCyvNv1nc25CVPALzIXXzOxP4OGIluoTmdlTkuqEN24PHCfpBnyle4VVD5dcTB77XJiEimgfSVrbqjdBKZKtJle8rj9+gLczfChN7G242aOqjfR+3J6/EN52FF5cRPkAACAASURBVDxy7DQgp+x3UXr8g3ijkcJnsAeuYHJ5Ma1MH5b0aeAJ2mfW9sdCkj4MvKddNIvlZYKDt308T9IuuFK+BPhiVSHFzT4FQfzHvJTM2/AotMuryhsk80kT1QzKNOVvQdLH8fa8y+Md2TbAfXx5Zd27bGewNB5VczTenKYXVU0xJbkbA6uY2WkpEmMJM3u0oox7gE1wx9016Xlx8702N7IjyV4Qd2rvhq/qJ5nZgAu4JcfbHaX5lDHL7xnQ1knbOjZAWe3yKeYYG4Cc4/Cb6sFmNiuNjcBv6v8xs4Oqzi3JmGxm4+Y2VkHeurhSHQkcgZuxjjWzWyvI2BhXfB/Cb9hlsiJYSrIPxIMCxgD7F6aUTFlT8OKIo/ACdXfgXdA+kimvMfNJ+m4sjvvPcqsZlOV9Aw/I2Bw3FRseJVa5/7M8wm5dvL3qWpLejjeR6i+MtW953aQMCiRtbC2x7IU/IUPWYXim8Kpm9rakzc83s43mcmmrnMfo25xgmY7BsvwF8S/nPnhc9dIVrm10K1yS22STkVPxvIdytM6wqjc0SQ8Db2td5clDVR80s1Wqzi1d/wCe7FRUpB0LXGZm76go50wz20vSQeYF/2oj6dNm9uOWsdl2/wpyysXfBOwNTMWDBHILwZXNWZ8BFk0mxjkysCvKXA4vWTLb+pFrPhks6vogJN1hZuvKu6Wtb2YvSppmmYmJ3WYmKjgBT14pc2KbsYGwM14B9U4AM3tSeVVLV8lxeM6N5BDdDd9pXIc7pXLDNxuh5KRdrsVcN4Jqxb3KHIAnrxWmvhvpSbSrgrXb7psXdKuzMvo8nsRWrp20f4acddKC42OSzqBlt1bVSZv4GHOWSb6F6t+H1s/9hX2MV0WSNsQVfOFIrZyLUhJ2FF5q+n56ouMMj/DKkVermkGLrL3bjGEZGdfADHmk08XAJEnP4L7NLLpKGaQP1HuA0S2rmBHkf7heMjMrbhTJvpnDLZJm4LHxV1hez9N27I37Cvav4UT+SkNzKXgS9xfsQO9s6Fn4TTOHT6aV5+zVpzxJrurq+X5Je7d++eTliB/MnBtmdoU8HLQoq/Bg5v/jp3jG+0r4366sDCyNDwj1dP5atMUnNAKPaKmEtSkA1xAH4WG9F5nZNHlF2v7anc6NnfGdfHZQRYGaqWZQptzsaBHcXHQnPQ20BoyZ7ZyefkseRr8UmVnq0GVmInmY5SbAJ/EvVcEs4HeW6pVUlPlF3Fu/BR73/THg15aRlSvPrByfHsvhjtrLgetrRgO9Bd95/EFe/nd4YQ+vKGcjvIl7sb0u7KNZJqxki/+3pazZZIpZ2MxeyJDVzuRU2byVzAcX4vbfcsHARYGdzeyJqnNLcote1G8xs0+oZi9qSSeb2QFzP7NfGRPw3hbj6O3Mn4WHDGc5kCVNAna13nWizjGzrGALSe+y/Jr+7eRdnubXRFvZxqoZ9CF/JP63q9QDOc1jmjXYp6KrlEGBpLdYZlhaH/K2oNQQ28wmNSBzQdxpNh5XYDPNbNsMOZ8A9gPeYGYrp5vQTy2jz7A8jPbz+E1ydvKZZRbrk5cb+EDxpZSH0V1lZu+pIGMPvNDYxrhpqGAE8GrO+0xyN6NUMNBaSotnyKvdi7okq9Evuprt/EU7e34dv5OkG/GM4V/ixQhrVQaVdAGwJr7DKtedyglpnop3J3w6Hb8BNxU1pQwWBO4zs1XnevKc1/4W+Iw11Keiq8xEJV6QdCxzZvlVjopJ5qZzm1AAJZmLAiuaN4C5Jo1l9QbG7ejrAbcBmNnD8tyFHJ4zs8ohff2wSHl1Zmb/l26SVfgj3u9haXpnfc/CnZdZlP/2DVG7F3Vpbo02pDGzC9Rs5c1Xy3NLO9M6BRzfKw8p3QeYIu+78EszuypT5CXMGT2VS7tqBtmRSuqdl7IAnmGem/XeaJ+KblUGZ+F29O1wk9EEYGamrCXxJjdPJ5nnW4UCda0kZ9SxeJz7WElrAYfn/gOBF83speK+I0+wyf1iXpuU6IX0XlFVzTMo+LdKeQqS1qFimYG0w3sc2FC9yzA/YDXLTzdMo72oafCLroYqb5b4GnCTpOvxG+R78d1pNmb2J3ltp8l4AMi7kzL9alVzlpmdXlpwPVRzXmdLuo6ez91XLLOaQeL7peevAI9bZmltvEBlc5hZ1z2AKenn1NLYHTVlrgEciTsZ/1Bnbrij567S2H015B2DJ9o8iPs1LgKOzJR1bZvHNTXmti7e4elG3D8yHVgnU9auuFI4HXe2PQrsMtSftdL8tgCuxxcdZ+GZ0pvUkPf+do9MWVNbfi6BZ5fXeb9L44ut7YCla8paAy+78Sc8QmztNL4sfrOsKm974CHg0XS8FnBJ5tyuHshYBXlHD2Ssgry34KZYcIW/ZK6sbt0ZFCGcf0vb4yfxWkV1eApvYP0vINcMA/CymT3XYkGo0zfgEDwc7148lPEyMgu4mdmmNebRTt4d8kSYcney3PDarwPrWksZZiCrJn/TWLO9qDGz61sCAxYjPyKusazXEq/i34lFgNVSeGRuHP+J+Gf2q2Y2e+doHsad00f6W7jp9Lok5+4UoTRg5I2oFgOWTg7yciRWrkkXfNHQGr23dZuxgcxxtr8QWDnN66f0lD2vRLcqg+/I29R9Af+gjSAzpFHSp/C4/dG4be8T5pVLc5kmLxEwLDl7P4vbxbMwT+G/GLjYzHJNYcAcSUUFz+E7rbsz5M0RYSMpN8Km6TLMg8H76ekStyA9Bf8q0/AXvakqo8XcGi2DYGaziy2mG+8KZjY1vVapwU2iiQXX/njjnWWZMzy6NWdjrkg6APgUsFJyShcsiWdd59Ckv7A7zURNPnAH0loNylsMNzfdkR7fwR2tVeUIXwH9E6+E+jRuovhmjbn9Gt+q/yA9HsIV4B3AlzPknYv3kL6v9N7vzpzbscCVeKjkR/GQ3Ozt9SB8Tk4CrsKdoPvg8d4/qSHvbtyvVDYn3tvAPBcGlqop4158R3B3On47XrsnV951+ILtDbj57zbghzXknYJHoE3Fw8JPxCPsqshYF989fSYdT8Cd0ifgkXtV57QUXrrjbNy0UzwqyyrJvC39vCv9LAryZcnrqtBS9VEls8AqhJYplV9WHw3PLSMTNIUM/sEaMMekVfzWwH6W6iSlrfDJeFLbcRkybwC2sd6hoJfi4a9TrGLVR6XaPOWwQ0n3WMU6TMmRuDz+BW2kDHPTpLDcd1j6QsmLzE2ziuUoSvJuM7P1i79dCgy40yqENKr/VotYfp5Bo2UQSu/x4/iu4DBl1J0qyVsMd3LPDgcHjrAKHc+Sye8DZva0pPcB5wCfwf0P7zCzXXLmlmSX+4zfbJkBGpKOAZ7FE08/g+887jezr+XI6zYzUZNVMn+NO8em0NM9qaBSJujsizxk8DVJS1nNWGpgL7yV4Wy7tHlJ7D3xFWplZYD7QsoRMC8DbzKz/8h7y1alkQgbMzNJl5mXcs6ttDnYTMernxb5LSvQvlb/QLleUlGBcwv8i/67ijK27+c1I/9v2WgZBGC4pGVwc2zWjayMeVLj12rKKvdn2A1v/3oBcEFSglnIC9V9iJ6//WmSzjez7/RzWV805i+ELlMG5iFlo/Ht13RLGZKZsrZLK9L3W0NJHYn/A+6VZ3GWQwarJsQsaG0clGY2MyWy5HAWcFtKZgG/mfxaXoIjx09yGG4uWUHSWcBGuIknhzslrWs1mww1TSlufEnggRQKasD61AvfnOOLbmaVOmzZ4LRaBPebPUtDZRCAw/HV+83mQQcrAZWrBRRIGodH2I2hd6G6KjuNYZKGm4cvb07v0Nk69809gTWLXYq83MXd9PTWGDDmWdE/p2bntYJuMxN9HPguHs44Fjeh1Eo+UY2m5n3Im9Bu3Kp3Ouuz+md/rw1A7jj8pg3+5ay125L0RnoibG5tp8AGKOdB3P77GDWaDDWN+ug0V2AVOs61yJ2jamm7sbnIWB+v478yrlQ+ZmYP9H9Vv/K2B07Fd4yv4c3ms4MfBgtJDwFfwt/zbMexVahKIOlreLHFf+I7vrXTDvWtwOlWsWpxSe61eNmTopTHSNzfMmDnu7x0dX/m8DzzWpcpg/uATdPqeCU8tX3DmjIbaXvZNJJepbSzKL+EO6QHvDsYDP9ISXYjpYTVR1esKl/weYG8HlP5vWb97dopdFUs+SBpMp4tewNeNPDjllk/KMmbiiuAB5OiOcZKkUCZMnfEgwwK38pkPAnzplxzqqSbzGzjuZ85Vzkb4E7kqyz175ZnSi9Rw85/Me77moTf0LfAd5AzYGAWgr6+CwW534luUwat7e5qt79sekWqZhudN4Kk3yez2KP0XnHULVR3NG5vnUbPCs2sQhZtCpX7Kt569F7ge9ZgX+WmkLQfbu74L/5es/526rsW05J4t7wBh5Y2/X0YBHkH4KawL9Pj7xuHm0yOx/MOKjd9krQ53mmutTbRkPub+rIMFGRYCBopUgld5jNgznaXvY4z7PLQXNvLgtPoaXS+KanRecO/oxJmtl36ObZh0TtRv5TwGbgT/0TcoX8C+X6HweRLwOq5ZrASTdZiGtkSUdTrOOPm+D/qnYvS69iqN7f5LLBRy+7pmmSOmkF+ufN98HDXBSktQuiA4ANrsFSG5sxFWZ4aSWfdtjNoVOsmme+ip0b9A2Z2X87cSvJqNzofLOQlrO82s3+nqKS1gR/lOtDVQClhtYSiNrHbGwwkXQF80DLKcw8Wkk7r52Wz6l3iDuvvdavY70DSA9ZH6K2kBy2zaqukhyyjCui8ICm67wMLmdlY1ahNlqKa1sPzDYrQ7WwfZ7ftDJYDLjezu+oKkmcw/xYPEZyKb/vfJekvwI41TBVNNDofLE4G1pS0Jp69/QvgTDyzNocXgLsl1SolrN7lAIaVj+v4MxrmUOCPkm6jZtlkmJ0jcDQe7ivI6r07EXfaN7Xi+xNuP88qad6G5yWtaWb3lAfT569O6PUfJa1m9SoFDBbfomapjBJNFqnsOmXwZ7wT0ZrAPXiW6lVm9kyGrCNwO+Zm1tPYYgHgKDyD+DOZczwIz8T9bPodm+LZjZ3AKyliYkfcaX6KpH3nelXfNFFKeCnm7PhVOO+y8j0GiZ/hJbF7RbDU4Bhg+zrRP3gy0k8k/YmeDnt1Km6uCJyfQpevxr9ft9dQNl8ALkk7mHKjoQl4CGYuG+CLkEdxxdwRkWeJJmuTXa/6uSiz6SozURl5m7/xeBbiMLyo2RVmNqDYb0n3A2tYS5nkpH3v7Wt7OwC5K5vZn3OuHWzkJYmvwLu5vRcvRHZP7rZzfqJqpM8A5N2cG77YRtbb8Wz1rXDlei3+f77ZUhe6ivKWBD6Af7/WAx5I8q60iuXd5WXJD6TUaAgv45GttDo58kzSKbgiPQT4X3xRuKCZfTJD1gK4A76caf2LXOXctcqgTAr32wLYyswGVHddbbo5DeS1Aci9Hnf03IFHi9xgDbb8q4O8Z+6H8XLfN0paES/DnNOsu/BBfIuG2mh2MpK+i0ec/Y7eZqLc0NLjgTfjWb6NRcQk5+WmuHLY0MzG1ZGXZK6W5G1ZJ3Q1yepVqK6GnEZKPjSNepfKAL+Bf8cqlMooyVoc+K810FYWulQZpD/4F/AP1X7K6EebQkr3oLd5gnT8q9ydQZK9EB5rvAmeXbqEmdUtsd0IalM2OTdUTQ230exkkkmilTphue2cv5WdviV5o3D/VzkHIjdW/kLcn3RFYUKtg7x5zA5pblPwHenNZtauiu5A5H0T739RKM6d8KZUOSUfOhY10Fa2l7wuVQa1+9HKMwX7xDKLzUnaGDfBvBcYiaei32hmZ+fIaxI12E85ybvNzNZvdJJBZSQdgYfjPkLvfI+sktOSPoCHb26AV7U9rU6YpJovVPcQvUs+LIpHyQ15hJG8DM2u1pOBPAo4J2dH1c5CUcdq0W0O5IIm+tF+xMyeHIS5XYcrqu/h9WZeGoTfkUuz9dEbbqOZFOkqZnaavAbVEpYqtg4Vkr5sZsek57ua2fml175rZl+tKK+xyrslPoR/Jxr5rJnZH4A/pIi7PdLzv+I1cn5l1RsYNVqoDm9mtQieAAhetvuJBuQ2wdJWqplmZs/U+I7VbitbpluVQRPVMn8hL89wHe4cu6nVmZzJ0njtn/cBn5X0GnCLmTXbzzSPRkPV8GJt4BEiBUZGE5QU4z4O75p2Gp5Q9Ct66igNFbvjkT/g4aXl5ubj8ezpKjRZebfgPnwX+tTcThwo8ppTe+LVc+/CixxujEcCbVJRXFGo7iZroFAdHpY6La3CZ5d8UEpAzQ33bYjXJK1oKXcnmWVzv2Ofw6O7nsTN12/GM/6z6FZlULtappltI299twmwM/D9lGNQhOhlJWKZ2bOSHsHtt8sD78FvbJ1Ao6Fquaa0PtgZeDcprNS8JeKSDcrPRX08b3c8VywlRkp6V4OBBd8D7pLX7irv0ConOqW5XYQr5TPx8Ne/pZfOlddDqkTaTZ1fOn4Ej7TJ5SJ6d5m7roaspvkqcFMKJBFuLh5QUEsr1mxb2e7zGaRwq13w8K3a1TJbZI/FoybGA282s/UyZDyCN6+/CS8gdnunmIoaD1VzM8Jh+C4IvGH84ZZXfOx2M1tPKQM5RVLcMtSx4yplRKvB2j2SbsTNG7/ECy5mJ2FJmobnQbRW8cytqLqNmV3WMrawZZYdSSa/TzBnyeksZ3mSuRDwtnRY6ybZFKV70zX4vQnqVfI9EP9slP0Pe5jZSVnyuk0ZACh12GpQXmuEzXA8pKvyTVzSAk1EYAwW6YuJ1eynnGRdgJsoijIge+GOvX47cPUh64t4wcAt8JXux4Bfm9mJdedZB/VUjxWwKJ51TTquVD22jexV8Pe5K17Z8pdmdlWGnDvMbN3cebSR166iah3F90c8zLo16uyCTHmb4J+5x/D/wwrABMuolts0Td6b+nAgZ+e7dKsyOAqvQ34uvRvI5LSqbDrCZiyevTyG3qugrC17EyTn+mHAp+kpmvcqcKKZHV5DbqPRDsl0NXvXYmaTcuf2eiHFju+EF+h7Hn/vX7UK+QaSfoibhy6hhiNfnoeyHO6r+TA9ZrAR+Hcit5ZQ9meiD3lTgA8XEU7ystNnW2fU/2ry3nQvnhhb+EaH4T2Qs9qPdqvPoHCiHFgayy1d0HSEzcV4w+7f0UzZgib4PO5XWdda+ilL+rxl9FNO/EfSxmZ2U5K5EZnRDkmJ3lgoAEmLShpjZo9lzq2jkbQGHr65LV77fnszu1PSssAtVKvAWawUNyiN5Tjyt8J9b8sD5Qqls6juKC/z+3ampxosWA51NbM/Kb/7X9M0eW+6AvfT/Cwd74+XCMmiK3cGTaIGGpO3k9fwNGsh6S5a+imn8dF4EkvettNrRJ2Bl0AAeAb4qLUUJhugrMnAewrTXLIJ39yk+aOTSA7GXwC/MbP/tLy2l5mdOTQzA0n/m2vC6UPeLGBx4CW8ixpUL8pXlncqvtD6VRr6CJ48me2D6ESSD2I/ekpWT8V9mQf2fVXfdOvOAEmrM2cDmZyyCo1G2ADHpzDJq2gg9r4hBqOfMummv6a8HAhWrynN8LKPJoXALlRDXseStvtP9HXDr6oI5PV/vgssa2Zby8tHbGhmp1SUs6eZ/QoYo959DYp5Ve1nUFzXdFTYAfjKuwghvRHIcqo2TfI5Hoz3M8iqjlBgZq/Jq+SujOdoLA1kK+muVAbpZrsJrgwuwyOAbsJXqVX5CvBxSo3J8RVbLu/CHamb0bvxRlY2aEP05wjPcZIfDDxX3GwKJSCvgLqkmf0oY44zJe1gqae1vLJq7QixTsTMXpW0gqSFGoo0+yWem1EkdP0Jt1lXUgb46h0GoeS6pB3oiTq7LufmWJCimn5Ib1NWp1BUaC1KRjyBh9VWKZXzNjzZbw96/A+1Q7m70kyUHCtrAneZ2ZppZfQrM9uiopxhwLRcx1gfMqcDq3VKOCn0ioiZ4yUyImKSA2+D1nC+tJKfnGNikycOngUsm+b1V7zcyPSqsl4PSDoD7wt8Cb0djZVvcEU0UTnSpGmnbR2SU3Vd/P8LfpObbGaHVpQzKI3im6SIJmr5X/Rq4DQAGa/hu519i8+/pEesZgHIrtwZAP9JW6hXkoniKTy8rBJphfaQShmDDdB4NmhdzGxYwyKHt4vrTqadyolY6do/AxvIi3FhNbqnvU74c3osgPc/rsO/5RnDRdTJBmQ0j1HvlrJzYPmZvdsAa1lP35DT8azmSsoAb4va6TRRHeGDeOb7tfIOe+eQkeDYSrcqg8mSRuK1UqYA/4dHYOQwCk9tv53eK7TcUNCRwIOS7qCBbNAOZQFJb7KW2vZph5aNpG3xuveLFDqlTuhrJ2OphWRDyu9gfIexsqSbgdF47kJVpsz9lGxGAkV45VL9ndgXVupXoDaN4utPsRGaqI5wMXCxPPFyR7wsxf9IOhm4KCcXBbrUTFRG0hhghGXWR5fUtuWj5WdvNiqvE5G0N+68+wI9XcnWAY7FO6jl9KL+Kd4hblPcZ7MLnr1dpxNbx5ICIM7Em52D24b3NrNpGbIWxvNGVsVXkA8BC1hmxnDTyAtKHoU33RHuOzjEzM7NlNdoblCTSFo6PV0fGq2OMApX8Ltl50B1ozKQdCZe6uFGM3twqOdTMBg+iE5F0tZ4N6fV8S3xNOAoM8uKg1YqaVz6uQTe7/q9zc26c5Bn5X7NzK5Nx5sA37WMWvVqKGNY0o/M7HOSfkcb23yd3a28amkRJny71et01mij+CaQtD1wKvAKrph3M7Obh2o+7eiUrVPTnIoXgDox2eTuwjuKHV9VULKvnog78xbCW2j+OycGepB8EB1HWuldZWZtd0GZFOWIX0iJV/8ClmlQfqexeKEIAMzsumQWGDDqyRheVN4GtpwxvFjGnIqQ1u9nXDsHkt5uZg/Ku5IBzEg/l5W0bI1w66ar7zbBkcB70/tdH6902+T3ozZdqQzM7FpJN+ArjU2BT+K25srKAPgx7qw5Hy+hvDc9BbByaNoH0Yk03Tgd4HfJD3Qsbnoy3CfUrTwi6Rv03ID3xJvTVKGcMfwDepRBVsawmU1JP69PkWFvx/8PD2VGxx2Mm3N+0O7XkR9u3XRuUBO8UlgpzOw2dUbF3V50q5noajwm+hY8BOsmM8uK3imFgs3uvKQ6xaDmA59BgRponK7UMEbSWOsplbEwHvKaXcmz00k24G/jPQLAP8ffMrNnMmQ1nTG8LfBTPNpJwFhg/xomwEWspQdwu7EK8hqtvtsEkmbQO+/h4PJxbsJek3SrMjgOd1i+CNyM+w9usZa0/gHKugG/of0C+DvwN7ykwoDjggNHGY3T1VOyOrsq5vyOpIPwZKdZ+G5qbdxBmxd14r2ttyvFuK8MXJrrC2vKp1G69oNpPh3hIIfZibB9UkSPDSXdaib6PMxemX4U/yK8Ga8PX5W98FjvT+MF3VYgo/GGpJvMbGN5HZayBhY16rB0MiVbcJmLqWau+5ekq4CVJF3S+mKXmddmI2kcbsoZQ+/qtjmJUx8zs+MlbQW8Ef9Mn4mXRMlhVkuy3yO4oqnEIPg0CrYHjksLuXPxZlRNdCmsw59wP9q/hngefdKVykDSp3EH8jp4TfNT8W12Dm8FnjIvqVBHe38EBqUOSydzEr4KnYp/0VfHo4qWknTAAFem2yYZZ9LettytnAV8iZaGNJkUN9ltgDPMbFpO8l9acYPn8VwGnIcvbHYF7siYV18+jeepUQXVzPZJ/qqt8Wzmn0iaZGYfz5XZAIPhR2uUbjUTfZHULKPuiiBlQ26IJ8TciJucbqpqu1XvjlgXmFmdtn6vCyRdCHyjiI1PZqLDgS8DF9oAyyGkkNyfDfGXeZ5S7CQbknUavgIfi5dpGYbX/6lU3z/J6RMz2ydzfo36NEpyF8T9VfsA7zOzpedyyaDThB9t0ObWTcpA3sC+TyyjgURJ9rJ4otMX8eqPlXZV6l2LJNsB/XpC0n1mtnq7MVWsjSPpFjPbsPlZdiaSNsdXtVfTO1O9Sh+DQtYCwFrAI+Y9uN8ILJebiNk0kr4LHGO92zd+wcy+nilva7xvwCZ4/+PzcBPNUJuK5iDHjzZYdJuZaAq+bRW+LXsmPR8J/AVfGVVC0p64yeldeBboj8kzOVkfz7uZafIU+XPS8W7A/SkaqGpP2ruTz+B8eofkVr45vk7YBw/dXJDe1W1z3u/5uKn0boBkt862XUtaBI/WeSe9S8Tn9gvY2sxmm4XM7BlJ2wBZygAP/z4Xj3DqGCdyQVJ2q9Dzt7vDzIbcBNpVO4MCST/Ha3Rclo63BnYys/0zZP0TD6H7KXCtZXbWUv+9crvVgbwoHuNdmDtuxv0I/wUWswr1dvowUViNG1BHI+khM1u1IVkfwJXLBrhiOM1KncAy5J0PPIi3vjwc94c9YGYHZcqbinfZezEdL4pXLc1q35hkzFGbyMwqO7mbRtLHgYNwP8nd+P/kFjMbyhL2QPcqgzlSz+uko0t6J14vZWNcoz9kZnvVn2kQtCcpv2PN7P4GZS6Fm56+hpcA/zle2r3SLk09Xf+K0iAL4qVfNpjrxe3lfQWPACoU/j7AJWZ2TKa8Tq5NdC+eDHurma0l6e14mZEPzuXSQafbzEQFT0r6Or3b3j2ZI0heAntF4C14mN9SzD9mnlrIex5/C//blcMjK9ddlzf0OBl4U/I5rAHsYGbfaWi6ncYGuGnsUdxnUOwgc9utvhHPYt4LL89yFr64mYDb1qtQKI9n5QX1/g5k9wU3s6PT7qC4WR9hZlfmyqP5vuVN8l8z+68kJC1sXp6ikR1gXbpVGeyBl4q9CL9x35DGcrip9Pixmc2Yy/lBD6fguRlT8OJcdfg5Hmr5MwAzmyrp10C3KoPxTQmSdBFesfRMPFmsKAJ3rry3dFUmJrv3N/DS2EsA36wzx5S9nN3MvYVOrE1UMENeVuViYJKkZ4DH53LNMins8gAADwJJREFUPKHrzEQpDPEMM/tIQ/I+ZGbntYztambnNyG/m5F0m5mt35Csju7WNRhI2hi3e58maTSwhKWSHAO8fl3cHPQO83pdE/DGKI/jpS2yo+uaJOUvHI3vLkRNP5qkY4BncUfyZ3C/1f1m9rV+L5zHyEvTLIUnxQ1558OuUwbgMdrAZk38gZtOlZ+fkLczHIZHwJTDIytXo5R0OZ4Ffr55eYpd8LZ/Wzc1304ilS8YhzdLf1sKbT7fzDaqIONO4ANm9rSk9+FRXZ/Bw0zfYWa7VJzTnmb2K3mP6zmwzPo68law25vZAznXt5HXibWJRpjZ832Fv3eCYu5WM9EjwM0pFDGrf2yKQNoGWE692/2NwGuSB3On2BWMK43lVqM8EJgIvF3SE8CjpKzuLmVn4N2k5kBm9qSqV7ocVrrJ7AZMTMldF8hr/lelKKHddBb9P5pSBADmLW8vBi42s5lNya3Jr/G2nOXw9wIDavUvboJuVQZN9I99GpgM7EDvdn+zcDt4MBfMbNNmxdkH5DX9FzCzWZIq5428jnjJzExS0Su3Ui+DxDBJw1Oy1eZ4hE1B5e++mRX+mqaLqk2WdC5uR89OsJM7CQ7Dd5ALpLFXgRNtiNujmtl26WfHfma7Uhk09GE9OZkjtrKMNo3zM4NkTrgAWNvM/l0a+w1ef6obOU/Sz4CRKVTyY1Tv33A2Xtv/n8B/SMmSkt4KVC7/3bJDngMz+2xVmYkReN7NlmVxVE+w+zzeU3hd6yl3vhJwsqTPm9lxmfOrjdoXbZxNjum0abpSGSRn25eZM0OyinliIUkfBtZXT4Gu2XRx5msTNGZOSHHY78SL25X/DyMo/W+7DTP7vrwxy/N4JNA3zWxSRRlHynt7LIOXYyhs5gvgvoOqTJn7KdWxzJpGbdgL2MJKPYXN7JFUReAqYMiUAT1FFhfBzab34KaiNXALxJCXWulKZYDHUJ+L2+g+icdSV7UdfhK3SY/EE2LK5JYFmC8ozAn49ryXYyzDtLMq/n9s/T/MAj6RPcnXAenmX0kBtJFxa5uxP2XK6rVDlrSYmb3Q1/kDJSXYteupXDW7fEFr01zezGamxLghozCZyos3rm1m96bj1fFcnCGnW6OJppjZOurdnewOM1t3bte2kbWvmZ3S/Cy7H0k343Vnnk/H78AjYlbv/8q2sjY0s1uanmOnoTn7Xcx+iQ4pWyJpQzyHZAkzW1HSmngdoE9lyitX8F0Ed54/WdXs1F+UX6dEAEqaZi1lNtqNDQXdujMoMiT/Jm/R9yTQb0XTfjhT0mfxchQA1+Op7VULrc2PfBfvXbwtvsI/g/wIoOnyvrZj6J3N3G21ia7GGzFdCJxjZn8Z4vm040d4L4JLAMzsnhS6moW1lK+WdDae5FmVNSU932ZcdI5JcaqkX9C7OkJHVI/tVmXwnVSH5QvAibh9OTcC6CS8cuRJ6XgvvCzCfFNbPxczuzRtz6/C/Qc755oogN/iDtA/UD+buWMxs53SZ/eDwM/lFULPxRXDkMeiF5jZX9W7P06T/5NVyChvYWbDGpzDYLEPcABerA68OsLJQzedHrrSTNQkku6xln7H7caCHiSdSG9Tx+Z4qO9jkBd10u3Zxu1IyVO7AyfgxcyGvGk6gKTf4M3cf4znkhwEjDOz3TPltZrG/g4c2rpj6BbkVVRXtBqVYweDrtoZtLkJ9SIz9O1VSSub2Z/T71iJLl6ZNkRrvZsmolB+L2kbS2XJuxlJ78Frab0XN5fsbGa5bVsHg0/ifayXA57Ad36V/QWSNjKzm4HRZvbfZqfYmUjaATgWWAgYK2kt4HDrgF7eXbUzSLVXCr6NJ6DMJidfQN5x6jQ8qxncZr2PmV2bOc35gkGoETULD1l9EfcJdYxDtUkkPYbX1TkHuIaWbPdOiEdvJRWt+5SZHVnxuiLQoyOcu/MCSVPwDPzrrKfGVnZ5/Sbpqp1B+WYv6XN1ksWKIl9mdrW8Hvr+wE74Kuie2pPtcszsVUlvkbSQNVAjysyaLoHQqTyG7263oqe2TkFuKY9GkLQCXql0Wbwi8Dn4omtvPMGtKi9Lmggs3y6hrUYSWyfzspk91+Jv6YgVeVcpgxbq/oF/hjeuBreLHkJPka+JeD/koH+aqBHV8ZmbTWJmmwz1HPrhDDya7gK8xPZkvFvXGtZTFrsK2+Hfsa0YpIS2DmRaSmYdlhaZnwX+OMRzArrMTFSm7taz7CSW9BNgppl9Kx3Pd87MHFLlzTmoUi5EUn/mOKuYVf66RNJEM9tv7mcO+jx6BU5ImoE7Ql/r57KByF3TzOaL3bakxfBOc+WKqkd0gs+kq5RBS1TCYtToMyzpPmAtM3tF0oPAfmZ2Q/FaTuLU/IqkJQCsQs/joIdOsalLugfvilbYOK4tH+eGvko6HTjIzJ5Nx6OAH3RhDklH01Vmoobtyo0W+ZofSan2Z5IS/tLfcm8zm1ZTbkeslOchTw31BBJLkUpqlyiO65RhXqNQBABm9oykd2fK6kiSqbRPOiGaqKuUQZMMQpGv+ZGJwMFF5JWkTfDKm++pKXfc3E95/SJpJ+CtwL1mdqWZNdYCsyarDFLm/QKSRpnZMwDyBjDddm/aEO86dzbem1n9nz7v6bY/eKM0WeRrPmXxcgiumV2nvLr8rXTKSrlxJJ2EV2n9I3CEpPXM7IghnlbBLclPcAXeqvGxhuT+IMk+H79J7gJUClN9HfBmYAs8f+TDwKXA2XV3yU3SVT6DoLOQN2K/EzcVAewJrGNmO1eU02ul3OwsO4vkq1ozheYuBtxoZh3Ts0HSGDySaDyedHYT3sj+ejN7se8r5yp3NXrCZq8xs/vrzbRzkbQwrhSOBb5tZj8e4ikBoQyCQSQ5Ar8NbJyGbsA//M9UkFFeKW8O/K6DVsqN0+os7hTncTtS3an34ophEzzibtsK13d8X+AmSUpgW1wRjMEL/Z1qZk8M5bwKQhkEg0a5jEcNGR29Um4aSS8A04tDYOV0XETErTFUcyuQtD1waWtIqaTlqtzYJP3ezLaT9Ci984KK9zrkfYGbQtIZwOrAZXjRwfuGeEpzEMogGDQkXQ8sD9yBR2PdYKmpRwUZr5uVchNIekt/r5vZ4/NqLn0h6Ve4Q/QCfGX74BBPqeOR9Bo9iZftFN+Ql1UJZRAMKpIWAtbl/9u7mxi76jqM49+nTQwWZlAKYSFg0WDSYjJQjYBYjDUkdqVgoiaapmQMoCS8rWQBLonvlWIXjU2FGBc2cWETEyGERYMgtNK0UF1ULCGNWnlJmDJE0/Kw+J+L09th5p57z7m3d+b5JF3MOXP//Sdt7u+c/9tThhFuowSi9JwtMQ5PysuRpEnKcMctlC+3XZQJ0Zk+2roeOGD7LZWIyvXAVp+dWQ5LVopBtEbS5yhjyhsosZUHKMM8PZ9jMw5Pyk2a5zjn925xljxBdkhaTcn3uBv4K2WS/yHb22q2cxCYouQB/wr4JfA1259vtMOxoBSDaI2kk5QzZx4E/tDEgXUxepK+DGyhfPk/Cjxi+3g1p3PY9pqa7f3F9npJDwDHbO9c6sOBZ6PsM4g2XQhcT4kMvbMaN33a9v29NjBOT8rLyM3AzzrHs3TYnpU03Ud7M5Luoyw9vqEK9RlpgP1ytGLUHYilqzpi4CXgH8A/KeP9tbJybU/Ynpznz0QKwcj8q7sQSPoBgO0n+mjv65Sciunq9NNLKGvwY4gyTBStkfQS8DfKSqK9wLMZKhp/8w3hSDqYyfzxlmGiaNOWeZ4gO1GHMWYkfYcSb/nxatK3YwLo+99U0rXANmAtJQ5yJXDC9vkDdDdqyptBtOZ9niAzMTimJJ0PfJiyIOB7c27NDLJbWNI+4BvAbsohhJuBT9i+b4DuRk15M4jGSbqOcjLpRZLunXNrkvLUF+PJto9KuqP7hqQLBikIto9IWmn7FLBL0vNAisEQpRhEGz4AnEf5/zU3Y+JNEhc6zn5DiarcT1nh1Z3P3O/xEbPV5sQDkn5IWWyQxS1DlmGiaI2kj9p+WdIq27OLfyKWo2pj4b8pDxH3UEJ0tts+suAHo1EpBtGaarhoJ+UIisskTQG32f7uiLsWfZC04FyP7e4UtBgjKQbRGkl/pgwL/d721dW15EePKUlPLnDbtjcucH++9q6ghMO/DvyUkoK3Afg78G3bz/Xb16gvcwbRKtuvSKcl/J0aVV9iMLa/0HCTuyjHWUxSoiDvBm6iFISHgWsa/vtiASkG0aZXJH0WcBWEchflQLMYc5I+CawDzulcs/1ozWbOs72jau9227ur649Lyg7kIUsxiDbdDvycEo94DHgMOGNZYowXSd+nHEm+jhLWsokSf1m3GMwNx3lzgXsxBJkziIhaJB2iHDn9vO0pSRcDv7Z9Y812OlkVc3MqqH7+mO1zG+x2LCJvBtG46iji9+OlnGG8TLxt+x1JJ6uQm+PApX20s7bhfsUAUgyiDW/Nc+1cYBpYDaQYjLd9kj5EWf2zHzgBPF23kfmCiSTtsH3r4F2MujJMFK2SNEGZOJ4Gfgv8xPbx0fYqmiJpDTBp++Aiv9prezm7akTyZhCtkHQBcC/wTeARYL3tN0bbq2iCpDMyKSTd0H1CbZ/yoDAieTOIxlXLAm8GdgC/sH1ixF2KBknaM+fHc4DPAPvrbjqr2voKJT7zkO0/NtTF6EOKQTSuirf8L3CS0yMrE1W5BEm6FNhq+6s1P7cduBL4E/BFYE8WF4xOikFEDERli/mLttfV/NwLwJTtU5JWAXttf6qVTsaiMmcQEbVI2sb/3/hWAFcD/RxS978qvwDbs+o6tySGK28GEVFLFW7TCSl6DTjaT5TpnE1ncPrGs85wYjKVhyhvBhHRk+p8qR9RYimPVpcvpuQXPyXpKtsHajSZTWdnkbwZRERPJD0ErALusT1TXZsEfkw5jfZLti8fYRdjACkGEdETSUeAK9z1pSFpJfAqsMn2MzXam+H01Wbv3SKrzoYuw0QR0at3ugsBQLUa6D91CkH1uYnFfyuGJaHTEdGrw5I2d1+U9C2SUzH2MkwUET2R9BHgd8DblAPqAD4NfBC4yfaxUfUtBpdiEBG1SNpI2TkMcNj2E6PsTzQjxSAiIjJnEBERKQYREUGKQUREkGIQERHAuxHnmvkxXZ4TAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM0uqhO771Aa",
        "colab_type": "text"
      },
      "source": [
        "Init and set data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGvBvbzo8P3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAINING_RANDOM_STATE=42\n",
        "TEST_RANDOM_STATE=42\n",
        "\n",
        "TRAINING_SAMPLE_SIZE = 90720\n",
        "TEST_SAMPLE_SIZE = 10240\n",
        "\n",
        "max_bert_len = 70\n",
        "\n",
        "BATCH_SIZE=128\n",
        "EPOCHS =10\n",
        "HIDDEN_SIZE=768\n",
        "EMB_DIM=100\n",
        "\n",
        "\n",
        "catagories = list(set(data_training[category_col].unique()))\n",
        "\n",
        "OUTPUT_DIM= len(catagories)   #num of catagories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZljrXDtajm5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "catagories1 = list(set(data_training[category_col].unique()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-knpyoxYjuIp",
        "colab_type": "code",
        "outputId": "5d103815-706f-4626-9c28-b1bbe7865f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "catagories1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Quality/Reliability/Package',\n",
              " 'Firmware/BIOS',\n",
              " 'DesignTools',\n",
              " 'Software/Driver/OS',\n",
              " 'Networking/Connectivity',\n",
              " 'Embedded',\n",
              " 'IP-MemoryInterfaces',\n",
              " 'System/Platform',\n",
              " 'IP-Interface Protocols',\n",
              " 'DevelopmentKits',\n",
              " 'Hardware/Platform',\n",
              " 'Power/Thermal/Mechanical',\n",
              " 'Other',\n",
              " 'Acceleration',\n",
              " 'Technology/Initiative',\n",
              " 'Storage',\n",
              " 'IP-BasicfunctionsQsysDSP',\n",
              " 'Devices',\n",
              " 'Media/Perceptual/UI']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9fP0WJE902X",
        "colab_type": "code",
        "outputId": "728f211f-365d-4bcb-df42-d3dd44e3baa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_training = data_training.sample(n=TRAINING_SAMPLE_SIZE, random_state=TRAINING_RANDOM_STATE);\n",
        "batch_test = data_test.sample(n=TEST_SAMPLE_SIZE, random_state=TEST_RANDOM_STATE);\n",
        "\n",
        "train_texts = batch_training[text_col]\n",
        "test_texts =  batch_test[text_col]\n",
        "train_labels = batch_training[category_col]\n",
        "test_labels =  batch_test[category_col]\n",
        "\n",
        "len(train_texts), len(test_texts), len(train_labels), len(test_labels)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90720, 10240, 90720, 10240)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn5EAD-Q7aK-",
        "colab_type": "text"
      },
      "source": [
        "1. Ngram baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXghfhlK7euk",
        "colab_type": "code",
        "outputId": "047679c5-5d73-4a39-fb52-57442b3d5886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#CounterVecorizer(ngram_range=(1,3), min_df=0.2, max_df=0.7, max_features=10000, stop_words=\"english\")\n",
        "#ngramCount_baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_labels)\n",
        "ngramCount_baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,2),stop_words=\"english\",max_features=30000, max_df=0.75), LogisticRegression()).fit(train_texts, train_labels)\n",
        "ngramCount_baseline_predicted = ngramCount_baseline_model.predict(test_texts)\n",
        "print(classification_report(test_labels, ngramCount_baseline_predicted))\n",
        "print(confusion_matrix(test_labels, ngramCount_baseline_predicted))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                             precision    recall  f1-score   support\n",
            "\n",
            "               Acceleration       0.67      0.43      0.53        67\n",
            "                DesignTools       0.74      0.78      0.76      1206\n",
            "            DevelopmentKits       0.64      0.50      0.56       119\n",
            "                    Devices       0.70      0.76      0.73      1261\n",
            "                   Embedded       0.72      0.60      0.66       263\n",
            "              Firmware/BIOS       0.66      0.64      0.65      1504\n",
            "          Hardware/Platform       0.66      0.78      0.72      1979\n",
            "   IP-BasicfunctionsQsysDSP       0.55      0.37      0.44       123\n",
            "     IP-Interface Protocols       0.71      0.55      0.62       337\n",
            "        IP-MemoryInterfaces       0.75      0.66      0.71       240\n",
            "        Media/Perceptual/UI       0.72      0.39      0.51        74\n",
            "    Networking/Connectivity       0.61      0.48      0.54       562\n",
            "                      Other       0.55      0.34      0.42       297\n",
            "Quality/Reliability/Package       0.60      0.53      0.56       133\n",
            "         Software/Driver/OS       0.65      0.72      0.69      1810\n",
            "                    Storage       0.73      0.51      0.60       120\n",
            "            System/Platform       0.21      0.06      0.09       145\n",
            "\n",
            "                   accuracy                           0.67     10240\n",
            "                  macro avg       0.64      0.54      0.58     10240\n",
            "               weighted avg       0.67      0.67      0.66     10240\n",
            "\n",
            "[[  29   14    0    4    0    1    5    0    1    2    0    1    0    0\n",
            "     9    0    1]\n",
            " [   9  936    9  149   20    1   13   10   24   12    0    1    4    6\n",
            "    11    1    0]\n",
            " [   1   17   60   23    8    0    1    1    1    3    0    0    0    0\n",
            "     4    0    0]\n",
            " [   0  137    7  961   18    5   17    7   31   23    1    0   10   36\n",
            "     8    0    0]\n",
            " [   0   44   10   34  159    2    2    4    0    2    0    0    1    1\n",
            "     4    0    0]\n",
            " [   0    0    0    1    0  960  308    0    0    0    0   25   16    0\n",
            "   177   10    7]\n",
            " [   1    3    0    4    0  202 1545    0    0    0    1   34   14    0\n",
            "   164    2    9]\n",
            " [   0   27    0   25    3    0    3   46   16    3    0    0    0    0\n",
            "     0    0    0]\n",
            " [   0   48    6   71    3    0    4   13  187    2    0    0    0    0\n",
            "     3    0    0]\n",
            " [   0   26    2   38    6    0    0    3    4  159    0    0    1    1\n",
            "     0    0    0]\n",
            " [   0    0    0    0    0    4    2    0    0    0   29    0    2    0\n",
            "    37    0    0]\n",
            " [   0    0    0    1    0   42   94    0    0    1    0  268    2    0\n",
            "   147    0    7]\n",
            " [   1    1    0   14    0   36   48    0    0    1    1    9  101    2\n",
            "    79    2    2]\n",
            " [   1    6    0   44    2    1    5    0    0    1    0    0    2   71\n",
            "     0    0    0]\n",
            " [   1    5    0    0    1  152  196    0    1    2    7   95   31    2\n",
            "  1306    7    4]\n",
            " [   0    0    0    1    0   11   17    0    0    0    1    1    0    0\n",
            "    27   61    1]\n",
            " [   0    1    0    0    0   38   70    0    0    0    0    2    1    0\n",
            "    25    0    8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZGI38mTy3dd-"
      },
      "source": [
        "2. BERT Fine-tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9uRf2q_BhKD",
        "colab_type": "text"
      },
      "source": [
        "Load BERT pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1InADgf5xm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "## Want Roberta instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.RobertaModel, ppb.RobertaTokenizer, 'roberta-base')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "BertModel = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blm0LshSE-f-",
        "colab_type": "text"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg82ndBA5xlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
        "tokenized_training = batch_training[text_col].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "tokenized_test = batch_test[text_col].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxIB32MKGAeE",
        "colab_type": "text"
      },
      "source": [
        "Limit tokenized to max_bert_len"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynA4SkHqa4LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#padded = np.array([if (len(i)>0) i=3, for i in tokenized.values])\n",
        "tokenized_limted_training = []\n",
        "for i in tokenized_training.values:\n",
        "  if len(i)>max_bert_len:\n",
        "    tokenized_limted_training.append(i[:max_bert_len])\n",
        "  else:\n",
        "    tokenized_limted_training.append(i)\n",
        "\n",
        "tokenized_limted_test = []\n",
        "for i in tokenized_test.values:\n",
        "  if len(i)>max_bert_len:\n",
        "    tokenized_limted_test.append(i[:max_bert_len])\n",
        "  else:\n",
        "    tokenized_limted_test.append(i)\n",
        "     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JglXYRwDGRB0",
        "colab_type": "text"
      },
      "source": [
        "Padding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URn-DWJt5xhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded_training = np.array([i + [0]*(max_bert_len-len(i)) for i in tokenized_limted_training])\n",
        "padded_test = np.array([i + [0]*(max_bert_len-len(i)) for i in tokenized_limted_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdjg306wjjmL",
        "colab_type": "text"
      },
      "source": [
        "Our dataset is now in the `padded` variable, we can view its dimensions below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjLgfvueLBd6",
        "colab_type": "text"
      },
      "source": [
        "Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3d9QSDKLCnY",
        "colab_type": "code",
        "outputId": "973e12c0-378d-4764-933c-72ffe70aa5ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "attention_mask_training = np.where(padded_training != 0, 1, 0)\n",
        "attention_mask_test = np.where(padded_test != 0, 1, 0)\n",
        "\n",
        "attention_mask_training.shape\n",
        "attention_mask_test.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10240, 70)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fHgYJXqLJqO",
        "colab_type": "text"
      },
      "source": [
        "Convert to tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeQeyUzKLL_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokens_tensor = torch.tensor(padded_training)  \n",
        "train_masks_tensor = torch.tensor(attention_mask_training)\n",
        "\n",
        "test_tokens_tensor = torch.tensor(padded_test)  \n",
        "test_masks_tensor = torch.tensor(attention_mask_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YhA8McnLTCS",
        "colab_type": "text"
      },
      "source": [
        "**And now Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6rh9UpGRUAe",
        "colab_type": "text"
      },
      "source": [
        "Biniarizatiom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9ed6sC6LZuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training (Note: must check the categories align between training and testing)\n",
        "mlb = preprocessing.MultiLabelBinarizer()\n",
        "data_labels =  [set(catos) & set(catagories) for catos in batch_training[[category_col]].values]\n",
        "bin_catagories = mlb.fit_transform(data_labels)\n",
        "target_tensor_bin = torch.tensor(bin_catagories.tolist())\n",
        "\n",
        "train_y_tensor=torch.max(target_tensor_bin,1)[1]\n",
        "\n",
        "#Test\n",
        "#mlb = preprocessing.MultiLabelBinarizer()\n",
        "data_labels =  [set(catos) & set(catagories) for catos in batch_test[[category_col]].values]\n",
        "bin_catagories = mlb.transform(data_labels)\n",
        "target_tensor_bin = torch.tensor(bin_catagories.tolist())\n",
        "\n",
        "test_y_tensor=torch.max(target_tensor_bin,1)[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY5BjiD9SEwS",
        "colab_type": "text"
      },
      "source": [
        "Dataloader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsyFHsYASDuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler = SequentialSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqnADjn6SZJO",
        "colab_type": "text"
      },
      "source": [
        "Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tCrjW7_SXv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertMultiClassifier(torch.nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertMultiClassifier, self).__init__()\n",
        "        # Need to define the right layer \n",
        "        self.bert = model_class.from_pretrained(pretrained_weights)\n",
        "        #self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.linear = torch.nn.Linear(HIDDEN_SIZE, OUTPUT_DIM)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        last_hidden_states = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = last_hidden_states[0][:,0,:]\n",
        "        #dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(pooled_output) # Logits \n",
        "        proba = self.sigmoid(linear_output) # ???\n",
        "        return linear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48vLQR5FShu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_clf = BertMultiClassifier()\n",
        "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-5)   # learning rates: 3e-4, 1e-4, 5e-5, 3e-5\n",
        "criterion = torch.nn.CrossEntropyLoss() # the same as log_softmax + NLLLoss.  Check BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TDXoNFhSxNX",
        "colab_type": "code",
        "outputId": "8aabaa41-37bd-4ede-f52e-b87d41c4974b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "        probas = bert_clf(token_ids, masks)\n",
        "        \n",
        "        batch_loss = criterion(probas, labels)\n",
        "        train_loss += batch_loss.item()\n",
        "               \n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        \n",
        "        #clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        #clear_output(wait=True)\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"{0}/{1} loss: {2} \".format(step_num, len(train_y_tensor) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
        " \n",
        "\n",
        "    bert_clf.eval()\n",
        "    bert_predicted = []\n",
        "    labels_y = []\n",
        "    with torch.no_grad():\n",
        "      for step_num_eval, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "          token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "\n",
        "          probas = bert_clf(token_ids, masks)\n",
        "        \n",
        "          bert_predicted += list(torch.max(probas,1)[1])  \n",
        "          print('Eval: ', epoch_num + 1)\n",
        "          print(\"{0}/{1}\".format(step_num_eval, len(test_y_tensor) / BATCH_SIZE))\n",
        " \n",
        "    print(classification_report(test_y_tensor, bert_predicted))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "0/240.0 loss: 2.8679699897766113 \n",
            "Epoch:  1\n",
            "1/240.0 loss: 2.7986998558044434 \n",
            "Epoch:  1\n",
            "2/240.0 loss: 2.758255879084269 \n",
            "Epoch:  1\n",
            "3/240.0 loss: 2.709742546081543 \n",
            "Epoch:  1\n",
            "4/240.0 loss: 2.6631901264190674 \n",
            "Epoch:  1\n",
            "5/240.0 loss: 2.614661534627279 \n",
            "Epoch:  1\n",
            "6/240.0 loss: 2.5772714614868164 \n",
            "Epoch:  1\n",
            "7/240.0 loss: 2.5566377341747284 \n",
            "Epoch:  1\n",
            "8/240.0 loss: 2.5235200458102756 \n",
            "Epoch:  1\n",
            "9/240.0 loss: 2.49167377948761 \n",
            "Epoch:  1\n",
            "10/240.0 loss: 2.471671386198564 \n",
            "Epoch:  1\n",
            "11/240.0 loss: 2.453937828540802 \n",
            "Epoch:  1\n",
            "12/240.0 loss: 2.4471863966721754 \n",
            "Epoch:  1\n",
            "13/240.0 loss: 2.4326633385249545 \n",
            "Epoch:  1\n",
            "14/240.0 loss: 2.4288131554921466 \n",
            "Epoch:  1\n",
            "15/240.0 loss: 2.420998379588127 \n",
            "Epoch:  1\n",
            "16/240.0 loss: 2.4055044651031494 \n",
            "Epoch:  1\n",
            "17/240.0 loss: 2.3965958489312067 \n",
            "Epoch:  1\n",
            "18/240.0 loss: 2.394152038975766 \n",
            "Epoch:  1\n",
            "19/240.0 loss: 2.3819087028503416 \n",
            "Epoch:  1\n",
            "20/240.0 loss: 2.3710590998331704 \n",
            "Epoch:  1\n",
            "21/240.0 loss: 2.3573324246840044 \n",
            "Epoch:  1\n",
            "22/240.0 loss: 2.332539475482443 \n",
            "Epoch:  1\n",
            "23/240.0 loss: 2.3202859461307526 \n",
            "Epoch:  1\n",
            "24/240.0 loss: 2.3068077468872072 \n",
            "Epoch:  1\n",
            "25/240.0 loss: 2.2956345814924974 \n",
            "Epoch:  1\n",
            "26/240.0 loss: 2.282673707714787 \n",
            "Epoch:  1\n",
            "27/240.0 loss: 2.270717535700117 \n",
            "Epoch:  1\n",
            "28/240.0 loss: 2.265234585466056 \n",
            "Epoch:  1\n",
            "29/240.0 loss: 2.2522963086764016 \n",
            "Epoch:  1\n",
            "30/240.0 loss: 2.240829994601588 \n",
            "Epoch:  1\n",
            "31/240.0 loss: 2.2265708334743977 \n",
            "Epoch:  1\n",
            "32/240.0 loss: 2.220245776754437 \n",
            "Epoch:  1\n",
            "33/240.0 loss: 2.209639615872327 \n",
            "Epoch:  1\n",
            "34/240.0 loss: 2.198171033178057 \n",
            "Epoch:  1\n",
            "35/240.0 loss: 2.1907829874091678 \n",
            "Epoch:  1\n",
            "36/240.0 loss: 2.17791313416249 \n",
            "Epoch:  1\n",
            "37/240.0 loss: 2.1678587574707833 \n",
            "Epoch:  1\n",
            "38/240.0 loss: 2.1572710336782994 \n",
            "Epoch:  1\n",
            "39/240.0 loss: 2.1502010256052015 \n",
            "Epoch:  1\n",
            "40/240.0 loss: 2.141783629975668 \n",
            "Epoch:  1\n",
            "41/240.0 loss: 2.133767369247618 \n",
            "Epoch:  1\n",
            "42/240.0 loss: 2.129186832627585 \n",
            "Epoch:  1\n",
            "43/240.0 loss: 2.1209587156772614 \n",
            "Epoch:  1\n",
            "44/240.0 loss: 2.113136503431532 \n",
            "Epoch:  1\n",
            "45/240.0 loss: 2.1053392602049787 \n",
            "Epoch:  1\n",
            "46/240.0 loss: 2.0974923220086605 \n",
            "Epoch:  1\n",
            "47/240.0 loss: 2.091151130696138 \n",
            "Epoch:  1\n",
            "48/240.0 loss: 2.0844374116586177 \n",
            "Epoch:  1\n",
            "49/240.0 loss: 2.076849811077118 \n",
            "Epoch:  1\n",
            "50/240.0 loss: 2.071358928493425 \n",
            "Epoch:  1\n",
            "51/240.0 loss: 2.063247662324172 \n",
            "Epoch:  1\n",
            "52/240.0 loss: 2.0582176829284093 \n",
            "Epoch:  1\n",
            "53/240.0 loss: 2.0515951381789312 \n",
            "Epoch:  1\n",
            "54/240.0 loss: 2.0444416197863493 \n",
            "Epoch:  1\n",
            "55/240.0 loss: 2.039281579000609 \n",
            "Epoch:  1\n",
            "56/240.0 loss: 2.031173963295786 \n",
            "Epoch:  1\n",
            "57/240.0 loss: 2.026559581016672 \n",
            "Epoch:  1\n",
            "58/240.0 loss: 2.018010133403843 \n",
            "Epoch:  1\n",
            "59/240.0 loss: 2.015181469917297 \n",
            "Epoch:  1\n",
            "60/240.0 loss: 2.0103646634054964 \n",
            "Epoch:  1\n",
            "61/240.0 loss: 2.009074032306671 \n",
            "Epoch:  1\n",
            "62/240.0 loss: 2.003914469764346 \n",
            "Epoch:  1\n",
            "63/240.0 loss: 1.9996201861649752 \n",
            "Epoch:  1\n",
            "64/240.0 loss: 1.9958092707854052 \n",
            "Epoch:  1\n",
            "65/240.0 loss: 1.9886162515842554 \n",
            "Epoch:  1\n",
            "66/240.0 loss: 1.9823827761322705 \n",
            "Epoch:  1\n",
            "67/240.0 loss: 1.9787769124788397 \n",
            "Epoch:  1\n",
            "68/240.0 loss: 1.9745832197908042 \n",
            "Epoch:  1\n",
            "69/240.0 loss: 1.9689482467515127 \n",
            "Epoch:  1\n",
            "70/240.0 loss: 1.9659429822169558 \n",
            "Epoch:  1\n",
            "71/240.0 loss: 1.963118819726838 \n",
            "Epoch:  1\n",
            "72/240.0 loss: 1.9610882654581985 \n",
            "Epoch:  1\n",
            "73/240.0 loss: 1.954442882860029 \n",
            "Epoch:  1\n",
            "74/240.0 loss: 1.9482352145512898 \n",
            "Epoch:  1\n",
            "75/240.0 loss: 1.9451494044379185 \n",
            "Epoch:  1\n",
            "76/240.0 loss: 1.9406198913400823 \n",
            "Epoch:  1\n",
            "77/240.0 loss: 1.9381472651775067 \n",
            "Epoch:  1\n",
            "78/240.0 loss: 1.933580522295795 \n",
            "Epoch:  1\n",
            "79/240.0 loss: 1.9321178764104843 \n",
            "Epoch:  1\n",
            "80/240.0 loss: 1.9287140663759208 \n",
            "Epoch:  1\n",
            "81/240.0 loss: 1.9265965034322041 \n",
            "Epoch:  1\n",
            "82/240.0 loss: 1.9218492321221226 \n",
            "Epoch:  1\n",
            "83/240.0 loss: 1.9155721905685605 \n",
            "Epoch:  1\n",
            "84/240.0 loss: 1.9130236232981963 \n",
            "Epoch:  1\n",
            "85/240.0 loss: 1.9107244763263436 \n",
            "Epoch:  1\n",
            "86/240.0 loss: 1.9086653350413532 \n",
            "Epoch:  1\n",
            "87/240.0 loss: 1.9061918488957665 \n",
            "Epoch:  1\n",
            "88/240.0 loss: 1.90376395991679 \n",
            "Epoch:  1\n",
            "89/240.0 loss: 1.9004901011784872 \n",
            "Epoch:  1\n",
            "90/240.0 loss: 1.8965277894512638 \n",
            "Epoch:  1\n",
            "91/240.0 loss: 1.892779634050701 \n",
            "Epoch:  1\n",
            "92/240.0 loss: 1.8898632731488956 \n",
            "Epoch:  1\n",
            "93/240.0 loss: 1.887141562522726 \n",
            "Epoch:  1\n",
            "94/240.0 loss: 1.8836288063149704 \n",
            "Epoch:  1\n",
            "95/240.0 loss: 1.8789730208615463 \n",
            "Epoch:  1\n",
            "96/240.0 loss: 1.8762615869954689 \n",
            "Epoch:  1\n",
            "97/240.0 loss: 1.8727803303270925 \n",
            "Epoch:  1\n",
            "98/240.0 loss: 1.869204616305804 \n",
            "Epoch:  1\n",
            "99/240.0 loss: 1.8671251404285432 \n",
            "Epoch:  1\n",
            "100/240.0 loss: 1.8623321976992164 \n",
            "Epoch:  1\n",
            "101/240.0 loss: 1.8589854707904891 \n",
            "Epoch:  1\n",
            "102/240.0 loss: 1.8570902891529417 \n",
            "Epoch:  1\n",
            "103/240.0 loss: 1.8517315914997687 \n",
            "Epoch:  1\n",
            "104/240.0 loss: 1.8470448357718332 \n",
            "Epoch:  1\n",
            "105/240.0 loss: 1.841183232811262 \n",
            "Epoch:  1\n",
            "106/240.0 loss: 1.8376702549301576 \n",
            "Epoch:  1\n",
            "107/240.0 loss: 1.8342886313244149 \n",
            "Epoch:  1\n",
            "108/240.0 loss: 1.8344678605368379 \n",
            "Epoch:  1\n",
            "109/240.0 loss: 1.8313366088000211 \n",
            "Epoch:  1\n",
            "110/240.0 loss: 1.8283323425430436 \n",
            "Epoch:  1\n",
            "111/240.0 loss: 1.825248261647565 \n",
            "Epoch:  1\n",
            "112/240.0 loss: 1.823700481811456 \n",
            "Epoch:  1\n",
            "113/240.0 loss: 1.8207132492149085 \n",
            "Epoch:  1\n",
            "114/240.0 loss: 1.818161203550256 \n",
            "Epoch:  1\n",
            "115/240.0 loss: 1.8155192794470951 \n",
            "Epoch:  1\n",
            "116/240.0 loss: 1.8150280673279722 \n",
            "Epoch:  1\n",
            "117/240.0 loss: 1.8116197222370212 \n",
            "Epoch:  1\n",
            "118/240.0 loss: 1.8100897805029605 \n",
            "Epoch:  1\n",
            "119/240.0 loss: 1.8075264682372412 \n",
            "Epoch:  1\n",
            "120/240.0 loss: 1.8070365131393937 \n",
            "Epoch:  1\n",
            "121/240.0 loss: 1.8042058671107057 \n",
            "Epoch:  1\n",
            "122/240.0 loss: 1.8017275633850718 \n",
            "Epoch:  1\n",
            "123/240.0 loss: 1.7987147819611333 \n",
            "Epoch:  1\n",
            "124/240.0 loss: 1.7952273654937745 \n",
            "Epoch:  1\n",
            "125/240.0 loss: 1.7928288181622822 \n",
            "Epoch:  1\n",
            "126/240.0 loss: 1.7906074195396242 \n",
            "Epoch:  1\n",
            "127/240.0 loss: 1.7879365291446447 \n",
            "Epoch:  1\n",
            "128/240.0 loss: 1.786913128786309 \n",
            "Epoch:  1\n",
            "129/240.0 loss: 1.7846855777960557 \n",
            "Epoch:  1\n",
            "130/240.0 loss: 1.7809660152624582 \n",
            "Epoch:  1\n",
            "131/240.0 loss: 1.778458373113112 \n",
            "Epoch:  1\n",
            "132/240.0 loss: 1.7756468210005223 \n",
            "Epoch:  1\n",
            "133/240.0 loss: 1.773804234924601 \n",
            "Epoch:  1\n",
            "134/240.0 loss: 1.7725355015860664 \n",
            "Epoch:  1\n",
            "135/240.0 loss: 1.7703598483520395 \n",
            "Epoch:  1\n",
            "136/240.0 loss: 1.7677642489871839 \n",
            "Epoch:  1\n",
            "137/240.0 loss: 1.7661232645960823 \n",
            "Epoch:  1\n",
            "138/240.0 loss: 1.7645376620532798 \n",
            "Epoch:  1\n",
            "139/240.0 loss: 1.7619362601212092 \n",
            "Epoch:  1\n",
            "140/240.0 loss: 1.7595768972491541 \n",
            "Epoch:  1\n",
            "141/240.0 loss: 1.7584501336997664 \n",
            "Epoch:  1\n",
            "142/240.0 loss: 1.755105291213189 \n",
            "Epoch:  1\n",
            "143/240.0 loss: 1.7519728069504101 \n",
            "Epoch:  1\n",
            "144/240.0 loss: 1.750070652468451 \n",
            "Epoch:  1\n",
            "145/240.0 loss: 1.7469657308434787 \n",
            "Epoch:  1\n",
            "146/240.0 loss: 1.7459126183775817 \n",
            "Epoch:  1\n",
            "147/240.0 loss: 1.744657053335293 \n",
            "Epoch:  1\n",
            "148/240.0 loss: 1.7426957940095222 \n",
            "Epoch:  1\n",
            "149/240.0 loss: 1.7394011521339416 \n",
            "Epoch:  1\n",
            "150/240.0 loss: 1.7371890686995146 \n",
            "Epoch:  1\n",
            "151/240.0 loss: 1.7355847727311284 \n",
            "Epoch:  1\n",
            "152/240.0 loss: 1.7326056380677068 \n",
            "Epoch:  1\n",
            "153/240.0 loss: 1.731588761527817 \n",
            "Epoch:  1\n",
            "154/240.0 loss: 1.729290549985824 \n",
            "Epoch:  1\n",
            "155/240.0 loss: 1.728191200739298 \n",
            "Epoch:  1\n",
            "156/240.0 loss: 1.726580937197254 \n",
            "Epoch:  1\n",
            "157/240.0 loss: 1.7239118965366218 \n",
            "Epoch:  1\n",
            "158/240.0 loss: 1.721113585825986 \n",
            "Epoch:  1\n",
            "159/240.0 loss: 1.719223926216364 \n",
            "Epoch:  1\n",
            "160/240.0 loss: 1.7171231708171204 \n",
            "Epoch:  1\n",
            "161/240.0 loss: 1.7154333017490528 \n",
            "Epoch:  1\n",
            "162/240.0 loss: 1.714489997530276 \n",
            "Epoch:  1\n",
            "163/240.0 loss: 1.7121597673834823 \n",
            "Epoch:  1\n",
            "164/240.0 loss: 1.7101733937407986 \n",
            "Epoch:  1\n",
            "165/240.0 loss: 1.708108181694904 \n",
            "Epoch:  1\n",
            "166/240.0 loss: 1.7048474835778424 \n",
            "Epoch:  1\n",
            "167/240.0 loss: 1.7038178862560363 \n",
            "Epoch:  1\n",
            "168/240.0 loss: 1.702206620803246 \n",
            "Epoch:  1\n",
            "169/240.0 loss: 1.6994566068929784 \n",
            "Epoch:  1\n",
            "170/240.0 loss: 1.6982240021577355 \n",
            "Epoch:  1\n",
            "171/240.0 loss: 1.6958089560963387 \n",
            "Epoch:  1\n",
            "172/240.0 loss: 1.694669354168666 \n",
            "Epoch:  1\n",
            "173/240.0 loss: 1.6935321304989956 \n",
            "Epoch:  1\n",
            "174/240.0 loss: 1.6914154597691127 \n",
            "Epoch:  1\n",
            "175/240.0 loss: 1.6883814151991496 \n",
            "Epoch:  1\n",
            "176/240.0 loss: 1.6863811824281336 \n",
            "Epoch:  1\n",
            "177/240.0 loss: 1.6848398540796858 \n",
            "Epoch:  1\n",
            "178/240.0 loss: 1.6828869534604376 \n",
            "Epoch:  1\n",
            "179/240.0 loss: 1.6807998968495264 \n",
            "Epoch:  1\n",
            "180/240.0 loss: 1.680256779681253 \n",
            "Epoch:  1\n",
            "181/240.0 loss: 1.6782598698532187 \n",
            "Epoch:  1\n",
            "182/240.0 loss: 1.677111335139457 \n",
            "Epoch:  1\n",
            "183/240.0 loss: 1.6756386782812036 \n",
            "Epoch:  1\n",
            "184/240.0 loss: 1.6738175688563166 \n",
            "Epoch:  1\n",
            "185/240.0 loss: 1.6728073845627487 \n",
            "Epoch:  1\n",
            "186/240.0 loss: 1.6708251218744778 \n",
            "Epoch:  1\n",
            "187/240.0 loss: 1.669746570764704 \n",
            "Epoch:  1\n",
            "188/240.0 loss: 1.6676259248975724 \n",
            "Epoch:  1\n",
            "189/240.0 loss: 1.6660168666588633 \n",
            "Epoch:  1\n",
            "190/240.0 loss: 1.6648761040253164 \n",
            "Epoch:  1\n",
            "191/240.0 loss: 1.6639915108680725 \n",
            "Epoch:  1\n",
            "192/240.0 loss: 1.6612540607007673 \n",
            "Epoch:  1\n",
            "193/240.0 loss: 1.6600769782803722 \n",
            "Epoch:  1\n",
            "194/240.0 loss: 1.6583949009577432 \n",
            "Epoch:  1\n",
            "195/240.0 loss: 1.65683086915892 \n",
            "Epoch:  1\n",
            "196/240.0 loss: 1.6550824745052357 \n",
            "Epoch:  1\n",
            "197/240.0 loss: 1.6539310873156847 \n",
            "Epoch:  1\n",
            "198/240.0 loss: 1.653473429943449 \n",
            "Epoch:  1\n",
            "199/240.0 loss: 1.6522400587797166 \n",
            "Epoch:  1\n",
            "200/240.0 loss: 1.6508521897282766 \n",
            "Epoch:  1\n",
            "201/240.0 loss: 1.6496294123111386 \n",
            "Epoch:  1\n",
            "202/240.0 loss: 1.6474535512219508 \n",
            "Epoch:  1\n",
            "203/240.0 loss: 1.6456845431935554 \n",
            "Epoch:  1\n",
            "204/240.0 loss: 1.6433478332147367 \n",
            "Epoch:  1\n",
            "205/240.0 loss: 1.6407494967423597 \n",
            "Epoch:  1\n",
            "206/240.0 loss: 1.6386396643044292 \n",
            "Epoch:  1\n",
            "207/240.0 loss: 1.6373247183286226 \n",
            "Epoch:  1\n",
            "208/240.0 loss: 1.6349000765376114 \n",
            "Epoch:  1\n",
            "209/240.0 loss: 1.6332055347306387 \n",
            "Epoch:  1\n",
            "210/240.0 loss: 1.6314096157019737 \n",
            "Epoch:  1\n",
            "211/240.0 loss: 1.6299250564485226 \n",
            "Epoch:  1\n",
            "212/240.0 loss: 1.6281612495861144 \n",
            "Epoch:  1\n",
            "213/240.0 loss: 1.6265378126474184 \n",
            "Epoch:  1\n",
            "214/240.0 loss: 1.625423760192339 \n",
            "Epoch:  1\n",
            "215/240.0 loss: 1.6243260364841532 \n",
            "Epoch:  1\n",
            "216/240.0 loss: 1.622310996055603 \n",
            "Epoch:  1\n",
            "217/240.0 loss: 1.62050444379859 \n",
            "Epoch:  1\n",
            "218/240.0 loss: 1.6186495034117676 \n",
            "Epoch:  1\n",
            "219/240.0 loss: 1.6178334572098472 \n",
            "Epoch:  1\n",
            "220/240.0 loss: 1.6164426307333002 \n",
            "Epoch:  1\n",
            "221/240.0 loss: 1.615317355405103 \n",
            "Epoch:  1\n",
            "222/240.0 loss: 1.6144981849353943 \n",
            "Epoch:  1\n",
            "223/240.0 loss: 1.612944085150957 \n",
            "Epoch:  1\n",
            "224/240.0 loss: 1.6120370292663575 \n",
            "Epoch:  1\n",
            "225/240.0 loss: 1.6112384426910265 \n",
            "Epoch:  1\n",
            "226/240.0 loss: 1.6092481460865373 \n",
            "Epoch:  1\n",
            "227/240.0 loss: 1.607391444737451 \n",
            "Epoch:  1\n",
            "228/240.0 loss: 1.6059703644706693 \n",
            "Epoch:  1\n",
            "229/240.0 loss: 1.6042999666670095 \n",
            "Epoch:  1\n",
            "230/240.0 loss: 1.603037428030204 \n",
            "Epoch:  1\n",
            "231/240.0 loss: 1.6013495577820416 \n",
            "Epoch:  1\n",
            "232/240.0 loss: 1.599554592447731 \n",
            "Epoch:  1\n",
            "233/240.0 loss: 1.5980638948261228 \n",
            "Epoch:  1\n",
            "234/240.0 loss: 1.596365108895809 \n",
            "Epoch:  1\n",
            "235/240.0 loss: 1.5951338294198958 \n",
            "Epoch:  1\n",
            "236/240.0 loss: 1.5943786479249786 \n",
            "Epoch:  1\n",
            "237/240.0 loss: 1.5928666411327714 \n",
            "Epoch:  1\n",
            "238/240.0 loss: 1.5916536213463819 \n",
            "Epoch:  1\n",
            "239/240.0 loss: 1.590547494093577 \n",
            "Eval:  1\n",
            "0/80.0\n",
            "Eval:  1\n",
            "1/80.0\n",
            "Eval:  1\n",
            "2/80.0\n",
            "Eval:  1\n",
            "3/80.0\n",
            "Eval:  1\n",
            "4/80.0\n",
            "Eval:  1\n",
            "5/80.0\n",
            "Eval:  1\n",
            "6/80.0\n",
            "Eval:  1\n",
            "7/80.0\n",
            "Eval:  1\n",
            "8/80.0\n",
            "Eval:  1\n",
            "9/80.0\n",
            "Eval:  1\n",
            "10/80.0\n",
            "Eval:  1\n",
            "11/80.0\n",
            "Eval:  1\n",
            "12/80.0\n",
            "Eval:  1\n",
            "13/80.0\n",
            "Eval:  1\n",
            "14/80.0\n",
            "Eval:  1\n",
            "15/80.0\n",
            "Eval:  1\n",
            "16/80.0\n",
            "Eval:  1\n",
            "17/80.0\n",
            "Eval:  1\n",
            "18/80.0\n",
            "Eval:  1\n",
            "19/80.0\n",
            "Eval:  1\n",
            "20/80.0\n",
            "Eval:  1\n",
            "21/80.0\n",
            "Eval:  1\n",
            "22/80.0\n",
            "Eval:  1\n",
            "23/80.0\n",
            "Eval:  1\n",
            "24/80.0\n",
            "Eval:  1\n",
            "25/80.0\n",
            "Eval:  1\n",
            "26/80.0\n",
            "Eval:  1\n",
            "27/80.0\n",
            "Eval:  1\n",
            "28/80.0\n",
            "Eval:  1\n",
            "29/80.0\n",
            "Eval:  1\n",
            "30/80.0\n",
            "Eval:  1\n",
            "31/80.0\n",
            "Eval:  1\n",
            "32/80.0\n",
            "Eval:  1\n",
            "33/80.0\n",
            "Eval:  1\n",
            "34/80.0\n",
            "Eval:  1\n",
            "35/80.0\n",
            "Eval:  1\n",
            "36/80.0\n",
            "Eval:  1\n",
            "37/80.0\n",
            "Eval:  1\n",
            "38/80.0\n",
            "Eval:  1\n",
            "39/80.0\n",
            "Eval:  1\n",
            "40/80.0\n",
            "Eval:  1\n",
            "41/80.0\n",
            "Eval:  1\n",
            "42/80.0\n",
            "Eval:  1\n",
            "43/80.0\n",
            "Eval:  1\n",
            "44/80.0\n",
            "Eval:  1\n",
            "45/80.0\n",
            "Eval:  1\n",
            "46/80.0\n",
            "Eval:  1\n",
            "47/80.0\n",
            "Eval:  1\n",
            "48/80.0\n",
            "Eval:  1\n",
            "49/80.0\n",
            "Eval:  1\n",
            "50/80.0\n",
            "Eval:  1\n",
            "51/80.0\n",
            "Eval:  1\n",
            "52/80.0\n",
            "Eval:  1\n",
            "53/80.0\n",
            "Eval:  1\n",
            "54/80.0\n",
            "Eval:  1\n",
            "55/80.0\n",
            "Eval:  1\n",
            "56/80.0\n",
            "Eval:  1\n",
            "57/80.0\n",
            "Eval:  1\n",
            "58/80.0\n",
            "Eval:  1\n",
            "59/80.0\n",
            "Eval:  1\n",
            "60/80.0\n",
            "Eval:  1\n",
            "61/80.0\n",
            "Eval:  1\n",
            "62/80.0\n",
            "Eval:  1\n",
            "63/80.0\n",
            "Eval:  1\n",
            "64/80.0\n",
            "Eval:  1\n",
            "65/80.0\n",
            "Eval:  1\n",
            "66/80.0\n",
            "Eval:  1\n",
            "67/80.0\n",
            "Eval:  1\n",
            "68/80.0\n",
            "Eval:  1\n",
            "69/80.0\n",
            "Eval:  1\n",
            "70/80.0\n",
            "Eval:  1\n",
            "71/80.0\n",
            "Eval:  1\n",
            "72/80.0\n",
            "Eval:  1\n",
            "73/80.0\n",
            "Eval:  1\n",
            "74/80.0\n",
            "Eval:  1\n",
            "75/80.0\n",
            "Eval:  1\n",
            "76/80.0\n",
            "Eval:  1\n",
            "77/80.0\n",
            "Eval:  1\n",
            "78/80.0\n",
            "Eval:  1\n",
            "79/80.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        67\n",
            "           1       0.67      0.76      0.71      1206\n",
            "           2       0.50      0.01      0.02       119\n",
            "           3       0.63      0.70      0.66      1261\n",
            "           4       0.54      0.36      0.43       263\n",
            "           5       0.60      0.56      0.58      1504\n",
            "           6       0.62      0.75      0.68      1979\n",
            "           7       1.00      0.02      0.03       123\n",
            "           8       0.43      0.62      0.51       337\n",
            "           9       0.70      0.51      0.59       240\n",
            "          10       0.00      0.00      0.00        74\n",
            "          11       0.51      0.38      0.44       562\n",
            "          12       0.80      0.01      0.03       297\n",
            "          13       0.52      0.24      0.33       133\n",
            "          14       0.56      0.72      0.63      1810\n",
            "          15       0.00      0.00      0.00       120\n",
            "          16       0.00      0.00      0.00       145\n",
            "\n",
            "    accuracy                           0.60     10240\n",
            "   macro avg       0.48      0.33      0.33     10240\n",
            "weighted avg       0.58      0.60      0.56     10240\n",
            "\n",
            "Epoch:  2\n",
            "0/240.0 loss: 1.2264965772628784 \n",
            "Epoch:  2\n",
            "1/240.0 loss: 1.2990621328353882 \n",
            "Epoch:  2\n",
            "2/240.0 loss: 1.299997369448344 \n",
            "Epoch:  2\n",
            "3/240.0 loss: 1.2686575651168823 \n",
            "Epoch:  2\n",
            "4/240.0 loss: 1.2411037921905517 \n",
            "Epoch:  2\n",
            "5/240.0 loss: 1.2436959147453308 \n",
            "Epoch:  2\n",
            "6/240.0 loss: 1.2385753052575248 \n",
            "Epoch:  2\n",
            "7/240.0 loss: 1.2313554286956787 \n",
            "Epoch:  2\n",
            "8/240.0 loss: 1.2316703531477187 \n",
            "Epoch:  2\n",
            "9/240.0 loss: 1.2244981169700622 \n",
            "Epoch:  2\n",
            "10/240.0 loss: 1.2303546558726917 \n",
            "Epoch:  2\n",
            "11/240.0 loss: 1.2306757668654125 \n",
            "Epoch:  2\n",
            "12/240.0 loss: 1.2246197370382457 \n",
            "Epoch:  2\n",
            "13/240.0 loss: 1.2181851863861084 \n",
            "Epoch:  2\n",
            "14/240.0 loss: 1.223698616027832 \n",
            "Epoch:  2\n",
            "15/240.0 loss: 1.2283404842019081 \n",
            "Epoch:  2\n",
            "16/240.0 loss: 1.234092677340788 \n",
            "Epoch:  2\n",
            "17/240.0 loss: 1.2395189536942377 \n",
            "Epoch:  2\n",
            "18/240.0 loss: 1.2511171855424579 \n",
            "Epoch:  2\n",
            "19/240.0 loss: 1.2474065721035004 \n",
            "Epoch:  2\n",
            "20/240.0 loss: 1.2481221017383395 \n",
            "Epoch:  2\n",
            "21/240.0 loss: 1.2414098436182195 \n",
            "Epoch:  2\n",
            "22/240.0 loss: 1.2339672368505727 \n",
            "Epoch:  2\n",
            "23/240.0 loss: 1.233195220430692 \n",
            "Epoch:  2\n",
            "24/240.0 loss: 1.23735604763031 \n",
            "Epoch:  2\n",
            "25/240.0 loss: 1.2393021354308495 \n",
            "Epoch:  2\n",
            "26/240.0 loss: 1.2408251100116305 \n",
            "Epoch:  2\n",
            "27/240.0 loss: 1.2425520420074463 \n",
            "Epoch:  2\n",
            "28/240.0 loss: 1.2487855450860386 \n",
            "Epoch:  2\n",
            "29/240.0 loss: 1.2470569252967834 \n",
            "Epoch:  2\n",
            "30/240.0 loss: 1.2441663242155505 \n",
            "Epoch:  2\n",
            "31/240.0 loss: 1.242034275084734 \n",
            "Epoch:  2\n",
            "32/240.0 loss: 1.248343446037986 \n",
            "Epoch:  2\n",
            "33/240.0 loss: 1.2439060281304752 \n",
            "Epoch:  2\n",
            "34/240.0 loss: 1.2439213309969221 \n",
            "Epoch:  2\n",
            "35/240.0 loss: 1.2467987570497725 \n",
            "Epoch:  2\n",
            "36/240.0 loss: 1.2426729105614327 \n",
            "Epoch:  2\n",
            "37/240.0 loss: 1.2438108544600637 \n",
            "Epoch:  2\n",
            "38/240.0 loss: 1.2428131195215077 \n",
            "Epoch:  2\n",
            "39/240.0 loss: 1.242316761612892 \n",
            "Epoch:  2\n",
            "40/240.0 loss: 1.2416315485791463 \n",
            "Epoch:  2\n",
            "41/240.0 loss: 1.2376115009898232 \n",
            "Epoch:  2\n",
            "42/240.0 loss: 1.2404344774955927 \n",
            "Epoch:  2\n",
            "43/240.0 loss: 1.2393240684812719 \n",
            "Epoch:  2\n",
            "44/240.0 loss: 1.2379157967037624 \n",
            "Epoch:  2\n",
            "45/240.0 loss: 1.2353061100710994 \n",
            "Epoch:  2\n",
            "46/240.0 loss: 1.2355391801671778 \n",
            "Epoch:  2\n",
            "47/240.0 loss: 1.2304224322239559 \n",
            "Epoch:  2\n",
            "48/240.0 loss: 1.2275975747984282 \n",
            "Epoch:  2\n",
            "49/240.0 loss: 1.2286950135231018 \n",
            "Epoch:  2\n",
            "50/240.0 loss: 1.2298502150703878 \n",
            "Epoch:  2\n",
            "51/240.0 loss: 1.2277230620384216 \n",
            "Epoch:  2\n",
            "52/240.0 loss: 1.2275290241781271 \n",
            "Epoch:  2\n",
            "53/240.0 loss: 1.2267222669389513 \n",
            "Epoch:  2\n",
            "54/240.0 loss: 1.2237781459634953 \n",
            "Epoch:  2\n",
            "55/240.0 loss: 1.2226082299436842 \n",
            "Epoch:  2\n",
            "56/240.0 loss: 1.2196730856309856 \n",
            "Epoch:  2\n",
            "57/240.0 loss: 1.2200935709065404 \n",
            "Epoch:  2\n",
            "58/240.0 loss: 1.2176917225627575 \n",
            "Epoch:  2\n",
            "59/240.0 loss: 1.2182289083798727 \n",
            "Epoch:  2\n",
            "60/240.0 loss: 1.2162366186986204 \n",
            "Epoch:  2\n",
            "61/240.0 loss: 1.217590880009436 \n",
            "Epoch:  2\n",
            "62/240.0 loss: 1.216375055767241 \n",
            "Epoch:  2\n",
            "63/240.0 loss: 1.212879697792232 \n",
            "Epoch:  2\n",
            "64/240.0 loss: 1.2126942735451918 \n",
            "Epoch:  2\n",
            "65/240.0 loss: 1.208905063795321 \n",
            "Epoch:  2\n",
            "66/240.0 loss: 1.2062596358470064 \n",
            "Epoch:  2\n",
            "67/240.0 loss: 1.2057957885896458 \n",
            "Epoch:  2\n",
            "68/240.0 loss: 1.20591330960177 \n",
            "Epoch:  2\n",
            "69/240.0 loss: 1.203877536739622 \n",
            "Epoch:  2\n",
            "70/240.0 loss: 1.2035483667548275 \n",
            "Epoch:  2\n",
            "71/240.0 loss: 1.2032970860600471 \n",
            "Epoch:  2\n",
            "72/240.0 loss: 1.2026292763344228 \n",
            "Epoch:  2\n",
            "73/240.0 loss: 1.2009254685930304 \n",
            "Epoch:  2\n",
            "74/240.0 loss: 1.2003698261578877 \n",
            "Epoch:  2\n",
            "75/240.0 loss: 1.2023401127049798 \n",
            "Epoch:  2\n",
            "76/240.0 loss: 1.2013623954413772 \n",
            "Epoch:  2\n",
            "77/240.0 loss: 1.2019632451045208 \n",
            "Epoch:  2\n",
            "78/240.0 loss: 1.2000661794143388 \n",
            "Epoch:  2\n",
            "79/240.0 loss: 1.2001411236822606 \n",
            "Epoch:  2\n",
            "80/240.0 loss: 1.1999989343278201 \n",
            "Epoch:  2\n",
            "81/240.0 loss: 1.2002200171714876 \n",
            "Epoch:  2\n",
            "82/240.0 loss: 1.1982079751520271 \n",
            "Epoch:  2\n",
            "83/240.0 loss: 1.193689912557602 \n",
            "Epoch:  2\n",
            "84/240.0 loss: 1.1925331648658304 \n",
            "Epoch:  2\n",
            "85/240.0 loss: 1.1934276389521221 \n",
            "Epoch:  2\n",
            "86/240.0 loss: 1.1937897794548122 \n",
            "Epoch:  2\n",
            "87/240.0 loss: 1.1927266297015278 \n",
            "Epoch:  2\n",
            "88/240.0 loss: 1.192408513487055 \n",
            "Epoch:  2\n",
            "89/240.0 loss: 1.1919351432058547 \n",
            "Epoch:  2\n",
            "90/240.0 loss: 1.19039626959916 \n",
            "Epoch:  2\n",
            "91/240.0 loss: 1.1899215384669926 \n",
            "Epoch:  2\n",
            "92/240.0 loss: 1.190093877494976 \n",
            "Epoch:  2\n",
            "93/240.0 loss: 1.1898037106432813 \n",
            "Epoch:  2\n",
            "94/240.0 loss: 1.1892094461541427 \n",
            "Epoch:  2\n",
            "95/240.0 loss: 1.1867986184855301 \n",
            "Epoch:  2\n",
            "96/240.0 loss: 1.1847087040389936 \n",
            "Epoch:  2\n",
            "97/240.0 loss: 1.183572136017741 \n",
            "Epoch:  2\n",
            "98/240.0 loss: 1.1826330730409333 \n",
            "Epoch:  2\n",
            "99/240.0 loss: 1.182846387028694 \n",
            "Epoch:  2\n",
            "100/240.0 loss: 1.1806497685980089 \n",
            "Epoch:  2\n",
            "101/240.0 loss: 1.179984488323623 \n",
            "Epoch:  2\n",
            "102/240.0 loss: 1.1801376406428883 \n",
            "Epoch:  2\n",
            "103/240.0 loss: 1.1777558040160399 \n",
            "Epoch:  2\n",
            "104/240.0 loss: 1.1747801178977604 \n",
            "Epoch:  2\n",
            "105/240.0 loss: 1.171952542269005 \n",
            "Epoch:  2\n",
            "106/240.0 loss: 1.1705400186164356 \n",
            "Epoch:  2\n",
            "107/240.0 loss: 1.1690093345112271 \n",
            "Epoch:  2\n",
            "108/240.0 loss: 1.1694678571246087 \n",
            "Epoch:  2\n",
            "109/240.0 loss: 1.1691384575583719 \n",
            "Epoch:  2\n",
            "110/240.0 loss: 1.1682049304515392 \n",
            "Epoch:  2\n",
            "111/240.0 loss: 1.166418729616063 \n",
            "Epoch:  2\n",
            "112/240.0 loss: 1.167146802476022 \n",
            "Epoch:  2\n",
            "113/240.0 loss: 1.1676239125561296 \n",
            "Epoch:  2\n",
            "114/240.0 loss: 1.1681118804475537 \n",
            "Epoch:  2\n",
            "115/240.0 loss: 1.1678106481659 \n",
            "Epoch:  2\n",
            "116/240.0 loss: 1.1690566127116864 \n",
            "Epoch:  2\n",
            "117/240.0 loss: 1.1681913395049208 \n",
            "Epoch:  2\n",
            "118/240.0 loss: 1.1674492203888773 \n",
            "Epoch:  2\n",
            "119/240.0 loss: 1.1661244387427965 \n",
            "Epoch:  2\n",
            "120/240.0 loss: 1.1669158644912656 \n",
            "Epoch:  2\n",
            "121/240.0 loss: 1.165713031272419 \n",
            "Epoch:  2\n",
            "122/240.0 loss: 1.1650045184585136 \n",
            "Epoch:  2\n",
            "123/240.0 loss: 1.1640689714301018 \n",
            "Epoch:  2\n",
            "124/240.0 loss: 1.1628719353675843 \n",
            "Epoch:  2\n",
            "125/240.0 loss: 1.1613561761757685 \n",
            "Epoch:  2\n",
            "126/240.0 loss: 1.1612170385563467 \n",
            "Epoch:  2\n",
            "127/240.0 loss: 1.160856839735061 \n",
            "Epoch:  2\n",
            "128/240.0 loss: 1.160375172315642 \n",
            "Epoch:  2\n",
            "129/240.0 loss: 1.1601795815504514 \n",
            "Epoch:  2\n",
            "130/240.0 loss: 1.1586554077745395 \n",
            "Epoch:  2\n",
            "131/240.0 loss: 1.1582485536734264 \n",
            "Epoch:  2\n",
            "132/240.0 loss: 1.1576711432378095 \n",
            "Epoch:  2\n",
            "133/240.0 loss: 1.1575503616190668 \n",
            "Epoch:  2\n",
            "134/240.0 loss: 1.1581644155361035 \n",
            "Epoch:  2\n",
            "135/240.0 loss: 1.1573096145601833 \n",
            "Epoch:  2\n",
            "136/240.0 loss: 1.156729929638605 \n",
            "Epoch:  2\n",
            "137/240.0 loss: 1.156631200209908 \n",
            "Epoch:  2\n",
            "138/240.0 loss: 1.155854250887315 \n",
            "Epoch:  2\n",
            "139/240.0 loss: 1.155057794707162 \n",
            "Epoch:  2\n",
            "140/240.0 loss: 1.1548220722387867 \n",
            "Epoch:  2\n",
            "141/240.0 loss: 1.1548583625068127 \n",
            "Epoch:  2\n",
            "142/240.0 loss: 1.15408042927722 \n",
            "Epoch:  2\n",
            "143/240.0 loss: 1.1524797160592344 \n",
            "Epoch:  2\n",
            "144/240.0 loss: 1.151966732534869 \n",
            "Epoch:  2\n",
            "145/240.0 loss: 1.151125148959356 \n",
            "Epoch:  2\n",
            "146/240.0 loss: 1.1518752319472176 \n",
            "Epoch:  2\n",
            "147/240.0 loss: 1.1518583817256463 \n",
            "Epoch:  2\n",
            "148/240.0 loss: 1.1512918276274764 \n",
            "Epoch:  2\n",
            "149/240.0 loss: 1.149066819747289 \n",
            "Epoch:  2\n",
            "150/240.0 loss: 1.148734091528204 \n",
            "Epoch:  2\n",
            "151/240.0 loss: 1.1485616474559432 \n",
            "Epoch:  2\n",
            "152/240.0 loss: 1.1465457112960566 \n",
            "Epoch:  2\n",
            "153/240.0 loss: 1.1471248536140888 \n",
            "Epoch:  2\n",
            "154/240.0 loss: 1.146746003627777 \n",
            "Epoch:  2\n",
            "155/240.0 loss: 1.1471260086848185 \n",
            "Epoch:  2\n",
            "156/240.0 loss: 1.14673602163412 \n",
            "Epoch:  2\n",
            "157/240.0 loss: 1.1451136484930786 \n",
            "Epoch:  2\n",
            "158/240.0 loss: 1.1441552129181676 \n",
            "Epoch:  2\n",
            "159/240.0 loss: 1.1437789022922515 \n",
            "Epoch:  2\n",
            "160/240.0 loss: 1.1426438030248844 \n",
            "Epoch:  2\n",
            "161/240.0 loss: 1.1422277037744168 \n",
            "Epoch:  2\n",
            "162/240.0 loss: 1.1417421647376078 \n",
            "Epoch:  2\n",
            "163/240.0 loss: 1.140657934473782 \n",
            "Epoch:  2\n",
            "164/240.0 loss: 1.140083253022396 \n",
            "Epoch:  2\n",
            "165/240.0 loss: 1.1390205991555409 \n",
            "Epoch:  2\n",
            "166/240.0 loss: 1.1374808785444248 \n",
            "Epoch:  2\n",
            "167/240.0 loss: 1.1374285852625257 \n",
            "Epoch:  2\n",
            "168/240.0 loss: 1.1374080759533765 \n",
            "Epoch:  2\n",
            "169/240.0 loss: 1.1364130230510936 \n",
            "Epoch:  2\n",
            "170/240.0 loss: 1.1359605517303735 \n",
            "Epoch:  2\n",
            "171/240.0 loss: 1.13510980578356 \n",
            "Epoch:  2\n",
            "172/240.0 loss: 1.1353593458330011 \n",
            "Epoch:  2\n",
            "173/240.0 loss: 1.1356534690692508 \n",
            "Epoch:  2\n",
            "174/240.0 loss: 1.135558386530195 \n",
            "Epoch:  2\n",
            "175/240.0 loss: 1.1339925524186005 \n",
            "Epoch:  2\n",
            "176/240.0 loss: 1.1337973057886974 \n",
            "Epoch:  2\n",
            "177/240.0 loss: 1.133378270301926 \n",
            "Epoch:  2\n",
            "178/240.0 loss: 1.1328502177526165 \n",
            "Epoch:  2\n",
            "179/240.0 loss: 1.1323048495584065 \n",
            "Epoch:  2\n",
            "180/240.0 loss: 1.1321716535815876 \n",
            "Epoch:  2\n",
            "181/240.0 loss: 1.1315776165370102 \n",
            "Epoch:  2\n",
            "182/240.0 loss: 1.1316810849585819 \n",
            "Epoch:  2\n",
            "183/240.0 loss: 1.13162579646577 \n",
            "Epoch:  2\n",
            "184/240.0 loss: 1.130986225605011 \n",
            "Epoch:  2\n",
            "185/240.0 loss: 1.1314482185789334 \n",
            "Epoch:  2\n",
            "186/240.0 loss: 1.1309504843650655 \n",
            "Epoch:  2\n",
            "187/240.0 loss: 1.1308942015500778 \n",
            "Epoch:  2\n",
            "188/240.0 loss: 1.129371303099173 \n",
            "Epoch:  2\n",
            "189/240.0 loss: 1.1290194203979091 \n",
            "Epoch:  2\n",
            "190/240.0 loss: 1.1285662913197623 \n",
            "Epoch:  2\n",
            "191/240.0 loss: 1.1284946668893099 \n",
            "Epoch:  2\n",
            "192/240.0 loss: 1.1266469427341006 \n",
            "Epoch:  2\n",
            "193/240.0 loss: 1.126578935335592 \n",
            "Epoch:  2\n",
            "194/240.0 loss: 1.1260884832113216 \n",
            "Epoch:  2\n",
            "195/240.0 loss: 1.126036680170468 \n",
            "Epoch:  2\n",
            "196/240.0 loss: 1.1253720670182088 \n",
            "Epoch:  2\n",
            "197/240.0 loss: 1.1252659170916586 \n",
            "Epoch:  2\n",
            "198/240.0 loss: 1.1260396778284005 \n",
            "Epoch:  2\n",
            "199/240.0 loss: 1.126061207950115 \n",
            "Epoch:  2\n",
            "200/240.0 loss: 1.1257067269353724 \n",
            "Epoch:  2\n",
            "201/240.0 loss: 1.1253387653591609 \n",
            "Epoch:  2\n",
            "202/240.0 loss: 1.123836914306791 \n",
            "Epoch:  2\n",
            "203/240.0 loss: 1.122916479321087 \n",
            "Epoch:  2\n",
            "204/240.0 loss: 1.1218620686996275 \n",
            "Epoch:  2\n",
            "205/240.0 loss: 1.1207510157696252 \n",
            "Epoch:  2\n",
            "206/240.0 loss: 1.1200397031894629 \n",
            "Epoch:  2\n",
            "207/240.0 loss: 1.1195841007507765 \n",
            "Epoch:  2\n",
            "208/240.0 loss: 1.1188438779999765 \n",
            "Epoch:  2\n",
            "209/240.0 loss: 1.1181182268119993 \n",
            "Epoch:  2\n",
            "210/240.0 loss: 1.1175796621218677 \n",
            "Epoch:  2\n",
            "211/240.0 loss: 1.117648646235466 \n",
            "Epoch:  2\n",
            "212/240.0 loss: 1.1174102240884807 \n",
            "Epoch:  2\n",
            "213/240.0 loss: 1.1169333755970001 \n",
            "Epoch:  2\n",
            "214/240.0 loss: 1.1172073744064153 \n",
            "Epoch:  2\n",
            "215/240.0 loss: 1.1172513981108312 \n",
            "Epoch:  2\n",
            "216/240.0 loss: 1.1167591175725382 \n",
            "Epoch:  2\n",
            "217/240.0 loss: 1.116354694333645 \n",
            "Epoch:  2\n",
            "218/240.0 loss: 1.115601093257399 \n",
            "Epoch:  2\n",
            "219/240.0 loss: 1.1162232821637934 \n",
            "Epoch:  2\n",
            "220/240.0 loss: 1.1161966566586388 \n",
            "Epoch:  2\n",
            "221/240.0 loss: 1.116382611227465 \n",
            "Epoch:  2\n",
            "222/240.0 loss: 1.1166799843578594 \n",
            "Epoch:  2\n",
            "223/240.0 loss: 1.1161231284162827 \n",
            "Epoch:  2\n",
            "224/240.0 loss: 1.1162184670236375 \n",
            "Epoch:  2\n",
            "225/240.0 loss: 1.116532011105951 \n",
            "Epoch:  2\n",
            "226/240.0 loss: 1.1156514898795913 \n",
            "Epoch:  2\n",
            "227/240.0 loss: 1.1147776829046117 \n",
            "Epoch:  2\n",
            "228/240.0 loss: 1.1144359718243628 \n",
            "Epoch:  2\n",
            "229/240.0 loss: 1.1141476245030113 \n",
            "Epoch:  2\n",
            "230/240.0 loss: 1.1136506829426918 \n",
            "Epoch:  2\n",
            "231/240.0 loss: 1.112653587398858 \n",
            "Epoch:  2\n",
            "232/240.0 loss: 1.1122232859738395 \n",
            "Epoch:  2\n",
            "233/240.0 loss: 1.1116841891382494 \n",
            "Epoch:  2\n",
            "234/240.0 loss: 1.1109559444670982 \n",
            "Epoch:  2\n",
            "235/240.0 loss: 1.1104888148226981 \n",
            "Epoch:  2\n",
            "236/240.0 loss: 1.1106273056585578 \n",
            "Epoch:  2\n",
            "237/240.0 loss: 1.110042268989467 \n",
            "Epoch:  2\n",
            "238/240.0 loss: 1.1099656411294658 \n",
            "Epoch:  2\n",
            "239/240.0 loss: 1.1094692622621853 \n",
            "Eval:  2\n",
            "0/80.0\n",
            "Eval:  2\n",
            "1/80.0\n",
            "Eval:  2\n",
            "2/80.0\n",
            "Eval:  2\n",
            "3/80.0\n",
            "Eval:  2\n",
            "4/80.0\n",
            "Eval:  2\n",
            "5/80.0\n",
            "Eval:  2\n",
            "6/80.0\n",
            "Eval:  2\n",
            "7/80.0\n",
            "Eval:  2\n",
            "8/80.0\n",
            "Eval:  2\n",
            "9/80.0\n",
            "Eval:  2\n",
            "10/80.0\n",
            "Eval:  2\n",
            "11/80.0\n",
            "Eval:  2\n",
            "12/80.0\n",
            "Eval:  2\n",
            "13/80.0\n",
            "Eval:  2\n",
            "14/80.0\n",
            "Eval:  2\n",
            "15/80.0\n",
            "Eval:  2\n",
            "16/80.0\n",
            "Eval:  2\n",
            "17/80.0\n",
            "Eval:  2\n",
            "18/80.0\n",
            "Eval:  2\n",
            "19/80.0\n",
            "Eval:  2\n",
            "20/80.0\n",
            "Eval:  2\n",
            "21/80.0\n",
            "Eval:  2\n",
            "22/80.0\n",
            "Eval:  2\n",
            "23/80.0\n",
            "Eval:  2\n",
            "24/80.0\n",
            "Eval:  2\n",
            "25/80.0\n",
            "Eval:  2\n",
            "26/80.0\n",
            "Eval:  2\n",
            "27/80.0\n",
            "Eval:  2\n",
            "28/80.0\n",
            "Eval:  2\n",
            "29/80.0\n",
            "Eval:  2\n",
            "30/80.0\n",
            "Eval:  2\n",
            "31/80.0\n",
            "Eval:  2\n",
            "32/80.0\n",
            "Eval:  2\n",
            "33/80.0\n",
            "Eval:  2\n",
            "34/80.0\n",
            "Eval:  2\n",
            "35/80.0\n",
            "Eval:  2\n",
            "36/80.0\n",
            "Eval:  2\n",
            "37/80.0\n",
            "Eval:  2\n",
            "38/80.0\n",
            "Eval:  2\n",
            "39/80.0\n",
            "Eval:  2\n",
            "40/80.0\n",
            "Eval:  2\n",
            "41/80.0\n",
            "Eval:  2\n",
            "42/80.0\n",
            "Eval:  2\n",
            "43/80.0\n",
            "Eval:  2\n",
            "44/80.0\n",
            "Eval:  2\n",
            "45/80.0\n",
            "Eval:  2\n",
            "46/80.0\n",
            "Eval:  2\n",
            "47/80.0\n",
            "Eval:  2\n",
            "48/80.0\n",
            "Eval:  2\n",
            "49/80.0\n",
            "Eval:  2\n",
            "50/80.0\n",
            "Eval:  2\n",
            "51/80.0\n",
            "Eval:  2\n",
            "52/80.0\n",
            "Eval:  2\n",
            "53/80.0\n",
            "Eval:  2\n",
            "54/80.0\n",
            "Eval:  2\n",
            "55/80.0\n",
            "Eval:  2\n",
            "56/80.0\n",
            "Eval:  2\n",
            "57/80.0\n",
            "Eval:  2\n",
            "58/80.0\n",
            "Eval:  2\n",
            "59/80.0\n",
            "Eval:  2\n",
            "60/80.0\n",
            "Eval:  2\n",
            "61/80.0\n",
            "Eval:  2\n",
            "62/80.0\n",
            "Eval:  2\n",
            "63/80.0\n",
            "Eval:  2\n",
            "64/80.0\n",
            "Eval:  2\n",
            "65/80.0\n",
            "Eval:  2\n",
            "66/80.0\n",
            "Eval:  2\n",
            "67/80.0\n",
            "Eval:  2\n",
            "68/80.0\n",
            "Eval:  2\n",
            "69/80.0\n",
            "Eval:  2\n",
            "70/80.0\n",
            "Eval:  2\n",
            "71/80.0\n",
            "Eval:  2\n",
            "72/80.0\n",
            "Eval:  2\n",
            "73/80.0\n",
            "Eval:  2\n",
            "74/80.0\n",
            "Eval:  2\n",
            "75/80.0\n",
            "Eval:  2\n",
            "76/80.0\n",
            "Eval:  2\n",
            "77/80.0\n",
            "Eval:  2\n",
            "78/80.0\n",
            "Eval:  2\n",
            "79/80.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.10      0.18        67\n",
            "           1       0.71      0.75      0.73      1206\n",
            "           2       0.54      0.45      0.49       119\n",
            "           3       0.67      0.70      0.69      1261\n",
            "           4       0.61      0.47      0.53       263\n",
            "           5       0.63      0.65      0.64      1504\n",
            "           6       0.68      0.74      0.71      1979\n",
            "           7       0.47      0.19      0.27       123\n",
            "           8       0.45      0.70      0.55       337\n",
            "           9       0.72      0.66      0.69       240\n",
            "          10       0.53      0.11      0.18        74\n",
            "          11       0.57      0.45      0.50       562\n",
            "          12       0.71      0.11      0.20       297\n",
            "          13       0.57      0.37      0.45       133\n",
            "          14       0.61      0.72      0.66      1810\n",
            "          15       0.62      0.44      0.52       120\n",
            "          16       0.00      0.00      0.00       145\n",
            "\n",
            "    accuracy                           0.64     10240\n",
            "   macro avg       0.57      0.45      0.47     10240\n",
            "weighted avg       0.63      0.64      0.62     10240\n",
            "\n",
            "Epoch:  3\n",
            "0/240.0 loss: 1.008541226387024 \n",
            "Epoch:  3\n",
            "1/240.0 loss: 1.0438879132270813 \n",
            "Epoch:  3\n",
            "2/240.0 loss: 1.0382027228673298 \n",
            "Epoch:  3\n",
            "3/240.0 loss: 1.0153818726539612 \n",
            "Epoch:  3\n",
            "4/240.0 loss: 0.9893567204475403 \n",
            "Epoch:  3\n",
            "5/240.0 loss: 0.9717860817909241 \n",
            "Epoch:  3\n",
            "6/240.0 loss: 0.9698929701532636 \n",
            "Epoch:  3\n",
            "7/240.0 loss: 0.9728620052337646 \n",
            "Epoch:  3\n",
            "8/240.0 loss: 0.97519142097897 \n",
            "Epoch:  3\n",
            "9/240.0 loss: 0.9657001733779907 \n",
            "Epoch:  3\n",
            "10/240.0 loss: 0.9733894846656106 \n",
            "Epoch:  3\n",
            "11/240.0 loss: 0.9697172443072001 \n",
            "Epoch:  3\n",
            "12/240.0 loss: 0.9721939013554499 \n",
            "Epoch:  3\n",
            "13/240.0 loss: 0.9621431146349225 \n",
            "Epoch:  3\n",
            "14/240.0 loss: 0.9728225787480672 \n",
            "Epoch:  3\n",
            "15/240.0 loss: 0.9774238839745522 \n",
            "Epoch:  3\n",
            "16/240.0 loss: 0.9813690816654879 \n",
            "Epoch:  3\n",
            "17/240.0 loss: 0.9852644668685065 \n",
            "Epoch:  3\n",
            "18/240.0 loss: 0.9958972868166471 \n",
            "Epoch:  3\n",
            "19/240.0 loss: 0.9916150897741318 \n",
            "Epoch:  3\n",
            "20/240.0 loss: 0.9907219608624777 \n",
            "Epoch:  3\n",
            "21/240.0 loss: 0.987604634328322 \n",
            "Epoch:  3\n",
            "22/240.0 loss: 0.9822085266527922 \n",
            "Epoch:  3\n",
            "23/240.0 loss: 0.981890303393205 \n",
            "Epoch:  3\n",
            "24/240.0 loss: 0.9847748017311097 \n",
            "Epoch:  3\n",
            "25/240.0 loss: 0.9843524075471438 \n",
            "Epoch:  3\n",
            "26/240.0 loss: 0.9832659054685522 \n",
            "Epoch:  3\n",
            "27/240.0 loss: 0.9846455561263221 \n",
            "Epoch:  3\n",
            "28/240.0 loss: 0.9897599487469114 \n",
            "Epoch:  3\n",
            "29/240.0 loss: 0.9878682692845663 \n",
            "Epoch:  3\n",
            "30/240.0 loss: 0.9856693437022548 \n",
            "Epoch:  3\n",
            "31/240.0 loss: 0.9836754165589809 \n",
            "Epoch:  3\n",
            "32/240.0 loss: 0.9900215791933464 \n",
            "Epoch:  3\n",
            "33/240.0 loss: 0.9878600800738615 \n",
            "Epoch:  3\n",
            "34/240.0 loss: 0.9879551376615252 \n",
            "Epoch:  3\n",
            "35/240.0 loss: 0.9898452758789062 \n",
            "Epoch:  3\n",
            "36/240.0 loss: 0.9865944562731562 \n",
            "Epoch:  3\n",
            "37/240.0 loss: 0.9873428172186801 \n",
            "Epoch:  3\n",
            "38/240.0 loss: 0.9859033654897641 \n",
            "Epoch:  3\n",
            "39/240.0 loss: 0.9872005298733711 \n",
            "Epoch:  3\n",
            "40/240.0 loss: 0.9868970731409584 \n",
            "Epoch:  3\n",
            "41/240.0 loss: 0.9841306181181044 \n",
            "Epoch:  3\n",
            "42/240.0 loss: 0.9827793060347091 \n",
            "Epoch:  3\n",
            "43/240.0 loss: 0.982082415710796 \n",
            "Epoch:  3\n",
            "44/240.0 loss: 0.981461415025923 \n",
            "Epoch:  3\n",
            "45/240.0 loss: 0.9800880408805349 \n",
            "Epoch:  3\n",
            "46/240.0 loss: 0.9811626609335554 \n",
            "Epoch:  3\n",
            "47/240.0 loss: 0.9765799703697363 \n",
            "Epoch:  3\n",
            "48/240.0 loss: 0.974369084348484 \n",
            "Epoch:  3\n",
            "49/240.0 loss: 0.9757033240795135 \n",
            "Epoch:  3\n",
            "50/240.0 loss: 0.9774759809176127 \n",
            "Epoch:  3\n",
            "51/240.0 loss: 0.9768600979676614 \n",
            "Epoch:  3\n",
            "52/240.0 loss: 0.9766034270232579 \n",
            "Epoch:  3\n",
            "53/240.0 loss: 0.9749180486908665 \n",
            "Epoch:  3\n",
            "54/240.0 loss: 0.9733042847026478 \n",
            "Epoch:  3\n",
            "55/240.0 loss: 0.9715755667005267 \n",
            "Epoch:  3\n",
            "56/240.0 loss: 0.9692683115340116 \n",
            "Epoch:  3\n",
            "57/240.0 loss: 0.9710501679058733 \n",
            "Epoch:  3\n",
            "58/240.0 loss: 0.9707539344238023 \n",
            "Epoch:  3\n",
            "59/240.0 loss: 0.9710285772879919 \n",
            "Epoch:  3\n",
            "60/240.0 loss: 0.969873453749985 \n",
            "Epoch:  3\n",
            "61/240.0 loss: 0.9704324737671883 \n",
            "Epoch:  3\n",
            "62/240.0 loss: 0.9691425826814439 \n",
            "Epoch:  3\n",
            "63/240.0 loss: 0.9667770517989993 \n",
            "Epoch:  3\n",
            "64/240.0 loss: 0.9660175140087421 \n",
            "Epoch:  3\n",
            "65/240.0 loss: 0.9626267516251766 \n",
            "Epoch:  3\n",
            "66/240.0 loss: 0.9604582599739531 \n",
            "Epoch:  3\n",
            "67/240.0 loss: 0.9605587510501638 \n",
            "Epoch:  3\n",
            "68/240.0 loss: 0.9612767575443655 \n",
            "Epoch:  3\n",
            "69/240.0 loss: 0.960273677110672 \n",
            "Epoch:  3\n",
            "70/240.0 loss: 0.9609016507444247 \n",
            "Epoch:  3\n",
            "71/240.0 loss: 0.9610476502113872 \n",
            "Epoch:  3\n",
            "72/240.0 loss: 0.9603536308628239 \n",
            "Epoch:  3\n",
            "73/240.0 loss: 0.9590233822126646 \n",
            "Epoch:  3\n",
            "74/240.0 loss: 0.9584004433949789 \n",
            "Epoch:  3\n",
            "75/240.0 loss: 0.9605805073913775 \n",
            "Epoch:  3\n",
            "76/240.0 loss: 0.9596108987733916 \n",
            "Epoch:  3\n",
            "77/240.0 loss: 0.9605766825186901 \n",
            "Epoch:  3\n",
            "78/240.0 loss: 0.9585565781291527 \n",
            "Epoch:  3\n",
            "79/240.0 loss: 0.9586858205497265 \n",
            "Epoch:  3\n",
            "80/240.0 loss: 0.9581011747136529 \n",
            "Epoch:  3\n",
            "81/240.0 loss: 0.9582029283046722 \n",
            "Epoch:  3\n",
            "82/240.0 loss: 0.9566937972264118 \n",
            "Epoch:  3\n",
            "83/240.0 loss: 0.95242326600211 \n",
            "Epoch:  3\n",
            "84/240.0 loss: 0.9505197959787706 \n",
            "Epoch:  3\n",
            "85/240.0 loss: 0.9515207659366519 \n",
            "Epoch:  3\n",
            "86/240.0 loss: 0.9515934627631615 \n",
            "Epoch:  3\n",
            "87/240.0 loss: 0.9505893831903284 \n",
            "Epoch:  3\n",
            "88/240.0 loss: 0.9506520582049081 \n",
            "Epoch:  3\n",
            "89/240.0 loss: 0.9499955912431081 \n",
            "Epoch:  3\n",
            "90/240.0 loss: 0.9483130069879385 \n",
            "Epoch:  3\n",
            "91/240.0 loss: 0.9487870866837709 \n",
            "Epoch:  3\n",
            "92/240.0 loss: 0.9490098376427928 \n",
            "Epoch:  3\n",
            "93/240.0 loss: 0.9489347909359221 \n",
            "Epoch:  3\n",
            "94/240.0 loss: 0.9484026758294356 \n",
            "Epoch:  3\n",
            "95/240.0 loss: 0.9460037127137184 \n",
            "Epoch:  3\n",
            "96/240.0 loss: 0.9444721998627653 \n",
            "Epoch:  3\n",
            "97/240.0 loss: 0.9442656125341143 \n",
            "Epoch:  3\n",
            "98/240.0 loss: 0.9437679419613848 \n",
            "Epoch:  3\n",
            "99/240.0 loss: 0.944355211853981 \n",
            "Epoch:  3\n",
            "100/240.0 loss: 0.9437089940108875 \n",
            "Epoch:  3\n",
            "101/240.0 loss: 0.9429057836532593 \n",
            "Epoch:  3\n",
            "102/240.0 loss: 0.9429950777766773 \n",
            "Epoch:  3\n",
            "103/240.0 loss: 0.9416375504090235 \n",
            "Epoch:  3\n",
            "104/240.0 loss: 0.9395624189149766 \n",
            "Epoch:  3\n",
            "105/240.0 loss: 0.9370472515529057 \n",
            "Epoch:  3\n",
            "106/240.0 loss: 0.9362438326684114 \n",
            "Epoch:  3\n",
            "107/240.0 loss: 0.9344094809558656 \n",
            "Epoch:  3\n",
            "108/240.0 loss: 0.9353292748468731 \n",
            "Epoch:  3\n",
            "109/240.0 loss: 0.9349190224300731 \n",
            "Epoch:  3\n",
            "110/240.0 loss: 0.9337902906778697 \n",
            "Epoch:  3\n",
            "111/240.0 loss: 0.9320360917065825 \n",
            "Epoch:  3\n",
            "112/240.0 loss: 0.9322839606124743 \n",
            "Epoch:  3\n",
            "113/240.0 loss: 0.931987641673339 \n",
            "Epoch:  3\n",
            "114/240.0 loss: 0.931941454825194 \n",
            "Epoch:  3\n",
            "115/240.0 loss: 0.9321023775585766 \n",
            "Epoch:  3\n",
            "116/240.0 loss: 0.9337278028838655 \n",
            "Epoch:  3\n",
            "117/240.0 loss: 0.9333385673619933 \n",
            "Epoch:  3\n",
            "118/240.0 loss: 0.9333805587111401 \n",
            "Epoch:  3\n",
            "119/240.0 loss: 0.9323169698317846 \n",
            "Epoch:  3\n",
            "120/240.0 loss: 0.9332581719091116 \n",
            "Epoch:  3\n",
            "121/240.0 loss: 0.9320759939365699 \n",
            "Epoch:  3\n",
            "122/240.0 loss: 0.9316092899175195 \n",
            "Epoch:  3\n",
            "123/240.0 loss: 0.9305308759212494 \n",
            "Epoch:  3\n",
            "124/240.0 loss: 0.9294770636558533 \n",
            "Epoch:  3\n",
            "125/240.0 loss: 0.9284084335206046 \n",
            "Epoch:  3\n",
            "126/240.0 loss: 0.9283779118004747 \n",
            "Epoch:  3\n",
            "127/240.0 loss: 0.9287845892831683 \n",
            "Epoch:  3\n",
            "128/240.0 loss: 0.9285455683405085 \n",
            "Epoch:  3\n",
            "129/240.0 loss: 0.9288345781656412 \n",
            "Epoch:  3\n",
            "130/240.0 loss: 0.9273480750222243 \n",
            "Epoch:  3\n",
            "131/240.0 loss: 0.9274244958704169 \n",
            "Epoch:  3\n",
            "132/240.0 loss: 0.927559773276623 \n",
            "Epoch:  3\n",
            "133/240.0 loss: 0.9271173339281509 \n",
            "Epoch:  3\n",
            "134/240.0 loss: 0.9279061065779792 \n",
            "Epoch:  3\n",
            "135/240.0 loss: 0.9277383737704333 \n",
            "Epoch:  3\n",
            "136/240.0 loss: 0.9276927405030188 \n",
            "Epoch:  3\n",
            "137/240.0 loss: 0.9279228759848553 \n",
            "Epoch:  3\n",
            "138/240.0 loss: 0.9270358561611861 \n",
            "Epoch:  3\n",
            "139/240.0 loss: 0.926400061590331 \n",
            "Epoch:  3\n",
            "140/240.0 loss: 0.9265928412160129 \n",
            "Epoch:  3\n",
            "141/240.0 loss: 0.9268454451796034 \n",
            "Epoch:  3\n",
            "142/240.0 loss: 0.9264994888872533 \n",
            "Epoch:  3\n",
            "143/240.0 loss: 0.9255468452142345 \n",
            "Epoch:  3\n",
            "144/240.0 loss: 0.9248476307967614 \n",
            "Epoch:  3\n",
            "145/240.0 loss: 0.9245434346264356 \n",
            "Epoch:  3\n",
            "146/240.0 loss: 0.9256670904808304 \n",
            "Epoch:  3\n",
            "147/240.0 loss: 0.9259270224216822 \n",
            "Epoch:  3\n",
            "148/240.0 loss: 0.9259060277234787 \n",
            "Epoch:  3\n",
            "149/240.0 loss: 0.9237044699986776 \n",
            "Epoch:  3\n",
            "150/240.0 loss: 0.9237714536142665 \n",
            "Epoch:  3\n",
            "151/240.0 loss: 0.9237178618970671 \n",
            "Epoch:  3\n",
            "152/240.0 loss: 0.9221324468749801 \n",
            "Epoch:  3\n",
            "153/240.0 loss: 0.9225433975845189 \n",
            "Epoch:  3\n",
            "154/240.0 loss: 0.9221576502246241 \n",
            "Epoch:  3\n",
            "155/240.0 loss: 0.9229945559532214 \n",
            "Epoch:  3\n",
            "156/240.0 loss: 0.9225496304262976 \n",
            "Epoch:  3\n",
            "157/240.0 loss: 0.9211369111568113 \n",
            "Epoch:  3\n",
            "158/240.0 loss: 0.9202467398073688 \n",
            "Epoch:  3\n",
            "159/240.0 loss: 0.9203801300376654 \n",
            "Epoch:  3\n",
            "160/240.0 loss: 0.9192286615045915 \n",
            "Epoch:  3\n",
            "161/240.0 loss: 0.9190809907000742 \n",
            "Epoch:  3\n",
            "162/240.0 loss: 0.9189923884678473 \n",
            "Epoch:  3\n",
            "163/240.0 loss: 0.918295170475797 \n",
            "Epoch:  3\n",
            "164/240.0 loss: 0.9171985510623817 \n",
            "Epoch:  3\n",
            "165/240.0 loss: 0.9162732621273363 \n",
            "Epoch:  3\n",
            "166/240.0 loss: 0.9150803838661331 \n",
            "Epoch:  3\n",
            "167/240.0 loss: 0.9148547021406037 \n",
            "Epoch:  3\n",
            "168/240.0 loss: 0.9147805952461514 \n",
            "Epoch:  3\n",
            "169/240.0 loss: 0.9141995605300455 \n",
            "Epoch:  3\n",
            "170/240.0 loss: 0.9138800400739525 \n",
            "Epoch:  3\n",
            "171/240.0 loss: 0.9134348319020382 \n",
            "Epoch:  3\n",
            "172/240.0 loss: 0.9136538298832888 \n",
            "Epoch:  3\n",
            "173/240.0 loss: 0.914171986196233 \n",
            "Epoch:  3\n",
            "174/240.0 loss: 0.9145512775012425 \n",
            "Epoch:  3\n",
            "175/240.0 loss: 0.9133603440766985 \n",
            "Epoch:  3\n",
            "176/240.0 loss: 0.9133029903395701 \n",
            "Epoch:  3\n",
            "177/240.0 loss: 0.9129694546206614 \n",
            "Epoch:  3\n",
            "178/240.0 loss: 0.9127451271318191 \n",
            "Epoch:  3\n",
            "179/240.0 loss: 0.9126583702034421 \n",
            "Epoch:  3\n",
            "180/240.0 loss: 0.9127115202213519 \n",
            "Epoch:  3\n",
            "181/240.0 loss: 0.912766851239152 \n",
            "Epoch:  3\n",
            "182/240.0 loss: 0.913171615756926 \n",
            "Epoch:  3\n",
            "183/240.0 loss: 0.9129858567662861 \n",
            "Epoch:  3\n",
            "184/240.0 loss: 0.9122449916762274 \n",
            "Epoch:  3\n",
            "185/240.0 loss: 0.9128206131919738 \n",
            "Epoch:  3\n",
            "186/240.0 loss: 0.9126654146189358 \n",
            "Epoch:  3\n",
            "187/240.0 loss: 0.9127724303844127 \n",
            "Epoch:  3\n",
            "188/240.0 loss: 0.9119144851568515 \n",
            "Epoch:  3\n",
            "189/240.0 loss: 0.9116929383654343 \n",
            "Epoch:  3\n",
            "190/240.0 loss: 0.9114228234241146 \n",
            "Epoch:  3\n",
            "191/240.0 loss: 0.9113526552294692 \n",
            "Epoch:  3\n",
            "192/240.0 loss: 0.90999697063871 \n",
            "Epoch:  3\n",
            "193/240.0 loss: 0.9099041960903049 \n",
            "Epoch:  3\n",
            "194/240.0 loss: 0.9095348975597284 \n",
            "Epoch:  3\n",
            "195/240.0 loss: 0.9096687460432247 \n",
            "Epoch:  3\n",
            "196/240.0 loss: 0.9093761081017818 \n",
            "Epoch:  3\n",
            "197/240.0 loss: 0.9096063017243087 \n",
            "Epoch:  3\n",
            "198/240.0 loss: 0.9107227094808416 \n",
            "Epoch:  3\n",
            "199/240.0 loss: 0.9104687893390655 \n",
            "Epoch:  3\n",
            "200/240.0 loss: 0.9103611698791162 \n",
            "Epoch:  3\n",
            "201/240.0 loss: 0.9098121789422365 \n",
            "Epoch:  3\n",
            "202/240.0 loss: 0.9085240648885079 \n",
            "Epoch:  3\n",
            "203/240.0 loss: 0.9077347867629108 \n",
            "Epoch:  3\n",
            "204/240.0 loss: 0.9069055696813072 \n",
            "Epoch:  3\n",
            "205/240.0 loss: 0.9061078443689253 \n",
            "Epoch:  3\n",
            "206/240.0 loss: 0.9057241064914758 \n",
            "Epoch:  3\n",
            "207/240.0 loss: 0.9052577064587519 \n",
            "Epoch:  3\n",
            "208/240.0 loss: 0.9044931730585235 \n",
            "Epoch:  3\n",
            "209/240.0 loss: 0.9039498099258968 \n",
            "Epoch:  3\n",
            "210/240.0 loss: 0.9037238243631842 \n",
            "Epoch:  3\n",
            "211/240.0 loss: 0.9037590785971228 \n",
            "Epoch:  3\n",
            "212/240.0 loss: 0.9035003814898747 \n",
            "Epoch:  3\n",
            "213/240.0 loss: 0.903266210700864 \n",
            "Epoch:  3\n",
            "214/240.0 loss: 0.9036512810130451 \n",
            "Epoch:  3\n",
            "215/240.0 loss: 0.9039193640152613 \n",
            "Epoch:  3\n",
            "216/240.0 loss: 0.9037096533357822 \n",
            "Epoch:  3\n",
            "217/240.0 loss: 0.9034911722218225 \n",
            "Epoch:  3\n",
            "218/240.0 loss: 0.902906777651887 \n",
            "Epoch:  3\n",
            "219/240.0 loss: 0.903839589249004 \n",
            "Epoch:  3\n",
            "220/240.0 loss: 0.9040271917619317 \n",
            "Epoch:  3\n",
            "221/240.0 loss: 0.9044442721852312 \n",
            "Epoch:  3\n",
            "222/240.0 loss: 0.904637830674381 \n",
            "Epoch:  3\n",
            "223/240.0 loss: 0.9043521716126374 \n",
            "Epoch:  3\n",
            "224/240.0 loss: 0.904831502172682 \n",
            "Epoch:  3\n",
            "225/240.0 loss: 0.9051175605406804 \n",
            "Epoch:  3\n",
            "226/240.0 loss: 0.9047070962216885 \n",
            "Epoch:  3\n",
            "227/240.0 loss: 0.9037973982723135 \n",
            "Epoch:  3\n",
            "228/240.0 loss: 0.903601242204941 \n",
            "Epoch:  3\n",
            "229/240.0 loss: 0.90391020645266 \n",
            "Epoch:  3\n",
            "230/240.0 loss: 0.9036414721311429 \n",
            "Epoch:  3\n",
            "231/240.0 loss: 0.9029871173973741 \n",
            "Epoch:  3\n",
            "232/240.0 loss: 0.9026150905523177 \n",
            "Epoch:  3\n",
            "233/240.0 loss: 0.9021244181527032 \n",
            "Epoch:  3\n",
            "234/240.0 loss: 0.9018618284387792 \n",
            "Epoch:  3\n",
            "235/240.0 loss: 0.9016471793085842 \n",
            "Epoch:  3\n",
            "236/240.0 loss: 0.9019195289048465 \n",
            "Epoch:  3\n",
            "237/240.0 loss: 0.9012440260217971 \n",
            "Epoch:  3\n",
            "238/240.0 loss: 0.9011173609909153 \n",
            "Epoch:  3\n",
            "239/240.0 loss: 0.9006283596158028 \n",
            "Eval:  3\n",
            "0/80.0\n",
            "Eval:  3\n",
            "1/80.0\n",
            "Eval:  3\n",
            "2/80.0\n",
            "Eval:  3\n",
            "3/80.0\n",
            "Eval:  3\n",
            "4/80.0\n",
            "Eval:  3\n",
            "5/80.0\n",
            "Eval:  3\n",
            "6/80.0\n",
            "Eval:  3\n",
            "7/80.0\n",
            "Eval:  3\n",
            "8/80.0\n",
            "Eval:  3\n",
            "9/80.0\n",
            "Eval:  3\n",
            "10/80.0\n",
            "Eval:  3\n",
            "11/80.0\n",
            "Eval:  3\n",
            "12/80.0\n",
            "Eval:  3\n",
            "13/80.0\n",
            "Eval:  3\n",
            "14/80.0\n",
            "Eval:  3\n",
            "15/80.0\n",
            "Eval:  3\n",
            "16/80.0\n",
            "Eval:  3\n",
            "17/80.0\n",
            "Eval:  3\n",
            "18/80.0\n",
            "Eval:  3\n",
            "19/80.0\n",
            "Eval:  3\n",
            "20/80.0\n",
            "Eval:  3\n",
            "21/80.0\n",
            "Eval:  3\n",
            "22/80.0\n",
            "Eval:  3\n",
            "23/80.0\n",
            "Eval:  3\n",
            "24/80.0\n",
            "Eval:  3\n",
            "25/80.0\n",
            "Eval:  3\n",
            "26/80.0\n",
            "Eval:  3\n",
            "27/80.0\n",
            "Eval:  3\n",
            "28/80.0\n",
            "Eval:  3\n",
            "29/80.0\n",
            "Eval:  3\n",
            "30/80.0\n",
            "Eval:  3\n",
            "31/80.0\n",
            "Eval:  3\n",
            "32/80.0\n",
            "Eval:  3\n",
            "33/80.0\n",
            "Eval:  3\n",
            "34/80.0\n",
            "Eval:  3\n",
            "35/80.0\n",
            "Eval:  3\n",
            "36/80.0\n",
            "Eval:  3\n",
            "37/80.0\n",
            "Eval:  3\n",
            "38/80.0\n",
            "Eval:  3\n",
            "39/80.0\n",
            "Eval:  3\n",
            "40/80.0\n",
            "Eval:  3\n",
            "41/80.0\n",
            "Eval:  3\n",
            "42/80.0\n",
            "Eval:  3\n",
            "43/80.0\n",
            "Eval:  3\n",
            "44/80.0\n",
            "Eval:  3\n",
            "45/80.0\n",
            "Eval:  3\n",
            "46/80.0\n",
            "Eval:  3\n",
            "47/80.0\n",
            "Eval:  3\n",
            "48/80.0\n",
            "Eval:  3\n",
            "49/80.0\n",
            "Eval:  3\n",
            "50/80.0\n",
            "Eval:  3\n",
            "51/80.0\n",
            "Eval:  3\n",
            "52/80.0\n",
            "Eval:  3\n",
            "53/80.0\n",
            "Eval:  3\n",
            "54/80.0\n",
            "Eval:  3\n",
            "55/80.0\n",
            "Eval:  3\n",
            "56/80.0\n",
            "Eval:  3\n",
            "57/80.0\n",
            "Eval:  3\n",
            "58/80.0\n",
            "Eval:  3\n",
            "59/80.0\n",
            "Eval:  3\n",
            "60/80.0\n",
            "Eval:  3\n",
            "61/80.0\n",
            "Eval:  3\n",
            "62/80.0\n",
            "Eval:  3\n",
            "63/80.0\n",
            "Eval:  3\n",
            "64/80.0\n",
            "Eval:  3\n",
            "65/80.0\n",
            "Eval:  3\n",
            "66/80.0\n",
            "Eval:  3\n",
            "67/80.0\n",
            "Eval:  3\n",
            "68/80.0\n",
            "Eval:  3\n",
            "69/80.0\n",
            "Eval:  3\n",
            "70/80.0\n",
            "Eval:  3\n",
            "71/80.0\n",
            "Eval:  3\n",
            "72/80.0\n",
            "Eval:  3\n",
            "73/80.0\n",
            "Eval:  3\n",
            "74/80.0\n",
            "Eval:  3\n",
            "75/80.0\n",
            "Eval:  3\n",
            "76/80.0\n",
            "Eval:  3\n",
            "77/80.0\n",
            "Eval:  3\n",
            "78/80.0\n",
            "Eval:  3\n",
            "79/80.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.36      0.44        67\n",
            "           1       0.72      0.74      0.73      1206\n",
            "           2       0.50      0.49      0.49       119\n",
            "           3       0.69      0.68      0.69      1261\n",
            "           4       0.63      0.51      0.56       263\n",
            "           5       0.64      0.66      0.65      1504\n",
            "           6       0.68      0.77      0.72      1979\n",
            "           7       0.38      0.30      0.34       123\n",
            "           8       0.50      0.72      0.59       337\n",
            "           9       0.72      0.68      0.70       240\n",
            "          10       0.54      0.19      0.28        74\n",
            "          11       0.66      0.38      0.48       562\n",
            "          12       0.70      0.13      0.22       297\n",
            "          13       0.55      0.38      0.45       133\n",
            "          14       0.61      0.75      0.67      1810\n",
            "          15       0.76      0.40      0.52       120\n",
            "          16       0.25      0.01      0.01       145\n",
            "\n",
            "    accuracy                           0.65     10240\n",
            "   macro avg       0.59      0.48      0.50     10240\n",
            "weighted avg       0.65      0.65      0.63     10240\n",
            "\n",
            "Epoch:  4\n",
            "0/240.0 loss: 0.7480396628379822 \n",
            "Epoch:  4\n",
            "1/240.0 loss: 0.864787220954895 \n",
            "Epoch:  4\n",
            "2/240.0 loss: 0.8708113034566244 \n",
            "Epoch:  4\n",
            "3/240.0 loss: 0.8643121123313904 \n",
            "Epoch:  4\n",
            "4/240.0 loss: 0.8431803226470947 \n",
            "Epoch:  4\n",
            "5/240.0 loss: 0.8296075761318207 \n",
            "Epoch:  4\n",
            "6/240.0 loss: 0.8299056461879185 \n",
            "Epoch:  4\n",
            "7/240.0 loss: 0.8243876174092293 \n",
            "Epoch:  4\n",
            "8/240.0 loss: 0.8191991978221469 \n",
            "Epoch:  4\n",
            "9/240.0 loss: 0.8078367710113525 \n",
            "Epoch:  4\n",
            "10/240.0 loss: 0.8097340356219899 \n",
            "Epoch:  4\n",
            "11/240.0 loss: 0.8092070370912552 \n",
            "Epoch:  4\n",
            "12/240.0 loss: 0.8069041829842788 \n",
            "Epoch:  4\n",
            "13/240.0 loss: 0.7976587244442531 \n",
            "Epoch:  4\n",
            "14/240.0 loss: 0.8014130512873332 \n",
            "Epoch:  4\n",
            "15/240.0 loss: 0.8023788928985596 \n",
            "Epoch:  4\n",
            "16/240.0 loss: 0.8046549137900857 \n",
            "Epoch:  4\n",
            "17/240.0 loss: 0.8061872786945767 \n",
            "Epoch:  4\n",
            "18/240.0 loss: 0.8151577993443138 \n",
            "Epoch:  4\n",
            "19/240.0 loss: 0.8100141733884811 \n",
            "Epoch:  4\n",
            "20/240.0 loss: 0.8079784767968314 \n",
            "Epoch:  4\n",
            "21/240.0 loss: 0.8052305308255282 \n",
            "Epoch:  4\n",
            "22/240.0 loss: 0.799218589844911 \n",
            "Epoch:  4\n",
            "23/240.0 loss: 0.7975175554553667 \n",
            "Epoch:  4\n",
            "24/240.0 loss: 0.8005216145515441 \n",
            "Epoch:  4\n",
            "25/240.0 loss: 0.7986353314839877 \n",
            "Epoch:  4\n",
            "26/240.0 loss: 0.7972595316392405 \n",
            "Epoch:  4\n",
            "27/240.0 loss: 0.7988202358995166 \n",
            "Epoch:  4\n",
            "28/240.0 loss: 0.8060381289186149 \n",
            "Epoch:  4\n",
            "29/240.0 loss: 0.8041617155075074 \n",
            "Epoch:  4\n",
            "30/240.0 loss: 0.7994895923522211 \n",
            "Epoch:  4\n",
            "31/240.0 loss: 0.7986733689904213 \n",
            "Epoch:  4\n",
            "32/240.0 loss: 0.8054224830685239 \n",
            "Epoch:  4\n",
            "33/240.0 loss: 0.8026321916019216 \n",
            "Epoch:  4\n",
            "34/240.0 loss: 0.8028806873730251 \n",
            "Epoch:  4\n",
            "35/240.0 loss: 0.8030383437871933 \n",
            "Epoch:  4\n",
            "36/240.0 loss: 0.800138178709391 \n",
            "Epoch:  4\n",
            "37/240.0 loss: 0.8000020823980633 \n",
            "Epoch:  4\n",
            "38/240.0 loss: 0.7984045912057925 \n",
            "Epoch:  4\n",
            "39/240.0 loss: 0.7997373804450035 \n",
            "Epoch:  4\n",
            "40/240.0 loss: 0.7993382738857735 \n",
            "Epoch:  4\n",
            "41/240.0 loss: 0.7967379689216614 \n",
            "Epoch:  4\n",
            "42/240.0 loss: 0.7943931610085243 \n",
            "Epoch:  4\n",
            "43/240.0 loss: 0.7934287569739602 \n",
            "Epoch:  4\n",
            "44/240.0 loss: 0.7926773932245043 \n",
            "Epoch:  4\n",
            "45/240.0 loss: 0.7927457249682882 \n",
            "Epoch:  4\n",
            "46/240.0 loss: 0.7935595804072441 \n",
            "Epoch:  4\n",
            "47/240.0 loss: 0.7888322820266088 \n",
            "Epoch:  4\n",
            "48/240.0 loss: 0.7879314787533819 \n",
            "Epoch:  4\n",
            "49/240.0 loss: 0.7886038541793823 \n",
            "Epoch:  4\n",
            "50/240.0 loss: 0.7895581511890187 \n",
            "Epoch:  4\n",
            "51/240.0 loss: 0.7883451741475326 \n",
            "Epoch:  4\n",
            "52/240.0 loss: 0.7880855517567329 \n",
            "Epoch:  4\n",
            "53/240.0 loss: 0.7874901669996756 \n",
            "Epoch:  4\n",
            "54/240.0 loss: 0.7862407630140131 \n",
            "Epoch:  4\n",
            "55/240.0 loss: 0.785718464425632 \n",
            "Epoch:  4\n",
            "56/240.0 loss: 0.7836699370752301 \n",
            "Epoch:  4\n",
            "57/240.0 loss: 0.7853604729833275 \n",
            "Epoch:  4\n",
            "58/240.0 loss: 0.7851586331755428 \n",
            "Epoch:  4\n",
            "59/240.0 loss: 0.7853915164868037 \n",
            "Epoch:  4\n",
            "60/240.0 loss: 0.7846971847971932 \n",
            "Epoch:  4\n",
            "61/240.0 loss: 0.7849998935576408 \n",
            "Epoch:  4\n",
            "62/240.0 loss: 0.7848637321638683 \n",
            "Epoch:  4\n",
            "63/240.0 loss: 0.7834963882341981 \n",
            "Epoch:  4\n",
            "64/240.0 loss: 0.7831541198950548 \n",
            "Epoch:  4\n",
            "65/240.0 loss: 0.7804258366425832 \n",
            "Epoch:  4\n",
            "66/240.0 loss: 0.7786449523114446 \n",
            "Epoch:  4\n",
            "67/240.0 loss: 0.7787253918016658 \n",
            "Epoch:  4\n",
            "68/240.0 loss: 0.7790449581284454 \n",
            "Epoch:  4\n",
            "69/240.0 loss: 0.7773049448217665 \n",
            "Epoch:  4\n",
            "70/240.0 loss: 0.7779971839676441 \n",
            "Epoch:  4\n",
            "71/240.0 loss: 0.7797001360191239 \n",
            "Epoch:  4\n",
            "72/240.0 loss: 0.7797952661775562 \n",
            "Epoch:  4\n",
            "73/240.0 loss: 0.7788335922602061 \n",
            "Epoch:  4\n",
            "74/240.0 loss: 0.7780575466156006 \n",
            "Epoch:  4\n",
            "75/240.0 loss: 0.7799672751050246 \n",
            "Epoch:  4\n",
            "76/240.0 loss: 0.7800223161647846 \n",
            "Epoch:  4\n",
            "77/240.0 loss: 0.7817407510219476 \n",
            "Epoch:  4\n",
            "78/240.0 loss: 0.7801173993303806 \n",
            "Epoch:  4\n",
            "79/240.0 loss: 0.7795267321169377 \n",
            "Epoch:  4\n",
            "80/240.0 loss: 0.7794821453683171 \n",
            "Epoch:  4\n",
            "81/240.0 loss: 0.779291858760322 \n",
            "Epoch:  4\n",
            "82/240.0 loss: 0.7778106834515032 \n",
            "Epoch:  4\n",
            "83/240.0 loss: 0.7739916408345813 \n",
            "Epoch:  4\n",
            "84/240.0 loss: 0.7718553942792555 \n",
            "Epoch:  4\n",
            "85/240.0 loss: 0.77261978803679 \n",
            "Epoch:  4\n",
            "86/240.0 loss: 0.7723391446573981 \n",
            "Epoch:  4\n",
            "87/240.0 loss: 0.7716002877462994 \n",
            "Epoch:  4\n",
            "88/240.0 loss: 0.7723484454530009 \n",
            "Epoch:  4\n",
            "89/240.0 loss: 0.7713115427229139 \n",
            "Epoch:  4\n",
            "90/240.0 loss: 0.7698422434565785 \n",
            "Epoch:  4\n",
            "91/240.0 loss: 0.7702022259650023 \n",
            "Epoch:  4\n",
            "92/240.0 loss: 0.7697998849294518 \n",
            "Epoch:  4\n",
            "93/240.0 loss: 0.769445114313288 \n",
            "Epoch:  4\n",
            "94/240.0 loss: 0.7684050076886227 \n",
            "Epoch:  4\n",
            "95/240.0 loss: 0.7671163206299146 \n",
            "Epoch:  4\n",
            "96/240.0 loss: 0.7648902486280068 \n",
            "Epoch:  4\n",
            "97/240.0 loss: 0.7646833165567748 \n",
            "Epoch:  4\n",
            "98/240.0 loss: 0.7640758289231194 \n",
            "Epoch:  4\n",
            "99/240.0 loss: 0.7645436245203018 \n",
            "Epoch:  4\n",
            "100/240.0 loss: 0.7638904481831164 \n",
            "Epoch:  4\n",
            "101/240.0 loss: 0.7628543330173866 \n",
            "Epoch:  4\n",
            "102/240.0 loss: 0.7629271253798772 \n",
            "Epoch:  4\n",
            "103/240.0 loss: 0.7614430458499835 \n",
            "Epoch:  4\n",
            "104/240.0 loss: 0.7595350322269258 \n",
            "Epoch:  4\n",
            "105/240.0 loss: 0.7575913234701697 \n",
            "Epoch:  4\n",
            "106/240.0 loss: 0.7565930658411757 \n",
            "Epoch:  4\n",
            "107/240.0 loss: 0.7554607799759617 \n",
            "Epoch:  4\n",
            "108/240.0 loss: 0.7558259619485348 \n",
            "Epoch:  4\n",
            "109/240.0 loss: 0.755374058268287 \n",
            "Epoch:  4\n",
            "110/240.0 loss: 0.7543440058424666 \n",
            "Epoch:  4\n",
            "111/240.0 loss: 0.7534045789922986 \n",
            "Epoch:  4\n",
            "112/240.0 loss: 0.7530399381586935 \n",
            "Epoch:  4\n",
            "113/240.0 loss: 0.7526974338188506 \n",
            "Epoch:  4\n",
            "114/240.0 loss: 0.7529615593993145 \n",
            "Epoch:  4\n",
            "115/240.0 loss: 0.7527432554754717 \n",
            "Epoch:  4\n",
            "116/240.0 loss: 0.7543106293066953 \n",
            "Epoch:  4\n",
            "117/240.0 loss: 0.753536273867397 \n",
            "Epoch:  4\n",
            "118/240.0 loss: 0.7536383005751282 \n",
            "Epoch:  4\n",
            "119/240.0 loss: 0.7525458559393883 \n",
            "Epoch:  4\n",
            "120/240.0 loss: 0.7529571583448362 \n",
            "Epoch:  4\n",
            "121/240.0 loss: 0.7517694254390529 \n",
            "Epoch:  4\n",
            "122/240.0 loss: 0.7518567670651568 \n",
            "Epoch:  4\n",
            "123/240.0 loss: 0.7512622687124437 \n",
            "Epoch:  4\n",
            "124/240.0 loss: 0.7509918742179871 \n",
            "Epoch:  4\n",
            "125/240.0 loss: 0.7502208167598361 \n",
            "Epoch:  4\n",
            "126/240.0 loss: 0.750416078905421 \n",
            "Epoch:  4\n",
            "127/240.0 loss: 0.7507353508844972 \n",
            "Epoch:  4\n",
            "128/240.0 loss: 0.750099451966988 \n",
            "Epoch:  4\n",
            "129/240.0 loss: 0.7510778706807356 \n",
            "Epoch:  4\n",
            "130/240.0 loss: 0.749545414029187 \n",
            "Epoch:  4\n",
            "131/240.0 loss: 0.7502685342774247 \n",
            "Epoch:  4\n",
            "132/240.0 loss: 0.7503106629938111 \n",
            "Epoch:  4\n",
            "133/240.0 loss: 0.7499007902928253 \n",
            "Epoch:  4\n",
            "134/240.0 loss: 0.7504738489786784 \n",
            "Epoch:  4\n",
            "135/240.0 loss: 0.750421974588843 \n",
            "Epoch:  4\n",
            "136/240.0 loss: 0.7501297419088601 \n",
            "Epoch:  4\n",
            "137/240.0 loss: 0.750065832466319 \n",
            "Epoch:  4\n",
            "138/240.0 loss: 0.7487793551932136 \n",
            "Epoch:  4\n",
            "139/240.0 loss: 0.7484298471893583 \n",
            "Epoch:  4\n",
            "140/240.0 loss: 0.7483629250357337 \n",
            "Epoch:  4\n",
            "141/240.0 loss: 0.7487961179773572 \n",
            "Epoch:  4\n",
            "142/240.0 loss: 0.7484642483971335 \n",
            "Epoch:  4\n",
            "143/240.0 loss: 0.7475754221280416 \n",
            "Epoch:  4\n",
            "144/240.0 loss: 0.7471135061362694 \n",
            "Epoch:  4\n",
            "145/240.0 loss: 0.7469468006532486 \n",
            "Epoch:  4\n",
            "146/240.0 loss: 0.7481869645670157 \n",
            "Epoch:  4\n",
            "147/240.0 loss: 0.748080182719875 \n",
            "Epoch:  4\n",
            "148/240.0 loss: 0.7478626190415965 \n",
            "Epoch:  4\n",
            "149/240.0 loss: 0.7459495218594869 \n",
            "Epoch:  4\n",
            "150/240.0 loss: 0.7460592029899951 \n",
            "Epoch:  4\n",
            "151/240.0 loss: 0.745889549976901 \n",
            "Epoch:  4\n",
            "152/240.0 loss: 0.7447012639513203 \n",
            "Epoch:  4\n",
            "153/240.0 loss: 0.7447602222492169 \n",
            "Epoch:  4\n",
            "154/240.0 loss: 0.7444753308450022 \n",
            "Epoch:  4\n",
            "155/240.0 loss: 0.7449165219679857 \n",
            "Epoch:  4\n",
            "156/240.0 loss: 0.7443473430196191 \n",
            "Epoch:  4\n",
            "157/240.0 loss: 0.7429499271549757 \n",
            "Epoch:  4\n",
            "158/240.0 loss: 0.7419449263398752 \n",
            "Epoch:  4\n",
            "159/240.0 loss: 0.7420274373143911 \n",
            "Epoch:  4\n",
            "160/240.0 loss: 0.7412159605796292 \n",
            "Epoch:  4\n",
            "161/240.0 loss: 0.7407241548285072 \n",
            "Epoch:  4\n",
            "162/240.0 loss: 0.7405148715329316 \n",
            "Epoch:  4\n",
            "163/240.0 loss: 0.7398251520424355 \n",
            "Epoch:  4\n",
            "164/240.0 loss: 0.7387730049364495 \n",
            "Epoch:  4\n",
            "165/240.0 loss: 0.737637328096183 \n",
            "Epoch:  4\n",
            "166/240.0 loss: 0.7365146511329148 \n",
            "Epoch:  4\n",
            "167/240.0 loss: 0.7365018746682576 \n",
            "Epoch:  4\n",
            "168/240.0 loss: 0.7361515587603552 \n",
            "Epoch:  4\n",
            "169/240.0 loss: 0.7358274985762203 \n",
            "Epoch:  4\n",
            "170/240.0 loss: 0.7357648415872228 \n",
            "Epoch:  4\n",
            "171/240.0 loss: 0.735425237999406 \n",
            "Epoch:  4\n",
            "172/240.0 loss: 0.7357707526642463 \n",
            "Epoch:  4\n",
            "173/240.0 loss: 0.7362182976185591 \n",
            "Epoch:  4\n",
            "174/240.0 loss: 0.7363430976867675 \n",
            "Epoch:  4\n",
            "175/240.0 loss: 0.7351734479042616 \n",
            "Epoch:  4\n",
            "176/240.0 loss: 0.7348215782036216 \n",
            "Epoch:  4\n",
            "177/240.0 loss: 0.7346336094850905 \n",
            "Epoch:  4\n",
            "178/240.0 loss: 0.7345769971442622 \n",
            "Epoch:  4\n",
            "179/240.0 loss: 0.7340684162245856 \n",
            "Epoch:  4\n",
            "180/240.0 loss: 0.7339440205479195 \n",
            "Epoch:  4\n",
            "181/240.0 loss: 0.7338547421680702 \n",
            "Epoch:  4\n",
            "182/240.0 loss: 0.7337381116679458 \n",
            "Epoch:  4\n",
            "183/240.0 loss: 0.7334106525649196 \n",
            "Epoch:  4\n",
            "184/240.0 loss: 0.7330674870594128 \n",
            "Epoch:  4\n",
            "185/240.0 loss: 0.7337838046012386 \n",
            "Epoch:  4\n",
            "186/240.0 loss: 0.7340537739947518 \n",
            "Epoch:  4\n",
            "187/240.0 loss: 0.7342698044599371 \n",
            "Epoch:  4\n",
            "188/240.0 loss: 0.7334704846932144 \n",
            "Epoch:  4\n",
            "189/240.0 loss: 0.7332306513660832 \n",
            "Epoch:  4\n",
            "190/240.0 loss: 0.7331794322473216 \n",
            "Epoch:  4\n",
            "191/240.0 loss: 0.7332221906011304 \n",
            "Epoch:  4\n",
            "192/240.0 loss: 0.732263781246126 \n",
            "Epoch:  4\n",
            "193/240.0 loss: 0.732323503678607 \n",
            "Epoch:  4\n",
            "194/240.0 loss: 0.7319652046912756 \n",
            "Epoch:  4\n",
            "195/240.0 loss: 0.7322670160507669 \n",
            "Epoch:  4\n",
            "196/240.0 loss: 0.7320982901578023 \n",
            "Epoch:  4\n",
            "197/240.0 loss: 0.7325623258195743 \n",
            "Epoch:  4\n",
            "198/240.0 loss: 0.7336942972849362 \n",
            "Epoch:  4\n",
            "199/240.0 loss: 0.733797028362751 \n",
            "Epoch:  4\n",
            "200/240.0 loss: 0.7337016780578082 \n",
            "Epoch:  4\n",
            "201/240.0 loss: 0.7329521730984792 \n",
            "Epoch:  4\n",
            "202/240.0 loss: 0.7316318353995901 \n",
            "Epoch:  4\n",
            "203/240.0 loss: 0.7305388184739094 \n",
            "Epoch:  4\n",
            "204/240.0 loss: 0.729978630600906 \n",
            "Epoch:  4\n",
            "205/240.0 loss: 0.7293281532028346 \n",
            "Epoch:  4\n",
            "206/240.0 loss: 0.7290881552558014 \n",
            "Epoch:  4\n",
            "207/240.0 loss: 0.7285538241267204 \n",
            "Epoch:  4\n",
            "208/240.0 loss: 0.7278501217445118 \n",
            "Epoch:  4\n",
            "209/240.0 loss: 0.7277849393231528 \n",
            "Epoch:  4\n",
            "210/240.0 loss: 0.7278541106183382 \n",
            "Epoch:  4\n",
            "211/240.0 loss: 0.7279009245476633 \n",
            "Epoch:  4\n",
            "212/240.0 loss: 0.7275789556368976 \n",
            "Epoch:  4\n",
            "213/240.0 loss: 0.7271404475252204 \n",
            "Epoch:  4\n",
            "214/240.0 loss: 0.7274502185888069 \n",
            "Epoch:  4\n",
            "215/240.0 loss: 0.7275908694223121 \n",
            "Epoch:  4\n",
            "216/240.0 loss: 0.7278950005632392 \n",
            "Epoch:  4\n",
            "217/240.0 loss: 0.72813556117749 \n",
            "Epoch:  4\n",
            "218/240.0 loss: 0.7276757228864382 \n",
            "Epoch:  4\n",
            "219/240.0 loss: 0.7284458406946875 \n",
            "Epoch:  4\n",
            "220/240.0 loss: 0.7284501791000366 \n",
            "Epoch:  4\n",
            "221/240.0 loss: 0.7288465558945596 \n",
            "Epoch:  4\n",
            "222/240.0 loss: 0.7290371041661421 \n",
            "Epoch:  4\n",
            "223/240.0 loss: 0.7284621290330376 \n",
            "Epoch:  4\n",
            "224/240.0 loss: 0.7291770739025539 \n",
            "Epoch:  4\n",
            "225/240.0 loss: 0.7295057156444651 \n",
            "Epoch:  4\n",
            "226/240.0 loss: 0.7291879971647053 \n",
            "Epoch:  4\n",
            "227/240.0 loss: 0.7287291607312989 \n",
            "Epoch:  4\n",
            "228/240.0 loss: 0.72852374606778 \n",
            "Epoch:  4\n",
            "229/240.0 loss: 0.728645136045373 \n",
            "Epoch:  4\n",
            "230/240.0 loss: 0.7285227393691158 \n",
            "Epoch:  4\n",
            "231/240.0 loss: 0.7279096962562923 \n",
            "Epoch:  4\n",
            "232/240.0 loss: 0.7275061635500372 \n",
            "Epoch:  4\n",
            "233/240.0 loss: 0.7271259618111146 \n",
            "Epoch:  4\n",
            "234/240.0 loss: 0.726748952206145 \n",
            "Epoch:  4\n",
            "235/240.0 loss: 0.7268854690305258 \n",
            "Epoch:  4\n",
            "236/240.0 loss: 0.7273102254304202 \n",
            "Epoch:  4\n",
            "237/240.0 loss: 0.7267760862322414 \n",
            "Epoch:  4\n",
            "238/240.0 loss: 0.7267162091562439 \n",
            "Epoch:  4\n",
            "239/240.0 loss: 0.7264176957309246 \n",
            "Eval:  4\n",
            "0/80.0\n",
            "Eval:  4\n",
            "1/80.0\n",
            "Eval:  4\n",
            "2/80.0\n",
            "Eval:  4\n",
            "3/80.0\n",
            "Eval:  4\n",
            "4/80.0\n",
            "Eval:  4\n",
            "5/80.0\n",
            "Eval:  4\n",
            "6/80.0\n",
            "Eval:  4\n",
            "7/80.0\n",
            "Eval:  4\n",
            "8/80.0\n",
            "Eval:  4\n",
            "9/80.0\n",
            "Eval:  4\n",
            "10/80.0\n",
            "Eval:  4\n",
            "11/80.0\n",
            "Eval:  4\n",
            "12/80.0\n",
            "Eval:  4\n",
            "13/80.0\n",
            "Eval:  4\n",
            "14/80.0\n",
            "Eval:  4\n",
            "15/80.0\n",
            "Eval:  4\n",
            "16/80.0\n",
            "Eval:  4\n",
            "17/80.0\n",
            "Eval:  4\n",
            "18/80.0\n",
            "Eval:  4\n",
            "19/80.0\n",
            "Eval:  4\n",
            "20/80.0\n",
            "Eval:  4\n",
            "21/80.0\n",
            "Eval:  4\n",
            "22/80.0\n",
            "Eval:  4\n",
            "23/80.0\n",
            "Eval:  4\n",
            "24/80.0\n",
            "Eval:  4\n",
            "25/80.0\n",
            "Eval:  4\n",
            "26/80.0\n",
            "Eval:  4\n",
            "27/80.0\n",
            "Eval:  4\n",
            "28/80.0\n",
            "Eval:  4\n",
            "29/80.0\n",
            "Eval:  4\n",
            "30/80.0\n",
            "Eval:  4\n",
            "31/80.0\n",
            "Eval:  4\n",
            "32/80.0\n",
            "Eval:  4\n",
            "33/80.0\n",
            "Eval:  4\n",
            "34/80.0\n",
            "Eval:  4\n",
            "35/80.0\n",
            "Eval:  4\n",
            "36/80.0\n",
            "Eval:  4\n",
            "37/80.0\n",
            "Eval:  4\n",
            "38/80.0\n",
            "Eval:  4\n",
            "39/80.0\n",
            "Eval:  4\n",
            "40/80.0\n",
            "Eval:  4\n",
            "41/80.0\n",
            "Eval:  4\n",
            "42/80.0\n",
            "Eval:  4\n",
            "43/80.0\n",
            "Eval:  4\n",
            "44/80.0\n",
            "Eval:  4\n",
            "45/80.0\n",
            "Eval:  4\n",
            "46/80.0\n",
            "Eval:  4\n",
            "47/80.0\n",
            "Eval:  4\n",
            "48/80.0\n",
            "Eval:  4\n",
            "49/80.0\n",
            "Eval:  4\n",
            "50/80.0\n",
            "Eval:  4\n",
            "51/80.0\n",
            "Eval:  4\n",
            "52/80.0\n",
            "Eval:  4\n",
            "53/80.0\n",
            "Eval:  4\n",
            "54/80.0\n",
            "Eval:  4\n",
            "55/80.0\n",
            "Eval:  4\n",
            "56/80.0\n",
            "Eval:  4\n",
            "57/80.0\n",
            "Eval:  4\n",
            "58/80.0\n",
            "Eval:  4\n",
            "59/80.0\n",
            "Eval:  4\n",
            "60/80.0\n",
            "Eval:  4\n",
            "61/80.0\n",
            "Eval:  4\n",
            "62/80.0\n",
            "Eval:  4\n",
            "63/80.0\n",
            "Eval:  4\n",
            "64/80.0\n",
            "Eval:  4\n",
            "65/80.0\n",
            "Eval:  4\n",
            "66/80.0\n",
            "Eval:  4\n",
            "67/80.0\n",
            "Eval:  4\n",
            "68/80.0\n",
            "Eval:  4\n",
            "69/80.0\n",
            "Eval:  4\n",
            "70/80.0\n",
            "Eval:  4\n",
            "71/80.0\n",
            "Eval:  4\n",
            "72/80.0\n",
            "Eval:  4\n",
            "73/80.0\n",
            "Eval:  4\n",
            "74/80.0\n",
            "Eval:  4\n",
            "75/80.0\n",
            "Eval:  4\n",
            "76/80.0\n",
            "Eval:  4\n",
            "77/80.0\n",
            "Eval:  4\n",
            "78/80.0\n",
            "Eval:  4\n",
            "79/80.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.37      0.45        67\n",
            "           1       0.76      0.70      0.73      1206\n",
            "           2       0.48      0.45      0.46       119\n",
            "           3       0.67      0.74      0.71      1261\n",
            "           4       0.59      0.52      0.55       263\n",
            "           5       0.66      0.64      0.65      1504\n",
            "           6       0.65      0.79      0.71      1979\n",
            "           7       0.37      0.28      0.32       123\n",
            "           8       0.53      0.72      0.61       337\n",
            "           9       0.69      0.68      0.69       240\n",
            "          10       0.54      0.35      0.43        74\n",
            "          11       0.69      0.35      0.46       562\n",
            "          12       0.68      0.18      0.28       297\n",
            "          13       0.55      0.39      0.46       133\n",
            "          14       0.62      0.73      0.67      1810\n",
            "          15       0.68      0.48      0.57       120\n",
            "          16       0.10      0.01      0.01       145\n",
            "\n",
            "    accuracy                           0.65     10240\n",
            "   macro avg       0.58      0.49      0.52     10240\n",
            "weighted avg       0.65      0.65      0.64     10240\n",
            "\n",
            "Epoch:  5\n",
            "0/240.0 loss: 0.5847995281219482 \n",
            "Epoch:  5\n",
            "1/240.0 loss: 0.6995028555393219 \n",
            "Epoch:  5\n",
            "2/240.0 loss: 0.6973860263824463 \n",
            "Epoch:  5\n",
            "3/240.0 loss: 0.6750962436199188 \n",
            "Epoch:  5\n",
            "4/240.0 loss: 0.6596423625946045 \n",
            "Epoch:  5\n",
            "5/240.0 loss: 0.6460358301798502 \n",
            "Epoch:  5\n",
            "6/240.0 loss: 0.650990332875933 \n",
            "Epoch:  5\n",
            "7/240.0 loss: 0.6473725065588951 \n",
            "Epoch:  5\n",
            "8/240.0 loss: 0.6406866245799594 \n",
            "Epoch:  5\n",
            "9/240.0 loss: 0.6362393140792847 \n",
            "Epoch:  5\n",
            "10/240.0 loss: 0.6383866721933539 \n",
            "Epoch:  5\n",
            "11/240.0 loss: 0.6426708797613779 \n",
            "Epoch:  5\n",
            "12/240.0 loss: 0.6364090717755831 \n",
            "Epoch:  5\n",
            "13/240.0 loss: 0.630944413798196 \n",
            "Epoch:  5\n",
            "14/240.0 loss: 0.632864781220754 \n",
            "Epoch:  5\n",
            "15/240.0 loss: 0.6362455226480961 \n",
            "Epoch:  5\n",
            "16/240.0 loss: 0.6373752495821785 \n",
            "Epoch:  5\n",
            "17/240.0 loss: 0.6428952117760977 \n",
            "Epoch:  5\n",
            "18/240.0 loss: 0.6479169851855228 \n",
            "Epoch:  5\n",
            "19/240.0 loss: 0.6422324806451798 \n",
            "Epoch:  5\n",
            "20/240.0 loss: 0.6412303504489717 \n",
            "Epoch:  5\n",
            "21/240.0 loss: 0.6393079947341572 \n",
            "Epoch:  5\n",
            "22/240.0 loss: 0.6337118148803711 \n",
            "Epoch:  5\n",
            "23/240.0 loss: 0.630797415971756 \n",
            "Epoch:  5\n",
            "24/240.0 loss: 0.6323714423179626 \n",
            "Epoch:  5\n",
            "25/240.0 loss: 0.6303737484491788 \n",
            "Epoch:  5\n",
            "26/240.0 loss: 0.6284386515617371 \n",
            "Epoch:  5\n",
            "27/240.0 loss: 0.6303589876208987 \n",
            "Epoch:  5\n",
            "28/240.0 loss: 0.6368398686935162 \n",
            "Epoch:  5\n",
            "29/240.0 loss: 0.635381543636322 \n",
            "Epoch:  5\n",
            "30/240.0 loss: 0.6286259630034047 \n",
            "Epoch:  5\n",
            "31/240.0 loss: 0.6264696726575494 \n",
            "Epoch:  5\n",
            "32/240.0 loss: 0.6322206665169109 \n",
            "Epoch:  5\n",
            "33/240.0 loss: 0.6293318631017909 \n",
            "Epoch:  5\n",
            "34/240.0 loss: 0.6278602591582707 \n",
            "Epoch:  5\n",
            "35/240.0 loss: 0.6288894241054853 \n",
            "Epoch:  5\n",
            "36/240.0 loss: 0.6258949258842984 \n",
            "Epoch:  5\n",
            "37/240.0 loss: 0.6267316568838922 \n",
            "Epoch:  5\n",
            "38/240.0 loss: 0.6257933829075251 \n",
            "Epoch:  5\n",
            "39/240.0 loss: 0.6260910727083683 \n",
            "Epoch:  5\n",
            "40/240.0 loss: 0.6265315289904432 \n",
            "Epoch:  5\n",
            "41/240.0 loss: 0.6255990685451598 \n",
            "Epoch:  5\n",
            "42/240.0 loss: 0.6229241774525753 \n",
            "Epoch:  5\n",
            "43/240.0 loss: 0.621170581064441 \n",
            "Epoch:  5\n",
            "44/240.0 loss: 0.6202198538515303 \n",
            "Epoch:  5\n",
            "45/240.0 loss: 0.619836252020753 \n",
            "Epoch:  5\n",
            "46/240.0 loss: 0.6195010574574166 \n",
            "Epoch:  5\n",
            "47/240.0 loss: 0.6159537229686975 \n",
            "Epoch:  5\n",
            "48/240.0 loss: 0.6155232209332135 \n",
            "Epoch:  5\n",
            "49/240.0 loss: 0.6169008213281632 \n",
            "Epoch:  5\n",
            "50/240.0 loss: 0.6174667728882209 \n",
            "Epoch:  5\n",
            "51/240.0 loss: 0.6162908426844157 \n",
            "Epoch:  5\n",
            "52/240.0 loss: 0.6147149761892715 \n",
            "Epoch:  5\n",
            "53/240.0 loss: 0.6140368884360349 \n",
            "Epoch:  5\n",
            "54/240.0 loss: 0.6137788062745875 \n",
            "Epoch:  5\n",
            "55/240.0 loss: 0.6140601650944778 \n",
            "Epoch:  5\n",
            "56/240.0 loss: 0.6129192160932642 \n",
            "Epoch:  5\n",
            "57/240.0 loss: 0.614153990971631 \n",
            "Epoch:  5\n",
            "58/240.0 loss: 0.614145171339229 \n",
            "Epoch:  5\n",
            "59/240.0 loss: 0.6143716300527254 \n",
            "Epoch:  5\n",
            "60/240.0 loss: 0.6133744174339733 \n",
            "Epoch:  5\n",
            "61/240.0 loss: 0.614065769699312 \n",
            "Epoch:  5\n",
            "62/240.0 loss: 0.6134526195034148 \n",
            "Epoch:  5\n",
            "63/240.0 loss: 0.6122713512741029 \n",
            "Epoch:  5\n",
            "64/240.0 loss: 0.6117502758136162 \n",
            "Epoch:  5\n",
            "65/240.0 loss: 0.6091022450815547 \n",
            "Epoch:  5\n",
            "66/240.0 loss: 0.6068915746994873 \n",
            "Epoch:  5\n",
            "67/240.0 loss: 0.6069026392172364 \n",
            "Epoch:  5\n",
            "68/240.0 loss: 0.6090342303117117 \n",
            "Epoch:  5\n",
            "69/240.0 loss: 0.6074758014508657 \n",
            "Epoch:  5\n",
            "70/240.0 loss: 0.6077173272488823 \n",
            "Epoch:  5\n",
            "71/240.0 loss: 0.6084745240708193 \n",
            "Epoch:  5\n",
            "72/240.0 loss: 0.6080210833516839 \n",
            "Epoch:  5\n",
            "73/240.0 loss: 0.606768870273152 \n",
            "Epoch:  5\n",
            "74/240.0 loss: 0.6066349216302236 \n",
            "Epoch:  5\n",
            "75/240.0 loss: 0.6100291441145697 \n",
            "Epoch:  5\n",
            "76/240.0 loss: 0.6093228726417987 \n",
            "Epoch:  5\n",
            "77/240.0 loss: 0.6106421767901151 \n",
            "Epoch:  5\n",
            "78/240.0 loss: 0.6091947861110107 \n",
            "Epoch:  5\n",
            "79/240.0 loss: 0.6087022084742785 \n",
            "Epoch:  5\n",
            "80/240.0 loss: 0.6080461810400457 \n",
            "Epoch:  5\n",
            "81/240.0 loss: 0.6079452877364507 \n",
            "Epoch:  5\n",
            "82/240.0 loss: 0.6066493374037455 \n",
            "Epoch:  5\n",
            "83/240.0 loss: 0.6036085784435272 \n",
            "Epoch:  5\n",
            "84/240.0 loss: 0.6018699965056251 \n",
            "Epoch:  5\n",
            "85/240.0 loss: 0.6028698110996291 \n",
            "Epoch:  5\n",
            "86/240.0 loss: 0.6036330430672087 \n",
            "Epoch:  5\n",
            "87/240.0 loss: 0.6031186821108515 \n",
            "Epoch:  5\n",
            "88/240.0 loss: 0.6042950967054689 \n",
            "Epoch:  5\n",
            "89/240.0 loss: 0.6029722852839364 \n",
            "Epoch:  5\n",
            "90/240.0 loss: 0.6010172285221436 \n",
            "Epoch:  5\n",
            "91/240.0 loss: 0.6005158012975818 \n",
            "Epoch:  5\n",
            "92/240.0 loss: 0.6003520472716259 \n",
            "Epoch:  5\n",
            "93/240.0 loss: 0.5999824541046265 \n",
            "Epoch:  5\n",
            "94/240.0 loss: 0.5994673650515707 \n",
            "Epoch:  5\n",
            "95/240.0 loss: 0.5976046950866779 \n",
            "Epoch:  5\n",
            "96/240.0 loss: 0.5961002526209527 \n",
            "Epoch:  5\n",
            "97/240.0 loss: 0.5956622018497817 \n",
            "Epoch:  5\n",
            "98/240.0 loss: 0.5961711174911923 \n",
            "Epoch:  5\n",
            "99/240.0 loss: 0.5965232077240944 \n",
            "Epoch:  5\n",
            "100/240.0 loss: 0.5965006395731822 \n",
            "Epoch:  5\n",
            "101/240.0 loss: 0.5956350012737162 \n",
            "Epoch:  5\n",
            "102/240.0 loss: 0.595189185107796 \n",
            "Epoch:  5\n",
            "103/240.0 loss: 0.5942016203816121 \n",
            "Epoch:  5\n",
            "104/240.0 loss: 0.5926319519678752 \n",
            "Epoch:  5\n",
            "105/240.0 loss: 0.5911941826343536 \n",
            "Epoch:  5\n",
            "106/240.0 loss: 0.5899988719236071 \n",
            "Epoch:  5\n",
            "107/240.0 loss: 0.5887567480957067 \n",
            "Epoch:  5\n",
            "108/240.0 loss: 0.5884609104843315 \n",
            "Epoch:  5\n",
            "109/240.0 loss: 0.5881946100430056 \n",
            "Epoch:  5\n",
            "110/240.0 loss: 0.5876406535908982 \n",
            "Epoch:  5\n",
            "111/240.0 loss: 0.5871794774596181 \n",
            "Epoch:  5\n",
            "112/240.0 loss: 0.5870197090954907 \n",
            "Epoch:  5\n",
            "113/240.0 loss: 0.5869716418939724 \n",
            "Epoch:  5\n",
            "114/240.0 loss: 0.5870295801888341 \n",
            "Epoch:  5\n",
            "115/240.0 loss: 0.586994037289044 \n",
            "Epoch:  5\n",
            "116/240.0 loss: 0.5879900539532686 \n",
            "Epoch:  5\n",
            "117/240.0 loss: 0.58655312379538 \n",
            "Epoch:  5\n",
            "118/240.0 loss: 0.5862763105821209 \n",
            "Epoch:  5\n",
            "119/240.0 loss: 0.5853655594090621 \n",
            "Epoch:  5\n",
            "120/240.0 loss: 0.5858419116370934 \n",
            "Epoch:  5\n",
            "121/240.0 loss: 0.5847694758997589 \n",
            "Epoch:  5\n",
            "122/240.0 loss: 0.5841957221670848 \n",
            "Epoch:  5\n",
            "123/240.0 loss: 0.5834783572823771 \n",
            "Epoch:  5\n",
            "124/240.0 loss: 0.5829774568080902 \n",
            "Epoch:  5\n",
            "125/240.0 loss: 0.5819610328901381 \n",
            "Epoch:  5\n",
            "126/240.0 loss: 0.5827547710711561 \n",
            "Epoch:  5\n",
            "127/240.0 loss: 0.5833736467175186 \n",
            "Epoch:  5\n",
            "128/240.0 loss: 0.5828401173732077 \n",
            "Epoch:  5\n",
            "129/240.0 loss: 0.5834115280554845 \n",
            "Epoch:  5\n",
            "130/240.0 loss: 0.5827490980843552 \n",
            "Epoch:  5\n",
            "131/240.0 loss: 0.5829606938994292 \n",
            "Epoch:  5\n",
            "132/240.0 loss: 0.5829321977339292 \n",
            "Epoch:  5\n",
            "133/240.0 loss: 0.5820521334213997 \n",
            "Epoch:  5\n",
            "134/240.0 loss: 0.5822294932824594 \n",
            "Epoch:  5\n",
            "135/240.0 loss: 0.5821619384429034 \n",
            "Epoch:  5\n",
            "136/240.0 loss: 0.582125788622529 \n",
            "Epoch:  5\n",
            "137/240.0 loss: 0.5823894963748213 \n",
            "Epoch:  5\n",
            "138/240.0 loss: 0.5814113863509336 \n",
            "Epoch:  5\n",
            "139/240.0 loss: 0.5816296924437795 \n",
            "Epoch:  5\n",
            "140/240.0 loss: 0.5820752080027939 \n",
            "Epoch:  5\n",
            "141/240.0 loss: 0.5826333515660863 \n",
            "Epoch:  5\n",
            "142/240.0 loss: 0.5825091660856367 \n",
            "Epoch:  5\n",
            "143/240.0 loss: 0.5817797256426679 \n",
            "Epoch:  5\n",
            "144/240.0 loss: 0.5811696410179138 \n",
            "Epoch:  5\n",
            "145/240.0 loss: 0.581395928990351 \n",
            "Epoch:  5\n",
            "146/240.0 loss: 0.582289437452952 \n",
            "Epoch:  5\n",
            "147/240.0 loss: 0.5824280038878724 \n",
            "Epoch:  5\n",
            "148/240.0 loss: 0.5827430190656009 \n",
            "Epoch:  5\n",
            "149/240.0 loss: 0.5811434956391652 \n",
            "Epoch:  5\n",
            "150/240.0 loss: 0.5807191824281452 \n",
            "Epoch:  5\n",
            "151/240.0 loss: 0.5805692657044059 \n",
            "Epoch:  5\n",
            "152/240.0 loss: 0.5790942876167546 \n",
            "Epoch:  5\n",
            "153/240.0 loss: 0.5790447178599122 \n",
            "Epoch:  5\n",
            "154/240.0 loss: 0.5790893881551681 \n",
            "Epoch:  5\n",
            "155/240.0 loss: 0.5794434425158378 \n",
            "Epoch:  5\n",
            "156/240.0 loss: 0.5789482566961057 \n",
            "Epoch:  5\n",
            "157/240.0 loss: 0.5778378046388868 \n",
            "Epoch:  5\n",
            "158/240.0 loss: 0.5770874673840385 \n",
            "Epoch:  5\n",
            "159/240.0 loss: 0.5776248885318637 \n",
            "Epoch:  5\n",
            "160/240.0 loss: 0.5766038407820352 \n",
            "Epoch:  5\n",
            "161/240.0 loss: 0.5764397539106416 \n",
            "Epoch:  5\n",
            "162/240.0 loss: 0.5762115236074646 \n",
            "Epoch:  5\n",
            "163/240.0 loss: 0.5757142899603378 \n",
            "Epoch:  5\n",
            "164/240.0 loss: 0.5749892962701393 \n",
            "Epoch:  5\n",
            "165/240.0 loss: 0.5742617576237184 \n",
            "Epoch:  5\n",
            "166/240.0 loss: 0.5733593622010625 \n",
            "Epoch:  5\n",
            "167/240.0 loss: 0.5733345383334727 \n",
            "Epoch:  5\n",
            "168/240.0 loss: 0.5727929381223825 \n",
            "Epoch:  5\n",
            "169/240.0 loss: 0.5723844570272109 \n",
            "Epoch:  5\n",
            "170/240.0 loss: 0.5720140780621802 \n",
            "Epoch:  5\n",
            "171/240.0 loss: 0.5716142935115237 \n",
            "Epoch:  5\n",
            "172/240.0 loss: 0.5716282449705752 \n",
            "Epoch:  5\n",
            "173/240.0 loss: 0.5721754243319062 \n",
            "Epoch:  5\n",
            "174/240.0 loss: 0.5722386567933219 \n",
            "Epoch:  5\n",
            "175/240.0 loss: 0.5716164162890478 \n",
            "Epoch:  5\n",
            "176/240.0 loss: 0.5712520810170362 \n",
            "Epoch:  5\n",
            "177/240.0 loss: 0.57153635848774 \n",
            "Epoch:  5\n",
            "178/240.0 loss: 0.5714822161797039 \n",
            "Epoch:  5\n",
            "179/240.0 loss: 0.5710833584268887 \n",
            "Epoch:  5\n",
            "180/240.0 loss: 0.5706768116568992 \n",
            "Epoch:  5\n",
            "181/240.0 loss: 0.5708413777443079 \n",
            "Epoch:  5\n",
            "182/240.0 loss: 0.570542408631799 \n",
            "Epoch:  5\n",
            "183/240.0 loss: 0.57038113264286 \n",
            "Epoch:  5\n",
            "184/240.0 loss: 0.5699015978220346 \n",
            "Epoch:  5\n",
            "185/240.0 loss: 0.5701905021103479 \n",
            "Epoch:  5\n",
            "186/240.0 loss: 0.5702069070249955 \n",
            "Epoch:  5\n",
            "187/240.0 loss: 0.570046603362611 \n",
            "Epoch:  5\n",
            "188/240.0 loss: 0.5690897089779061 \n",
            "Epoch:  5\n",
            "189/240.0 loss: 0.5685477112468921 \n",
            "Epoch:  5\n",
            "190/240.0 loss: 0.5682974101985312 \n",
            "Epoch:  5\n",
            "191/240.0 loss: 0.568192531975607 \n",
            "Epoch:  5\n",
            "192/240.0 loss: 0.5672811478221972 \n",
            "Epoch:  5\n",
            "193/240.0 loss: 0.567284744885779 \n",
            "Epoch:  5\n",
            "194/240.0 loss: 0.5670113344987233 \n",
            "Epoch:  5\n",
            "195/240.0 loss: 0.5672398804097759 \n",
            "Epoch:  5\n",
            "196/240.0 loss: 0.5672100493448035 \n",
            "Epoch:  5\n",
            "197/240.0 loss: 0.5675493630796972 \n",
            "Epoch:  5\n",
            "198/240.0 loss: 0.5681543303793998 \n",
            "Epoch:  5\n",
            "199/240.0 loss: 0.5678439988195896 \n",
            "Epoch:  5\n",
            "200/240.0 loss: 0.5676529919033619 \n",
            "Epoch:  5\n",
            "201/240.0 loss: 0.5669657651445653 \n",
            "Epoch:  5\n",
            "202/240.0 loss: 0.5658443088601963 \n",
            "Epoch:  5\n",
            "203/240.0 loss: 0.5653677148854032 \n",
            "Epoch:  5\n",
            "204/240.0 loss: 0.5645434404291758 \n",
            "Epoch:  5\n",
            "205/240.0 loss: 0.5639954148565681 \n",
            "Epoch:  5\n",
            "206/240.0 loss: 0.5640773015897631 \n",
            "Epoch:  5\n",
            "207/240.0 loss: 0.5636987845198467 \n",
            "Epoch:  5\n",
            "208/240.0 loss: 0.5632095098780673 \n",
            "Epoch:  5\n",
            "209/240.0 loss: 0.5633217963434402 \n",
            "Epoch:  5\n",
            "210/240.0 loss: 0.5635225339240938 \n",
            "Epoch:  5\n",
            "211/240.0 loss: 0.5634613617692353 \n",
            "Epoch:  5\n",
            "212/240.0 loss: 0.5632962328447423 \n",
            "Epoch:  5\n",
            "213/240.0 loss: 0.5629877445296706 \n",
            "Epoch:  5\n",
            "214/240.0 loss: 0.5632630861082742 \n",
            "Epoch:  5\n",
            "215/240.0 loss: 0.5629057572395714 \n",
            "Epoch:  5\n",
            "216/240.0 loss: 0.5628920090363322 \n",
            "Epoch:  5\n",
            "217/240.0 loss: 0.563320263810114 \n",
            "Epoch:  5\n",
            "218/240.0 loss: 0.562930619743861 \n",
            "Epoch:  5\n",
            "219/240.0 loss: 0.5634160051291639 \n",
            "Epoch:  5\n",
            "220/240.0 loss: 0.5634022642314703 \n",
            "Epoch:  5\n",
            "221/240.0 loss: 0.5637148912694003 \n",
            "Epoch:  5\n",
            "222/240.0 loss: 0.563982968774077 \n",
            "Epoch:  5\n",
            "223/240.0 loss: 0.5641174086236528 \n",
            "Epoch:  5\n",
            "224/240.0 loss: 0.5649250672923194 \n",
            "Epoch:  5\n",
            "225/240.0 loss: 0.5648929398935453 \n",
            "Epoch:  5\n",
            "226/240.0 loss: 0.5645526205127985 \n",
            "Epoch:  5\n",
            "227/240.0 loss: 0.5638924209695113 \n",
            "Epoch:  5\n",
            "228/240.0 loss: 0.5634157884068884 \n",
            "Epoch:  5\n",
            "229/240.0 loss: 0.5640015148598215 \n",
            "Epoch:  5\n",
            "230/240.0 loss: 0.5639352981662338 \n",
            "Epoch:  5\n",
            "231/240.0 loss: 0.5635401191639489 \n",
            "Epoch:  5\n",
            "232/240.0 loss: 0.5632564636514934 \n",
            "Epoch:  5\n",
            "233/240.0 loss: 0.5628094918962218 \n",
            "Epoch:  5\n",
            "234/240.0 loss: 0.5627788037695783 \n",
            "Epoch:  5\n",
            "235/240.0 loss: 0.5627991318450136 \n",
            "Epoch:  5\n",
            "236/240.0 loss: 0.5631745959384532 \n",
            "Epoch:  5\n",
            "237/240.0 loss: 0.5626805735235455 \n",
            "Epoch:  5\n",
            "238/240.0 loss: 0.562883656144641 \n",
            "Epoch:  5\n",
            "239/240.0 loss: 0.5624041782071193 \n",
            "Eval:  5\n",
            "0/80.0\n",
            "Eval:  5\n",
            "1/80.0\n",
            "Eval:  5\n",
            "2/80.0\n",
            "Eval:  5\n",
            "3/80.0\n",
            "Eval:  5\n",
            "4/80.0\n",
            "Eval:  5\n",
            "5/80.0\n",
            "Eval:  5\n",
            "6/80.0\n",
            "Eval:  5\n",
            "7/80.0\n",
            "Eval:  5\n",
            "8/80.0\n",
            "Eval:  5\n",
            "9/80.0\n",
            "Eval:  5\n",
            "10/80.0\n",
            "Eval:  5\n",
            "11/80.0\n",
            "Eval:  5\n",
            "12/80.0\n",
            "Eval:  5\n",
            "13/80.0\n",
            "Eval:  5\n",
            "14/80.0\n",
            "Eval:  5\n",
            "15/80.0\n",
            "Eval:  5\n",
            "16/80.0\n",
            "Eval:  5\n",
            "17/80.0\n",
            "Eval:  5\n",
            "18/80.0\n",
            "Eval:  5\n",
            "19/80.0\n",
            "Eval:  5\n",
            "20/80.0\n",
            "Eval:  5\n",
            "21/80.0\n",
            "Eval:  5\n",
            "22/80.0\n",
            "Eval:  5\n",
            "23/80.0\n",
            "Eval:  5\n",
            "24/80.0\n",
            "Eval:  5\n",
            "25/80.0\n",
            "Eval:  5\n",
            "26/80.0\n",
            "Eval:  5\n",
            "27/80.0\n",
            "Eval:  5\n",
            "28/80.0\n",
            "Eval:  5\n",
            "29/80.0\n",
            "Eval:  5\n",
            "30/80.0\n",
            "Eval:  5\n",
            "31/80.0\n",
            "Eval:  5\n",
            "32/80.0\n",
            "Eval:  5\n",
            "33/80.0\n",
            "Eval:  5\n",
            "34/80.0\n",
            "Eval:  5\n",
            "35/80.0\n",
            "Eval:  5\n",
            "36/80.0\n",
            "Eval:  5\n",
            "37/80.0\n",
            "Eval:  5\n",
            "38/80.0\n",
            "Eval:  5\n",
            "39/80.0\n",
            "Eval:  5\n",
            "40/80.0\n",
            "Eval:  5\n",
            "41/80.0\n",
            "Eval:  5\n",
            "42/80.0\n",
            "Eval:  5\n",
            "43/80.0\n",
            "Eval:  5\n",
            "44/80.0\n",
            "Eval:  5\n",
            "45/80.0\n",
            "Eval:  5\n",
            "46/80.0\n",
            "Eval:  5\n",
            "47/80.0\n",
            "Eval:  5\n",
            "48/80.0\n",
            "Eval:  5\n",
            "49/80.0\n",
            "Eval:  5\n",
            "50/80.0\n",
            "Eval:  5\n",
            "51/80.0\n",
            "Eval:  5\n",
            "52/80.0\n",
            "Eval:  5\n",
            "53/80.0\n",
            "Eval:  5\n",
            "54/80.0\n",
            "Eval:  5\n",
            "55/80.0\n",
            "Eval:  5\n",
            "56/80.0\n",
            "Eval:  5\n",
            "57/80.0\n",
            "Eval:  5\n",
            "58/80.0\n",
            "Eval:  5\n",
            "59/80.0\n",
            "Eval:  5\n",
            "60/80.0\n",
            "Eval:  5\n",
            "61/80.0\n",
            "Eval:  5\n",
            "62/80.0\n",
            "Eval:  5\n",
            "63/80.0\n",
            "Eval:  5\n",
            "64/80.0\n",
            "Eval:  5\n",
            "65/80.0\n",
            "Eval:  5\n",
            "66/80.0\n",
            "Eval:  5\n",
            "67/80.0\n",
            "Eval:  5\n",
            "68/80.0\n",
            "Eval:  5\n",
            "69/80.0\n",
            "Eval:  5\n",
            "70/80.0\n",
            "Eval:  5\n",
            "71/80.0\n",
            "Eval:  5\n",
            "72/80.0\n",
            "Eval:  5\n",
            "73/80.0\n",
            "Eval:  5\n",
            "74/80.0\n",
            "Eval:  5\n",
            "75/80.0\n",
            "Eval:  5\n",
            "76/80.0\n",
            "Eval:  5\n",
            "77/80.0\n",
            "Eval:  5\n",
            "78/80.0\n",
            "Eval:  5\n",
            "79/80.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.42      0.52        67\n",
            "           1       0.71      0.75      0.73      1206\n",
            "           2       0.50      0.44      0.47       119\n",
            "           3       0.71      0.69      0.70      1261\n",
            "           4       0.64      0.47      0.54       263\n",
            "           5       0.66      0.63      0.65      1504\n",
            "           6       0.64      0.77      0.70      1979\n",
            "           7       0.42      0.33      0.37       123\n",
            "           8       0.55      0.74      0.63       337\n",
            "           9       0.71      0.64      0.67       240\n",
            "          10       0.51      0.30      0.38        74\n",
            "          11       0.74      0.27      0.39       562\n",
            "          12       0.48      0.29      0.36       297\n",
            "          13       0.50      0.45      0.47       133\n",
            "          14       0.61      0.73      0.66      1810\n",
            "          15       0.73      0.39      0.51       120\n",
            "          16       0.17      0.03      0.06       145\n",
            "\n",
            "    accuracy                           0.64     10240\n",
            "   macro avg       0.59      0.49      0.52     10240\n",
            "weighted avg       0.64      0.64      0.63     10240\n",
            "\n",
            "Epoch:  6\n",
            "0/240.0 loss: 0.45989394187927246 \n",
            "Epoch:  6\n",
            "1/240.0 loss: 0.5392648875713348 \n",
            "Epoch:  6\n",
            "2/240.0 loss: 0.519149641195933 \n",
            "Epoch:  6\n",
            "3/240.0 loss: 0.49814586341381073 \n",
            "Epoch:  6\n",
            "4/240.0 loss: 0.48133763670921326 \n",
            "Epoch:  6\n",
            "5/240.0 loss: 0.4739643981059392 \n",
            "Epoch:  6\n",
            "6/240.0 loss: 0.48366420183862957 \n",
            "Epoch:  6\n",
            "7/240.0 loss: 0.4810931086540222 \n",
            "Epoch:  6\n",
            "8/240.0 loss: 0.4729318585660722 \n",
            "Epoch:  6\n",
            "9/240.0 loss: 0.46922066509723664 \n",
            "Epoch:  6\n",
            "10/240.0 loss: 0.46857074986804614 \n",
            "Epoch:  6\n",
            "11/240.0 loss: 0.4697706326842308 \n",
            "Epoch:  6\n",
            "12/240.0 loss: 0.4643094585492061 \n",
            "Epoch:  6\n",
            "13/240.0 loss: 0.4588535172598703 \n",
            "Epoch:  6\n",
            "14/240.0 loss: 0.46070487101872765 \n",
            "Epoch:  6\n",
            "15/240.0 loss: 0.4623049106448889 \n",
            "Epoch:  6\n",
            "16/240.0 loss: 0.46484173220746655 \n",
            "Epoch:  6\n",
            "17/240.0 loss: 0.4642248534493976 \n",
            "Epoch:  6\n",
            "18/240.0 loss: 0.469608054349297 \n",
            "Epoch:  6\n",
            "19/240.0 loss: 0.46361943036317826 \n",
            "Epoch:  6\n",
            "20/240.0 loss: 0.4630970358848572 \n",
            "Epoch:  6\n",
            "21/240.0 loss: 0.4607226537032561 \n",
            "Epoch:  6\n",
            "22/240.0 loss: 0.4566462856271993 \n",
            "Epoch:  6\n",
            "23/240.0 loss: 0.4533849060535431 \n",
            "Epoch:  6\n",
            "24/240.0 loss: 0.45409130573272705 \n",
            "Epoch:  6\n",
            "25/240.0 loss: 0.4496764105099898 \n",
            "Epoch:  6\n",
            "26/240.0 loss: 0.4485127627849579 \n",
            "Epoch:  6\n",
            "27/240.0 loss: 0.44924707072121756 \n",
            "Epoch:  6\n",
            "28/240.0 loss: 0.45822480012630595 \n",
            "Epoch:  6\n",
            "29/240.0 loss: 0.4556656301021576 \n",
            "Epoch:  6\n",
            "30/240.0 loss: 0.4496997844788336 \n",
            "Epoch:  6\n",
            "31/240.0 loss: 0.44843563064932823 \n",
            "Epoch:  6\n",
            "32/240.0 loss: 0.45320047212369513 \n",
            "Epoch:  6\n",
            "33/240.0 loss: 0.4520573230350719 \n",
            "Epoch:  6\n",
            "34/240.0 loss: 0.4505098717553275 \n",
            "Epoch:  6\n",
            "35/240.0 loss: 0.4520011709796058 \n",
            "Epoch:  6\n",
            "36/240.0 loss: 0.45008431093112844 \n",
            "Epoch:  6\n",
            "37/240.0 loss: 0.4502142189364684 \n",
            "Epoch:  6\n",
            "38/240.0 loss: 0.44838284758421093 \n",
            "Epoch:  6\n",
            "39/240.0 loss: 0.44967926368117334 \n",
            "Epoch:  6\n",
            "40/240.0 loss: 0.4521205606983929 \n",
            "Epoch:  6\n",
            "41/240.0 loss: 0.45295220613479614 \n",
            "Epoch:  6\n",
            "42/240.0 loss: 0.4502868021643439 \n",
            "Epoch:  6\n",
            "43/240.0 loss: 0.4496683735739101 \n",
            "Epoch:  6\n",
            "44/240.0 loss: 0.4472892787721422 \n",
            "Epoch:  6\n",
            "45/240.0 loss: 0.4480193853378296 \n",
            "Epoch:  6\n",
            "46/240.0 loss: 0.44812634397060314 \n",
            "Epoch:  6\n",
            "47/240.0 loss: 0.4467651502539714 \n",
            "Epoch:  6\n",
            "48/240.0 loss: 0.44630219863385573 \n",
            "Epoch:  6\n",
            "49/240.0 loss: 0.44924110531806943 \n",
            "Epoch:  6\n",
            "50/240.0 loss: 0.45039604341282563 \n",
            "Epoch:  6\n",
            "51/240.0 loss: 0.4497497179187261 \n",
            "Epoch:  6\n",
            "52/240.0 loss: 0.44982875911694653 \n",
            "Epoch:  6\n",
            "53/240.0 loss: 0.44873956784054086 \n",
            "Epoch:  6\n",
            "54/240.0 loss: 0.44906591393730855 \n",
            "Epoch:  6\n",
            "55/240.0 loss: 0.45009766199759077 \n",
            "Epoch:  6\n",
            "56/240.0 loss: 0.44895792059731066 \n",
            "Epoch:  6\n",
            "57/240.0 loss: 0.45112364302421437 \n",
            "Epoch:  6\n",
            "58/240.0 loss: 0.4509996039382482 \n",
            "Epoch:  6\n",
            "59/240.0 loss: 0.45234004507462183 \n",
            "Epoch:  6\n",
            "60/240.0 loss: 0.45157218077143685 \n",
            "Epoch:  6\n",
            "61/240.0 loss: 0.4516330659389496 \n",
            "Epoch:  6\n",
            "62/240.0 loss: 0.4516744589994824 \n",
            "Epoch:  6\n",
            "63/240.0 loss: 0.45159857580438256 \n",
            "Epoch:  6\n",
            "64/240.0 loss: 0.4527879068484673 \n",
            "Epoch:  6\n",
            "65/240.0 loss: 0.4510186282974301 \n",
            "Epoch:  6\n",
            "66/240.0 loss: 0.4491667556228922 \n",
            "Epoch:  6\n",
            "67/240.0 loss: 0.4503305155564757 \n",
            "Epoch:  6\n",
            "68/240.0 loss: 0.4518956468589064 \n",
            "Epoch:  6\n",
            "69/240.0 loss: 0.45186922890799386 \n",
            "Epoch:  6\n",
            "70/240.0 loss: 0.45184118814871344 \n",
            "Epoch:  6\n",
            "71/240.0 loss: 0.45260903735955554 \n",
            "Epoch:  6\n",
            "72/240.0 loss: 0.452754897205797 \n",
            "Epoch:  6\n",
            "73/240.0 loss: 0.45111767424119487 \n",
            "Epoch:  6\n",
            "74/240.0 loss: 0.4512659712632497 \n",
            "Epoch:  6\n",
            "75/240.0 loss: 0.45393453499204234 \n",
            "Epoch:  6\n",
            "76/240.0 loss: 0.454187645540609 \n",
            "Epoch:  6\n",
            "77/240.0 loss: 0.45442791015673906 \n",
            "Epoch:  6\n",
            "78/240.0 loss: 0.4534008597271352 \n",
            "Epoch:  6\n",
            "79/240.0 loss: 0.4530129641294479 \n",
            "Epoch:  6\n",
            "80/240.0 loss: 0.4525513093412658 \n",
            "Epoch:  6\n",
            "81/240.0 loss: 0.45224316672581 \n",
            "Epoch:  6\n",
            "82/240.0 loss: 0.45140825122235767 \n",
            "Epoch:  6\n",
            "83/240.0 loss: 0.44935672517333713 \n",
            "Epoch:  6\n",
            "84/240.0 loss: 0.44798691377920263 \n",
            "Epoch:  6\n",
            "85/240.0 loss: 0.4493710547685623 \n",
            "Epoch:  6\n",
            "86/240.0 loss: 0.44979001941352054 \n",
            "Epoch:  6\n",
            "87/240.0 loss: 0.44922162186015735 \n",
            "Epoch:  6\n",
            "88/240.0 loss: 0.45052113693751644 \n",
            "Epoch:  6\n",
            "89/240.0 loss: 0.4494442833794488 \n",
            "Epoch:  6\n",
            "90/240.0 loss: 0.44868017258224907 \n",
            "Epoch:  6\n",
            "91/240.0 loss: 0.44927094196495804 \n",
            "Epoch:  6\n",
            "92/240.0 loss: 0.4492777159778021 \n",
            "Epoch:  6\n",
            "93/240.0 loss: 0.447943148143748 \n",
            "Epoch:  6\n",
            "94/240.0 loss: 0.4470078013445202 \n",
            "Epoch:  6\n",
            "95/240.0 loss: 0.4461884203677376 \n",
            "Epoch:  6\n",
            "96/240.0 loss: 0.445171501833139 \n",
            "Epoch:  6\n",
            "97/240.0 loss: 0.4447841942310333 \n",
            "Epoch:  6\n",
            "98/240.0 loss: 0.4443415632151594 \n",
            "Epoch:  6\n",
            "99/240.0 loss: 0.444314167201519 \n",
            "Epoch:  6\n",
            "100/240.0 loss: 0.44403175581799875 \n",
            "Epoch:  6\n",
            "101/240.0 loss: 0.4439244848840377 \n",
            "Epoch:  6\n",
            "102/240.0 loss: 0.44358425753787883 \n",
            "Epoch:  6\n",
            "103/240.0 loss: 0.4429234750568867 \n",
            "Epoch:  6\n",
            "104/240.0 loss: 0.4418999339853014 \n",
            "Epoch:  6\n",
            "105/240.0 loss: 0.4407862975912274 \n",
            "Epoch:  6\n",
            "106/240.0 loss: 0.44055404117174235 \n",
            "Epoch:  6\n",
            "107/240.0 loss: 0.43943507693431993 \n",
            "Epoch:  6\n",
            "108/240.0 loss: 0.43955883219701436 \n",
            "Epoch:  6\n",
            "109/240.0 loss: 0.43946975686333395 \n",
            "Epoch:  6\n",
            "110/240.0 loss: 0.43899141131220637 \n",
            "Epoch:  6\n",
            "111/240.0 loss: 0.43841367294745787 \n",
            "Epoch:  6\n",
            "112/240.0 loss: 0.43907876325919565 \n",
            "Epoch:  6\n",
            "113/240.0 loss: 0.4399945607833695 \n",
            "Epoch:  6\n",
            "114/240.0 loss: 0.4407545294450677 \n",
            "Epoch:  6\n",
            "115/240.0 loss: 0.44098875517475195 \n",
            "Epoch:  6\n",
            "116/240.0 loss: 0.4420229316267193 \n",
            "Epoch:  6\n",
            "117/240.0 loss: 0.4417013801255469 \n",
            "Epoch:  6\n",
            "118/240.0 loss: 0.4410220404632953 \n",
            "Epoch:  6\n",
            "119/240.0 loss: 0.4400411310295264 \n",
            "Epoch:  6\n",
            "120/240.0 loss: 0.44028836932064086 \n",
            "Epoch:  6\n",
            "121/240.0 loss: 0.4395274336709351 \n",
            "Epoch:  6\n",
            "122/240.0 loss: 0.4392800595217604 \n",
            "Epoch:  6\n",
            "123/240.0 loss: 0.43818736412832815 \n",
            "Epoch:  6\n",
            "124/240.0 loss: 0.4383249402046204 \n",
            "Epoch:  6\n",
            "125/240.0 loss: 0.43824074263610535 \n",
            "Epoch:  6\n",
            "126/240.0 loss: 0.43799671249126826 \n",
            "Epoch:  6\n",
            "127/240.0 loss: 0.43837365158833563 \n",
            "Epoch:  6\n",
            "128/240.0 loss: 0.4378012915452321 \n",
            "Epoch:  6\n",
            "129/240.0 loss: 0.4381396905733989 \n",
            "Epoch:  6\n",
            "130/240.0 loss: 0.437119604749534 \n",
            "Epoch:  6\n",
            "131/240.0 loss: 0.4371474502664624 \n",
            "Epoch:  6\n",
            "132/240.0 loss: 0.43741462217237714 \n",
            "Epoch:  6\n",
            "133/240.0 loss: 0.43754696245513747 \n",
            "Epoch:  6\n",
            "134/240.0 loss: 0.4375477581112473 \n",
            "Epoch:  6\n",
            "135/240.0 loss: 0.436923111186308 \n",
            "Epoch:  6\n",
            "136/240.0 loss: 0.4370495561700668 \n",
            "Epoch:  6\n",
            "137/240.0 loss: 0.4372891353956167 \n",
            "Epoch:  6\n",
            "138/240.0 loss: 0.4368647909421715 \n",
            "Epoch:  6\n",
            "139/240.0 loss: 0.43664162031241827 \n",
            "Epoch:  6\n",
            "140/240.0 loss: 0.43750042441888903 \n",
            "Epoch:  6\n",
            "141/240.0 loss: 0.43816264624327 \n",
            "Epoch:  6\n",
            "142/240.0 loss: 0.4383350644078288 \n",
            "Epoch:  6\n",
            "143/240.0 loss: 0.4382146919767062 \n",
            "Epoch:  6\n",
            "144/240.0 loss: 0.4381075240414718 \n",
            "Epoch:  6\n",
            "145/240.0 loss: 0.437761368816846 \n",
            "Epoch:  6\n",
            "146/240.0 loss: 0.4385965008313964 \n",
            "Epoch:  6\n",
            "147/240.0 loss: 0.4393668263345151 \n",
            "Epoch:  6\n",
            "148/240.0 loss: 0.4398242887234528 \n",
            "Epoch:  6\n",
            "149/240.0 loss: 0.4391466724872589 \n",
            "Epoch:  6\n",
            "150/240.0 loss: 0.4392704807764647 \n",
            "Epoch:  6\n",
            "151/240.0 loss: 0.4386269148243101 \n",
            "Epoch:  6\n",
            "152/240.0 loss: 0.4375416909167969 \n",
            "Epoch:  6\n",
            "153/240.0 loss: 0.4383208082093821 \n",
            "Epoch:  6\n",
            "154/240.0 loss: 0.43847651270128063 \n",
            "Epoch:  6\n",
            "155/240.0 loss: 0.4391773830239589 \n",
            "Epoch:  6\n",
            "156/240.0 loss: 0.43845960972415415 \n",
            "Epoch:  6\n",
            "157/240.0 loss: 0.4374074539806269 \n",
            "Epoch:  6\n",
            "158/240.0 loss: 0.4369601740402246 \n",
            "Epoch:  6\n",
            "159/240.0 loss: 0.436880249530077 \n",
            "Epoch:  6\n",
            "160/240.0 loss: 0.43632913727938016 \n",
            "Epoch:  6\n",
            "161/240.0 loss: 0.4364820417062736 \n",
            "Epoch:  6\n",
            "162/240.0 loss: 0.4362663597417024 \n",
            "Epoch:  6\n",
            "163/240.0 loss: 0.43562224416471107 \n",
            "Epoch:  6\n",
            "164/240.0 loss: 0.4352334708878488 \n",
            "Epoch:  6\n",
            "165/240.0 loss: 0.43491953790905963 \n",
            "Epoch:  6\n",
            "166/240.0 loss: 0.4346506372540297 \n",
            "Epoch:  6\n",
            "167/240.0 loss: 0.43519582723577815 \n",
            "Epoch:  6\n",
            "168/240.0 loss: 0.4348786959986715 \n",
            "Epoch:  6\n",
            "169/240.0 loss: 0.43491312861442566 \n",
            "Epoch:  6\n",
            "170/240.0 loss: 0.4346898543207269 \n",
            "Epoch:  6\n",
            "171/240.0 loss: 0.4345316678978676 \n",
            "Epoch:  6\n",
            "172/240.0 loss: 0.43463354734327064 \n",
            "Epoch:  6\n",
            "173/240.0 loss: 0.43483455273611793 \n",
            "Epoch:  6\n",
            "174/240.0 loss: 0.43496947390692575 \n",
            "Epoch:  6\n",
            "175/240.0 loss: 0.43423027104952117 \n",
            "Epoch:  6\n",
            "176/240.0 loss: 0.43391030807953096 \n",
            "Epoch:  6\n",
            "177/240.0 loss: 0.4344135598185357 \n",
            "Epoch:  6\n",
            "178/240.0 loss: 0.43449929989250013 \n",
            "Epoch:  6\n",
            "179/240.0 loss: 0.43431131210592056 \n",
            "Epoch:  6\n",
            "180/240.0 loss: 0.43447264205684977 \n",
            "Epoch:  6\n",
            "181/240.0 loss: 0.43476998052754245 \n",
            "Epoch:  6\n",
            "182/240.0 loss: 0.4351911108350493 \n",
            "Epoch:  6\n",
            "183/240.0 loss: 0.43525677239117416 \n",
            "Epoch:  6\n",
            "184/240.0 loss: 0.43510407225505726 \n",
            "Epoch:  6\n",
            "185/240.0 loss: 0.4354431106839129 \n",
            "Epoch:  6\n",
            "186/240.0 loss: 0.43549650062851725 \n",
            "Epoch:  6\n",
            "187/240.0 loss: 0.43593734106484877 \n",
            "Epoch:  6\n",
            "188/240.0 loss: 0.43503307082034925 \n",
            "Epoch:  6\n",
            "189/240.0 loss: 0.435040095448494 \n",
            "Epoch:  6\n",
            "190/240.0 loss: 0.4351914386162583 \n",
            "Epoch:  6\n",
            "191/240.0 loss: 0.4350652270950377 \n",
            "Epoch:  6\n",
            "192/240.0 loss: 0.4345923915428201 \n",
            "Epoch:  6\n",
            "193/240.0 loss: 0.43455968612862617 \n",
            "Epoch:  6\n",
            "194/240.0 loss: 0.43502575357755024 \n",
            "Epoch:  6\n",
            "195/240.0 loss: 0.43535403405525247 \n",
            "Epoch:  6\n",
            "196/240.0 loss: 0.43545362822295447 \n",
            "Epoch:  6\n",
            "197/240.0 loss: 0.4354719140312888 \n",
            "Epoch:  6\n",
            "198/240.0 loss: 0.4359778854715165 \n",
            "Epoch:  6\n",
            "199/240.0 loss: 0.43548371851444245 \n",
            "Epoch:  6\n",
            "200/240.0 loss: 0.43549908186072733 \n",
            "Epoch:  6\n",
            "201/240.0 loss: 0.4348111866724373 \n",
            "Epoch:  6\n",
            "202/240.0 loss: 0.43437620938705107 \n",
            "Epoch:  6\n",
            "203/240.0 loss: 0.4340440521345419 \n",
            "Epoch:  6\n",
            "204/240.0 loss: 0.433560458479858 \n",
            "Epoch:  6\n",
            "205/240.0 loss: 0.43298971913393264 \n",
            "Epoch:  6\n",
            "206/240.0 loss: 0.43270370191422064 \n",
            "Epoch:  6\n",
            "207/240.0 loss: 0.43234617420687127 \n",
            "Epoch:  6\n",
            "208/240.0 loss: 0.431978507903204 \n",
            "Epoch:  6\n",
            "209/240.0 loss: 0.4320817772831236 \n",
            "Epoch:  6\n",
            "210/240.0 loss: 0.4322986196002689 \n",
            "Epoch:  6\n",
            "211/240.0 loss: 0.4322352885918797 \n",
            "Epoch:  6\n",
            "212/240.0 loss: 0.4322238791156823 \n",
            "Epoch:  6\n",
            "213/240.0 loss: 0.43214695072062664 \n",
            "Epoch:  6\n",
            "214/240.0 loss: 0.431888922563819 \n",
            "Epoch:  6\n",
            "215/240.0 loss: 0.4318442702017449 \n",
            "Epoch:  6\n",
            "216/240.0 loss: 0.43208192460547945 \n",
            "Epoch:  6\n",
            "217/240.0 loss: 0.4322527518513006 \n",
            "Epoch:  6\n",
            "218/240.0 loss: 0.4320201936377782 \n",
            "Epoch:  6\n",
            "219/240.0 loss: 0.4321746823462573 \n",
            "Epoch:  6\n",
            "220/240.0 loss: 0.43195676385547244 \n",
            "Epoch:  6\n",
            "221/240.0 loss: 0.43208015810798955 \n",
            "Epoch:  6\n",
            "222/240.0 loss: 0.43224160713999793 \n",
            "Epoch:  6\n",
            "223/240.0 loss: 0.43206732334303005 \n",
            "Epoch:  6\n",
            "224/240.0 loss: 0.43283519917064245 \n",
            "Epoch:  6\n",
            "225/240.0 loss: 0.4337524923339354 \n",
            "Epoch:  6\n",
            "226/240.0 loss: 0.43412368216178493 \n",
            "Epoch:  6\n",
            "227/240.0 loss: 0.4338433329473462 \n",
            "Epoch:  6\n",
            "228/240.0 loss: 0.4338967658286532 \n",
            "Epoch:  6\n",
            "229/240.0 loss: 0.4338634651640187 \n",
            "Epoch:  6\n",
            "230/240.0 loss: 0.43370005178761173 \n",
            "Epoch:  6\n",
            "231/240.0 loss: 0.43329824571465625 \n",
            "Epoch:  6\n",
            "232/240.0 loss: 0.43334272913155125 \n",
            "Epoch:  6\n",
            "233/240.0 loss: 0.43346604985049647 \n",
            "Epoch:  6\n",
            "234/240.0 loss: 0.4340891034045118 \n",
            "Epoch:  6\n",
            "235/240.0 loss: 0.43447556182489555 \n",
            "Epoch:  6\n",
            "236/240.0 loss: 0.435101965057196 \n",
            "Epoch:  6\n",
            "237/240.0 loss: 0.4349907254721938 \n",
            "Epoch:  6\n",
            "238/240.0 loss: 0.43535907759826054 \n",
            "Epoch:  6\n",
            "239/240.0 loss: 0.4357718406865994 \n",
            "Eval:  6\n",
            "0/80.0\n",
            "Eval:  6\n",
            "1/80.0\n",
            "Eval:  6\n",
            "2/80.0\n",
            "Eval:  6\n",
            "3/80.0\n",
            "Eval:  6\n",
            "4/80.0\n",
            "Eval:  6\n",
            "5/80.0\n",
            "Eval:  6\n",
            "6/80.0\n",
            "Eval:  6\n",
            "7/80.0\n",
            "Eval:  6\n",
            "8/80.0\n",
            "Eval:  6\n",
            "9/80.0\n",
            "Eval:  6\n",
            "10/80.0\n",
            "Eval:  6\n",
            "11/80.0\n",
            "Eval:  6\n",
            "12/80.0\n",
            "Eval:  6\n",
            "13/80.0\n",
            "Eval:  6\n",
            "14/80.0\n",
            "Eval:  6\n",
            "15/80.0\n",
            "Eval:  6\n",
            "16/80.0\n",
            "Eval:  6\n",
            "17/80.0\n",
            "Eval:  6\n",
            "18/80.0\n",
            "Eval:  6\n",
            "19/80.0\n",
            "Eval:  6\n",
            "20/80.0\n",
            "Eval:  6\n",
            "21/80.0\n",
            "Eval:  6\n",
            "22/80.0\n",
            "Eval:  6\n",
            "23/80.0\n",
            "Eval:  6\n",
            "24/80.0\n",
            "Eval:  6\n",
            "25/80.0\n",
            "Eval:  6\n",
            "26/80.0\n",
            "Eval:  6\n",
            "27/80.0\n",
            "Eval:  6\n",
            "28/80.0\n",
            "Eval:  6\n",
            "29/80.0\n",
            "Eval:  6\n",
            "30/80.0\n",
            "Eval:  6\n",
            "31/80.0\n",
            "Eval:  6\n",
            "32/80.0\n",
            "Eval:  6\n",
            "33/80.0\n",
            "Eval:  6\n",
            "34/80.0\n",
            "Eval:  6\n",
            "35/80.0\n",
            "Eval:  6\n",
            "36/80.0\n",
            "Eval:  6\n",
            "37/80.0\n",
            "Eval:  6\n",
            "38/80.0\n",
            "Eval:  6\n",
            "39/80.0\n",
            "Eval:  6\n",
            "40/80.0\n",
            "Eval:  6\n",
            "41/80.0\n",
            "Eval:  6\n",
            "42/80.0\n",
            "Eval:  6\n",
            "43/80.0\n",
            "Eval:  6\n",
            "44/80.0\n",
            "Eval:  6\n",
            "45/80.0\n",
            "Eval:  6\n",
            "46/80.0\n",
            "Eval:  6\n",
            "47/80.0\n",
            "Eval:  6\n",
            "48/80.0\n",
            "Eval:  6\n",
            "49/80.0\n",
            "Eval:  6\n",
            "50/80.0\n",
            "Eval:  6\n",
            "51/80.0\n",
            "Eval:  6\n",
            "52/80.0\n",
            "Eval:  6\n",
            "53/80.0\n",
            "Eval:  6\n",
            "54/80.0\n",
            "Eval:  6\n",
            "55/80.0\n",
            "Eval:  6\n",
            "56/80.0\n",
            "Eval:  6\n",
            "57/80.0\n",
            "Eval:  6\n",
            "58/80.0\n",
            "Eval:  6\n",
            "59/80.0\n",
            "Eval:  6\n",
            "60/80.0\n",
            "Eval:  6\n",
            "61/80.0\n",
            "Eval:  6\n",
            "62/80.0\n",
            "Eval:  6\n",
            "63/80.0\n",
            "Eval:  6\n",
            "64/80.0\n",
            "Eval:  6\n",
            "65/80.0\n",
            "Eval:  6\n",
            "66/80.0\n",
            "Eval:  6\n",
            "67/80.0\n",
            "Eval:  6\n",
            "68/80.0\n",
            "Eval:  6\n",
            "69/80.0\n",
            "Eval:  6\n",
            "70/80.0\n",
            "Eval:  6\n",
            "71/80.0\n",
            "Eval:  6\n",
            "72/80.0\n",
            "Eval:  6\n",
            "73/80.0\n",
            "Eval:  6\n",
            "74/80.0\n",
            "Eval:  6\n",
            "75/80.0\n",
            "Eval:  6\n",
            "76/80.0\n",
            "Eval:  6\n",
            "77/80.0\n",
            "Eval:  6\n",
            "78/80.0\n",
            "Eval:  6\n",
            "79/80.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.55      0.55        67\n",
            "           1       0.73      0.73      0.73      1206\n",
            "           2       0.49      0.39      0.44       119\n",
            "           3       0.73      0.61      0.66      1261\n",
            "           4       0.48      0.66      0.56       263\n",
            "           5       0.46      0.83      0.59      1504\n",
            "           6       0.77      0.54      0.63      1979\n",
            "           7       0.31      0.37      0.34       123\n",
            "           8       0.60      0.65      0.62       337\n",
            "           9       0.71      0.65      0.68       240\n",
            "          10       0.55      0.31      0.40        74\n",
            "          11       0.71      0.25      0.37       562\n",
            "          12       0.39      0.29      0.33       297\n",
            "          13       0.49      0.50      0.49       133\n",
            "          14       0.64      0.67      0.65      1810\n",
            "          15       0.73      0.46      0.56       120\n",
            "          16       0.15      0.06      0.09       145\n",
            "\n",
            "    accuracy                           0.61     10240\n",
            "   macro avg       0.56      0.50      0.51     10240\n",
            "weighted avg       0.64      0.61      0.60     10240\n",
            "\n",
            "Epoch:  7\n",
            "0/240.0 loss: 0.528561532497406 \n",
            "Epoch:  7\n",
            "1/240.0 loss: 0.5248455107212067 \n",
            "Epoch:  7\n",
            "2/240.0 loss: 0.483347882827123 \n",
            "Epoch:  7\n",
            "3/240.0 loss: 0.46935252100229263 \n",
            "Epoch:  7\n",
            "4/240.0 loss: 0.47158716320991517 \n",
            "Epoch:  7\n",
            "5/240.0 loss: 0.46762295564015705 \n",
            "Epoch:  7\n",
            "6/240.0 loss: 0.4721034126622336 \n",
            "Epoch:  7\n",
            "7/240.0 loss: 0.48194514587521553 \n",
            "Epoch:  7\n",
            "8/240.0 loss: 0.471679644452201 \n",
            "Epoch:  7\n",
            "9/240.0 loss: 0.46084190607070924 \n",
            "Epoch:  7\n",
            "10/240.0 loss: 0.44920956817540253 \n",
            "Epoch:  7\n",
            "11/240.0 loss: 0.4528244311610858 \n",
            "Epoch:  7\n",
            "12/240.0 loss: 0.44628368661953854 \n",
            "Epoch:  7\n",
            "13/240.0 loss: 0.438865738255637 \n",
            "Epoch:  7\n",
            "14/240.0 loss: 0.44239235122998555 \n",
            "Epoch:  7\n",
            "15/240.0 loss: 0.44633146934211254 \n",
            "Epoch:  7\n",
            "16/240.0 loss: 0.4458618216654834 \n",
            "Epoch:  7\n",
            "17/240.0 loss: 0.44434377716647255 \n",
            "Epoch:  7\n",
            "18/240.0 loss: 0.4430406407306069 \n",
            "Epoch:  7\n",
            "19/240.0 loss: 0.4332322083413601 \n",
            "Epoch:  7\n",
            "20/240.0 loss: 0.4273468050218764 \n",
            "Epoch:  7\n",
            "21/240.0 loss: 0.4261753105304458 \n",
            "Epoch:  7\n",
            "22/240.0 loss: 0.42221519027067267 \n",
            "Epoch:  7\n",
            "23/240.0 loss: 0.4198416080325842 \n",
            "Epoch:  7\n",
            "24/240.0 loss: 0.41822973906993866 \n",
            "Epoch:  7\n",
            "25/240.0 loss: 0.4151684120297432 \n",
            "Epoch:  7\n",
            "26/240.0 loss: 0.41183741169947163 \n",
            "Epoch:  7\n",
            "27/240.0 loss: 0.4105704772685255 \n",
            "Epoch:  7\n",
            "28/240.0 loss: 0.4142008634476826 \n",
            "Epoch:  7\n",
            "29/240.0 loss: 0.41178513318300247 \n",
            "Epoch:  7\n",
            "30/240.0 loss: 0.40687400919775807 \n",
            "Epoch:  7\n",
            "31/240.0 loss: 0.40482849022373557 \n",
            "Epoch:  7\n",
            "32/240.0 loss: 0.41039141696510895 \n",
            "Epoch:  7\n",
            "33/240.0 loss: 0.4071558315087767 \n",
            "Epoch:  7\n",
            "34/240.0 loss: 0.40486509501934054 \n",
            "Epoch:  7\n",
            "35/240.0 loss: 0.40581870451569557 \n",
            "Epoch:  7\n",
            "36/240.0 loss: 0.40335851464722605 \n",
            "Epoch:  7\n",
            "37/240.0 loss: 0.4045682137734012 \n",
            "Epoch:  7\n",
            "38/240.0 loss: 0.40325565254076934 \n",
            "Epoch:  7\n",
            "39/240.0 loss: 0.4015513863414526 \n",
            "Epoch:  7\n",
            "40/240.0 loss: 0.4030787425070274 \n",
            "Epoch:  7\n",
            "41/240.0 loss: 0.400346764141605 \n",
            "Epoch:  7\n",
            "42/240.0 loss: 0.3975705743529076 \n",
            "Epoch:  7\n",
            "43/240.0 loss: 0.3968759175728668 \n",
            "Epoch:  7\n",
            "44/240.0 loss: 0.39630325006114114 \n",
            "Epoch:  7\n",
            "45/240.0 loss: 0.39741548280353134 \n",
            "Epoch:  7\n",
            "46/240.0 loss: 0.3959929312163211 \n",
            "Epoch:  7\n",
            "47/240.0 loss: 0.3932417528703809 \n",
            "Epoch:  7\n",
            "48/240.0 loss: 0.3923609375345464 \n",
            "Epoch:  7\n",
            "49/240.0 loss: 0.39420754462480545 \n",
            "Epoch:  7\n",
            "50/240.0 loss: 0.3948492317223081 \n",
            "Epoch:  7\n",
            "51/240.0 loss: 0.39426014543725896 \n",
            "Epoch:  7\n",
            "52/240.0 loss: 0.39383188986553336 \n",
            "Epoch:  7\n",
            "53/240.0 loss: 0.3939261235020779 \n",
            "Epoch:  7\n",
            "54/240.0 loss: 0.3940299389037219 \n",
            "Epoch:  7\n",
            "55/240.0 loss: 0.3937929477542639 \n",
            "Epoch:  7\n",
            "56/240.0 loss: 0.3936032477700919 \n",
            "Epoch:  7\n",
            "57/240.0 loss: 0.39385920223490944 \n",
            "Epoch:  7\n",
            "58/240.0 loss: 0.39350586144600885 \n",
            "Epoch:  7\n",
            "59/240.0 loss: 0.39315394883354504 \n",
            "Epoch:  7\n",
            "60/240.0 loss: 0.3917199742110049 \n",
            "Epoch:  7\n",
            "61/240.0 loss: 0.3910863647057164 \n",
            "Epoch:  7\n",
            "62/240.0 loss: 0.3902168493895304 \n",
            "Epoch:  7\n",
            "63/240.0 loss: 0.3900361356791109 \n",
            "Epoch:  7\n",
            "64/240.0 loss: 0.39016876151928537 \n",
            "Epoch:  7\n",
            "65/240.0 loss: 0.3897768288399234 \n",
            "Epoch:  7\n",
            "66/240.0 loss: 0.39016427766920914 \n",
            "Epoch:  7\n",
            "67/240.0 loss: 0.39003729053279934 \n",
            "Epoch:  7\n",
            "68/240.0 loss: 0.3900157958269119 \n",
            "Epoch:  7\n",
            "69/240.0 loss: 0.388554672896862 \n",
            "Epoch:  7\n",
            "70/240.0 loss: 0.3893367755161205 \n",
            "Epoch:  7\n",
            "71/240.0 loss: 0.39004507722953957 \n",
            "Epoch:  7\n",
            "72/240.0 loss: 0.3905842216455773 \n",
            "Epoch:  7\n",
            "73/240.0 loss: 0.390228140797164 \n",
            "Epoch:  7\n",
            "74/240.0 loss: 0.3889429626862208 \n",
            "Epoch:  7\n",
            "75/240.0 loss: 0.38962369706285627 \n",
            "Epoch:  7\n",
            "76/240.0 loss: 0.3880384059308411 \n",
            "Epoch:  7\n",
            "77/240.0 loss: 0.3872407840994688 \n",
            "Epoch:  7\n",
            "78/240.0 loss: 0.38498849865001966 \n",
            "Epoch:  7\n",
            "79/240.0 loss: 0.38519156668335197 \n",
            "Epoch:  7\n",
            "80/240.0 loss: 0.3856760140186475 \n",
            "Epoch:  7\n",
            "81/240.0 loss: 0.3873368263608072 \n",
            "Epoch:  7\n",
            "82/240.0 loss: 0.38702144518674136 \n",
            "Epoch:  7\n",
            "83/240.0 loss: 0.3851165239300047 \n",
            "Epoch:  7\n",
            "84/240.0 loss: 0.3842951420475455 \n",
            "Epoch:  7\n",
            "85/240.0 loss: 0.3842428780572359 \n",
            "Epoch:  7\n",
            "86/240.0 loss: 0.38427701489678745 \n",
            "Epoch:  7\n",
            "87/240.0 loss: 0.38414808498187497 \n",
            "Epoch:  7\n",
            "88/240.0 loss: 0.38445288913973263 \n",
            "Epoch:  7\n",
            "89/240.0 loss: 0.38339472313721973 \n",
            "Epoch:  7\n",
            "90/240.0 loss: 0.3826419688188113 \n",
            "Epoch:  7\n",
            "91/240.0 loss: 0.38259255043838336 \n",
            "Epoch:  7\n",
            "92/240.0 loss: 0.3827508325858783 \n",
            "Epoch:  7\n",
            "93/240.0 loss: 0.38178811999077494 \n",
            "Epoch:  7\n",
            "94/240.0 loss: 0.38097777209783856 \n",
            "Epoch:  7\n",
            "95/240.0 loss: 0.37985575447479886 \n",
            "Epoch:  7\n",
            "96/240.0 loss: 0.3785367420653707 \n",
            "Epoch:  7\n",
            "97/240.0 loss: 0.3772430003297572 \n",
            "Epoch:  7\n",
            "98/240.0 loss: 0.3755721812597429 \n",
            "Epoch:  7\n",
            "99/240.0 loss: 0.37549763545393944 \n",
            "Epoch:  7\n",
            "100/240.0 loss: 0.37495333856285207 \n",
            "Epoch:  7\n",
            "101/240.0 loss: 0.3742899133586416 \n",
            "Epoch:  7\n",
            "102/240.0 loss: 0.37373172672628197 \n",
            "Epoch:  7\n",
            "103/240.0 loss: 0.37358339546391595 \n",
            "Epoch:  7\n",
            "104/240.0 loss: 0.37287692385060445 \n",
            "Epoch:  7\n",
            "105/240.0 loss: 0.37120053650073287 \n",
            "Epoch:  7\n",
            "106/240.0 loss: 0.37017127871513367 \n",
            "Epoch:  7\n",
            "107/240.0 loss: 0.368744820356369 \n",
            "Epoch:  7\n",
            "108/240.0 loss: 0.3686154503887946 \n",
            "Epoch:  7\n",
            "109/240.0 loss: 0.36763913577253166 \n",
            "Epoch:  7\n",
            "110/240.0 loss: 0.36765661057051235 \n",
            "Epoch:  7\n",
            "111/240.0 loss: 0.36712635920516085 \n",
            "Epoch:  7\n",
            "112/240.0 loss: 0.36720491246839543 \n",
            "Epoch:  7\n",
            "113/240.0 loss: 0.3669977449534232 \n",
            "Epoch:  7\n",
            "114/240.0 loss: 0.3683283945788508 \n",
            "Epoch:  7\n",
            "115/240.0 loss: 0.36858344103755625 \n",
            "Epoch:  7\n",
            "116/240.0 loss: 0.36973681587439317 \n",
            "Epoch:  7\n",
            "117/240.0 loss: 0.36862710143549965 \n",
            "Epoch:  7\n",
            "118/240.0 loss: 0.3675124849341497 \n",
            "Epoch:  7\n",
            "119/240.0 loss: 0.3661053283760945 \n",
            "Epoch:  7\n",
            "120/240.0 loss: 0.3660438931431652 \n",
            "Epoch:  7\n",
            "121/240.0 loss: 0.36522956351276303 \n",
            "Epoch:  7\n",
            "122/240.0 loss: 0.3653233963541868 \n",
            "Epoch:  7\n",
            "123/240.0 loss: 0.3644521540451434 \n",
            "Epoch:  7\n",
            "124/240.0 loss: 0.3642054406404495 \n",
            "Epoch:  7\n",
            "125/240.0 loss: 0.36331980036837713 \n",
            "Epoch:  7\n",
            "126/240.0 loss: 0.3625521278522146 \n",
            "Epoch:  7\n",
            "127/240.0 loss: 0.3620012680767104 \n",
            "Epoch:  7\n",
            "128/240.0 loss: 0.3615343028953833 \n",
            "Epoch:  7\n",
            "129/240.0 loss: 0.3614040595980791 \n",
            "Epoch:  7\n",
            "130/240.0 loss: 0.3606677076971258 \n",
            "Epoch:  7\n",
            "131/240.0 loss: 0.36026276140050456 \n",
            "Epoch:  7\n",
            "132/240.0 loss: 0.3596045821905136 \n",
            "Epoch:  7\n",
            "133/240.0 loss: 0.35932875135496484 \n",
            "Epoch:  7\n",
            "134/240.0 loss: 0.35864719649155935 \n",
            "Epoch:  7\n",
            "135/240.0 loss: 0.3583494276904008 \n",
            "Epoch:  7\n",
            "136/240.0 loss: 0.35856830845349025 \n",
            "Epoch:  7\n",
            "137/240.0 loss: 0.35886643664992374 \n",
            "Epoch:  7\n",
            "138/240.0 loss: 0.3586367884342619 \n",
            "Epoch:  7\n",
            "139/240.0 loss: 0.35784883584295 \n",
            "Epoch:  7\n",
            "140/240.0 loss: 0.3583413476639606 \n",
            "Epoch:  7\n",
            "141/240.0 loss: 0.35803203242765347 \n",
            "Epoch:  7\n",
            "142/240.0 loss: 0.35855561557349624 \n",
            "Epoch:  7\n",
            "143/240.0 loss: 0.3579154991441303 \n",
            "Epoch:  7\n",
            "144/240.0 loss: 0.35861741386610885 \n",
            "Epoch:  7\n",
            "145/240.0 loss: 0.3587326115533097 \n",
            "Epoch:  7\n",
            "146/240.0 loss: 0.35896018350205455 \n",
            "Epoch:  7\n",
            "147/240.0 loss: 0.3589460581943795 \n",
            "Epoch:  7\n",
            "148/240.0 loss: 0.35897854750588437 \n",
            "Epoch:  7\n",
            "149/240.0 loss: 0.3578099066019058 \n",
            "Epoch:  7\n",
            "150/240.0 loss: 0.358278296838533 \n",
            "Epoch:  7\n",
            "151/240.0 loss: 0.36001438784756157 \n",
            "Epoch:  7\n",
            "152/240.0 loss: 0.3591030233241374 \n",
            "Epoch:  7\n",
            "153/240.0 loss: 0.3592346508975153 \n",
            "Epoch:  7\n",
            "154/240.0 loss: 0.3587829460059443 \n",
            "Epoch:  7\n",
            "155/240.0 loss: 0.3598020291672303 \n",
            "Epoch:  7\n",
            "156/240.0 loss: 0.36090186912163047 \n",
            "Epoch:  7\n",
            "157/240.0 loss: 0.3606183599633507 \n",
            "Epoch:  7\n",
            "158/240.0 loss: 0.3604398901170155 \n",
            "Epoch:  7\n",
            "159/240.0 loss: 0.3604121689684689 \n",
            "Epoch:  7\n",
            "160/240.0 loss: 0.35952508310723746 \n",
            "Epoch:  7\n",
            "161/240.0 loss: 0.359521936579251 \n",
            "Epoch:  7\n",
            "162/240.0 loss: 0.3603451269353094 \n",
            "Epoch:  7\n",
            "163/240.0 loss: 0.35992737541474945 \n",
            "Epoch:  7\n",
            "164/240.0 loss: 0.3596292539979472 \n",
            "Epoch:  7\n",
            "165/240.0 loss: 0.3591630140162376 \n",
            "Epoch:  7\n",
            "166/240.0 loss: 0.35892329628239134 \n",
            "Epoch:  7\n",
            "167/240.0 loss: 0.35908919084994567 \n",
            "Epoch:  7\n",
            "168/240.0 loss: 0.35872614780473994 \n",
            "Epoch:  7\n",
            "169/240.0 loss: 0.3590436201761751 \n",
            "Epoch:  7\n",
            "170/240.0 loss: 0.3589685394575721 \n",
            "Epoch:  7\n",
            "171/240.0 loss: 0.3588226065905981 \n",
            "Epoch:  7\n",
            "172/240.0 loss: 0.35899217288962676 \n",
            "Epoch:  7\n",
            "173/240.0 loss: 0.359299931803654 \n",
            "Epoch:  7\n",
            "174/240.0 loss: 0.3601742735079357 \n",
            "Epoch:  7\n",
            "175/240.0 loss: 0.3600318861109289 \n",
            "Epoch:  7\n",
            "176/240.0 loss: 0.3603890527271282 \n",
            "Epoch:  7\n",
            "177/240.0 loss: 0.3604141357407141 \n",
            "Epoch:  7\n",
            "178/240.0 loss: 0.3598943796404247 \n",
            "Epoch:  7\n",
            "179/240.0 loss: 0.3596623082127836 \n",
            "Epoch:  7\n",
            "180/240.0 loss: 0.3592043063752559 \n",
            "Epoch:  7\n",
            "181/240.0 loss: 0.3590533501663051 \n",
            "Epoch:  7\n",
            "182/240.0 loss: 0.3587871453638285 \n",
            "Epoch:  7\n",
            "183/240.0 loss: 0.35879347400496836 \n",
            "Epoch:  7\n",
            "184/240.0 loss: 0.35886566872532305 \n",
            "Epoch:  7\n",
            "185/240.0 loss: 0.3594106108110438 \n",
            "Epoch:  7\n",
            "186/240.0 loss: 0.35934052835492525 \n",
            "Epoch:  7\n",
            "187/240.0 loss: 0.35890591453681603 \n",
            "Epoch:  7\n",
            "188/240.0 loss: 0.35840451599113526 \n",
            "Epoch:  7\n",
            "189/240.0 loss: 0.3580449603890118 \n",
            "Epoch:  7\n",
            "190/240.0 loss: 0.3579920078603385 \n",
            "Epoch:  7\n",
            "191/240.0 loss: 0.35794685369667906 \n",
            "Epoch:  7\n",
            "192/240.0 loss: 0.35755008559461704 \n",
            "Epoch:  7\n",
            "193/240.0 loss: 0.3573978003306487 \n",
            "Epoch:  7\n",
            "194/240.0 loss: 0.35712831027996844 \n",
            "Epoch:  7\n",
            "195/240.0 loss: 0.3576959249164377 \n",
            "Epoch:  7\n",
            "196/240.0 loss: 0.35758719294506885 \n",
            "Epoch:  7\n",
            "197/240.0 loss: 0.35836376392781133 \n",
            "Epoch:  7\n",
            "198/240.0 loss: 0.358731834208546 \n",
            "Epoch:  7\n",
            "199/240.0 loss: 0.35835409559309483 \n",
            "Epoch:  7\n",
            "200/240.0 loss: 0.3582930068916349 \n",
            "Epoch:  7\n",
            "201/240.0 loss: 0.35827604209137437 \n",
            "Epoch:  7\n",
            "202/240.0 loss: 0.3575593828420921 \n",
            "Epoch:  7\n",
            "203/240.0 loss: 0.35713354593106345 \n",
            "Epoch:  7\n",
            "204/240.0 loss: 0.35669496968025116 \n",
            "Epoch:  7\n",
            "205/240.0 loss: 0.3568552980579219 \n",
            "Epoch:  7\n",
            "206/240.0 loss: 0.3573319231686385 \n",
            "Epoch:  7\n",
            "207/240.0 loss: 0.35730715131816954 \n",
            "Epoch:  7\n",
            "208/240.0 loss: 0.35665726597514447 \n",
            "Epoch:  7\n",
            "209/240.0 loss: 0.3568697935768536 \n",
            "Epoch:  7\n",
            "210/240.0 loss: 0.35732997177053966 \n",
            "Epoch:  7\n",
            "211/240.0 loss: 0.35711893277629364 \n",
            "Epoch:  7\n",
            "212/240.0 loss: 0.3568528573697721 \n",
            "Epoch:  7\n",
            "213/240.0 loss: 0.3563690076225272 \n",
            "Epoch:  7\n",
            "214/240.0 loss: 0.3569638282060623 \n",
            "Epoch:  7\n",
            "215/240.0 loss: 0.35756833085583317 \n",
            "Epoch:  7\n",
            "216/240.0 loss: 0.3585666682451002 \n",
            "Epoch:  7\n",
            "217/240.0 loss: 0.3587358865305918 \n",
            "Epoch:  7\n",
            "218/240.0 loss: 0.358433520208755 \n",
            "Epoch:  7\n",
            "219/240.0 loss: 0.3585251922634515 \n",
            "Epoch:  7\n",
            "220/240.0 loss: 0.3585323124314865 \n",
            "Epoch:  7\n",
            "221/240.0 loss: 0.3582388297528834 \n",
            "Epoch:  7\n",
            "222/240.0 loss: 0.35813779073178503 \n",
            "Epoch:  7\n",
            "223/240.0 loss: 0.35808569013274144 \n",
            "Epoch:  7\n",
            "224/240.0 loss: 0.3587562643819385 \n",
            "Epoch:  7\n",
            "225/240.0 loss: 0.3592472363230401 \n",
            "Epoch:  7\n",
            "226/240.0 loss: 0.35888455900064126 \n",
            "Epoch:  7\n",
            "227/240.0 loss: 0.358591474004482 \n",
            "Epoch:  7\n",
            "228/240.0 loss: 0.35835531256344644 \n",
            "Epoch:  7\n",
            "229/240.0 loss: 0.35852628862080366 \n",
            "Epoch:  7\n",
            "230/240.0 loss: 0.3581940920709015 \n",
            "Epoch:  7\n",
            "231/240.0 loss: 0.35824157817867297 \n",
            "Epoch:  7\n",
            "232/240.0 loss: 0.35790194097762457 \n",
            "Epoch:  7\n",
            "233/240.0 loss: 0.3571653064244833 \n",
            "Epoch:  7\n",
            "234/240.0 loss: 0.3569306516900976 \n",
            "Epoch:  7\n",
            "235/240.0 loss: 0.35658084386486116 \n",
            "Epoch:  7\n",
            "236/240.0 loss: 0.35684439024341763 \n",
            "Epoch:  7\n",
            "237/240.0 loss: 0.35677759712483703 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTUPOAQhSy4o",
        "colab_type": "text"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_iL-xrxS1Nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "\n",
        "        probas = bert_clf(token_ids, masks)\n",
        "        \n",
        "        bert_predicted += list(torch.max(probas,1)[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQluUbtSTF7s",
        "colab_type": "code",
        "outputId": "5dd2ab50-7bbe-4743-c2e6-d3e48fedbab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print(classification_report(test_y_tensor, bert_predicted))\n",
        "print(confusion_matrix(test_y_tensor, bert_predicted))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93       229\n",
            "           1       0.84      0.94      0.88       175\n",
            "           2       0.92      0.93      0.93       517\n",
            "           3       0.80      0.80      0.80       129\n",
            "           4       0.98      0.94      0.96       485\n",
            "           5       0.89      0.83      0.86        65\n",
            "\n",
            "    accuracy                           0.92      1600\n",
            "   macro avg       0.89      0.89      0.89      1600\n",
            "weighted avg       0.92      0.92      0.92      1600\n",
            "\n",
            "[[212   8   6   0   3   0]\n",
            " [  2 164   1   0   4   4]\n",
            " [  5   3 480  24   2   3]\n",
            " [  0   0  25 103   1   0]\n",
            " [  8  12   5   2 458   0]\n",
            " [  0   9   2   0   0  54]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-dvetMCivH3",
        "colab_type": "code",
        "outputId": "dfc6c5ec-f572-4860-d77e-a123da10bd84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "print(classification_report(test_y_tensor, bert_predicted))\n",
        "print(confusion_matrix(test_y_tensor, bert_predicted))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        20\n",
            "           1       0.58      0.75      0.65       292\n",
            "           2       0.00      0.00      0.00        41\n",
            "           3       0.53      0.69      0.60       291\n",
            "           4       0.62      0.11      0.18        75\n",
            "           5       0.47      0.60      0.52       374\n",
            "           6       0.63      0.62      0.63       524\n",
            "           7       0.00      0.00      0.00        31\n",
            "           8       0.41      0.44      0.43        99\n",
            "           9       0.50      0.51      0.51        49\n",
            "          10       0.00      0.00      0.00        24\n",
            "          11       0.33      0.31      0.32       141\n",
            "          12       0.00      0.00      0.00        18\n",
            "          13       1.00      0.05      0.10        40\n",
            "          14       0.56      0.62      0.59       458\n",
            "          15       0.00      0.00      0.00        24\n",
            "          16       0.00      0.00      0.00        47\n",
            "          17       0.00      0.00      0.00        12\n",
            "\n",
            "    accuracy                           0.54      2560\n",
            "   macro avg       0.31      0.26      0.25      2560\n",
            "weighted avg       0.50      0.54      0.50      2560\n",
            "\n",
            "[[  0   9   0   4   0   2   0   0   2   0   0   0   0   0   3   0   0   0]\n",
            " [  0 218   0  47   1   2   1   0   9   8   0   2   0   0   4   0   0   0]\n",
            " [  0  14   0  18   1   1   1   0   5   1   0   0   0   0   0   0   0   0]\n",
            " [  0  47   0 202   2   1   9   0  18   9   0   2   0   0   1   0   0   0]\n",
            " [  0  29   0  25   8   5   0   0   7   1   0   0   0   0   0   0   0   0]\n",
            " [  0   4   0   2   0 223  71   0   3   0   0  17   0   0  54   0   0   0]\n",
            " [  0   1   0   3   0 113 327   0   8   3   0  21   0   0  48   0   0   0]\n",
            " [  0  15   0   5   1   0   2   0   7   1   0   0   0   0   0   0   0   0]\n",
            " [  0  20   0  29   0   0   3   0  44   2   0   0   0   0   1   0   0   0]\n",
            " [  0  11   0   9   0   1   2   0   1  25   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   5   2   0   0   0   0   0   0   0  17   0   0   0]\n",
            " [  0   0   0   1   0  17  25   0   0   0   0  44   0   0  54   0   0   0]\n",
            " [  0   0   0   0   0   2   5   0   0   0   0   0   0   0  11   0   0   0]\n",
            " [  0   1   0  33   0   0   2   0   1   0   0   0   0   2   1   0   0   0]\n",
            " [  0   5   0   2   0  72  49   0   2   0   0  46   0   0 282   0   0   0]\n",
            " [  0   1   0   0   0  12   1   0   0   0   0   0   0   0  10   0   0   0]\n",
            " [  0   1   0   0   0  15  17   0   1   0   0   3   0   0  10   0   0   0]\n",
            " [  0   0   0   0   0   5   2   0   0   0   0   0   0   0   5   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}