{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Logistic PyTorch.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwFZsMuvOeHd",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch Logistic Regression example\n",
        "\n",
        "### Moshe Wasserblat <br><br> March 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytNZMY1Tfau6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification,make_regression\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40rq_w-iOeHp",
        "colab_type": "text"
      },
      "source": [
        "### Input feature and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI_8q9A85Fq8",
        "colab_type": "code",
        "outputId": "511cf9c7-e6e7-4f95-c273-80787f39359b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,\n",
        "\tn_clusters_per_class=2, weights=[0.50], flip_y=0, random_state=4)\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "\n",
        "# scatter plot of examples by class label\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = np.where(y == label)[0]\n",
        "\tplt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "X = torch.from_numpy(X)\n",
        "y = torch.from_numpy(y)\n",
        "\n",
        "X.shape\n"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({1.0: 500, 0.0: 500})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nO29e5QcVbn//X26pybTE3Amk0QhMwmJ\nhhcXl0DMoLyGnx6JgoAkMWhQjhde5eR41IOgKxCENyQ5KoGcI+iRsySCB/mJkAHCEG4CBvz5C783\nQEKSgSAYFSEzAU1CJpJMZ6ane79/VFdPdfXeVbsufanu57MWK0x1XXZXVz372c+VhBBgGIZh4kui\n2gNgGIZhwsGCnGEYJuawIGcYhok5LMgZhmFiDgtyhmGYmNNUjYtOmjRJTJ8+vRqXZhiGiS1bt27d\nJ4SY7NxeFUE+ffp0bNmypRqXZhiGiS1E9LpsO5tWGIZhYg4LcoZhmJjDgpxhGCbmVMVGLiOTyaC/\nvx9Hjhyp9lBC09LSgq6uLhiGUe2hMAzTANSMIO/v78fRRx+N6dOng4iqPZzACCGwf/9+9Pf3Y8aM\nGdUeDsMwDUDNCPIjR47EXogDABFh4sSJ2Lt3b7WHwjQgvdsGsObxV7FnMI0p7SksPecELJzdWe1h\nMWWmZgQ5gNgLcYt6+R71QqMIt95tA7h6/YtIZ7IAgIHBNK5e/yIA1OX3ZcZgZydT11jCbWAwDYEx\n4da7baDaQ4ucNY+/WhDiFulMFmsef7VKI2IqRWhBTkQtRPQcEe0gop1EtDKKgVWDr3zlK3j3u9+N\nk08+Wfq5EAKXXXYZZs6ciVmzZuGFF16o8AgZv8RRuPVuG8Dc1U9hxrJHMHf1U9qTzp7BtK/tTP0Q\nhUY+DOAsIcSpAE4D8EkiOiOC81acSy65BL/+9a+Vnz/22GPYtWsXdu3ahbVr1+Jf/uVfKjg6Jghx\nE25hVhBT2lO+tjP1Q2hBLkwO5f808v+Vve1QUK3FjY985CPo6OhQfv7ggw/iS1/6EogIZ5xxBgYH\nB/Hmm2+Gvi5TPuIm3MKsIJaecwJSRrJoW8pIYuk5J0Q6Rqb2iMRGTkRJItoO4G8AnhRCPCvZZwkR\nbSGiLWEjOqpl9xwYGMDUqVMLf3d1dWFgoP5srfVE3IRbmBXEwtmduH7RKehsT4EAdLancP2iU9jR\n2QBEErUihMgCOI2I2gE8QEQnCyFecuyzFsBaAOju7g6lsbtpLfzQMnas5yEuUStT2lMYkAht3RXE\nwtmdNfvdmPIRafihEGKQiJ4G8EkAL3ntH5Rq2T07Ozuxe/fuwt/9/f3o7OSXptaJk3Bbes4JRSGE\nQG2vIJjaIIqolcl5TRxElALwCQCvhD2vG9Wye86fPx933nknhBDYvHkz2tracOyxx5b1mkxjweYR\nJghRaOTHAvgFESVhTgw9QoiHIzivknJpLZ///Ofx29/+Fvv27UNXVxdWrlyJTCYDAPja176G8847\nD48++ihmzpyJ1tZW/Pd//3eo6zGMjDitIJjaILQgF0L0AZgdwVi0KZfd8+6773b9nIhwyy23hLoG\nwzBM1NRUir4fWGthGIYx4RR9hmGYmBNbjZxhmPD4LSjWKAXI4gZr5AzToPhNrJPtf8W67bi298WK\njpsphTVyhokQu8baljJABAwOZSLTXqPUiP0m1sn2FwDu2vwGuo/rYM28irBGzjAR4dRYB9MZHBjK\nRFZGIurSFH4T61TbBVDT1SQbARbkDn7961/jhBNOwMyZM7F69eqSz4eHh3HRRRdh5syZ+NCHPoS/\n/OUvlR8kU5PINFY7YcvnRl2S129inVvCXdCs6nIUv2tEWJDbyGaz+MY3voHHHnsML7/8Mu6++268\n/PLLRfvcfvvtmDBhAv74xz/iiiuuwFVXXVWl0TKVREfgyGqkOAlTRiLq0hR+C4otPecEqHpfBcmq\nbqSmH+UmvoK8rwe46WRgRbv5b19P6FM+99xzmDlzJt773veiubkZn/vc5/Dggw8W7fPggw/iy1/+\nMgDgM5/5DDZu3Aghyl61l6kiOgKnd9uAUsjZCVNGIurSFH7LASyc3Yl/PGNayfcMmlVd6aYf9az9\nx9PZ2dcDPHQZkMlrIgd3m38DwKzFgU8rK1P77LPPKvdpampCW1sb9u/fj0mTJgW+LhMd5QiP03EK\nrnn8Vc8i/GHLSJSjNIXfxLrvLTwF3cd1RHKPK1n8rt77mcZTkG9cNSbELTJpc3sIQc7Em3K9rDoC\nx034EOBb4LlNSDpCVHa87rFeRJVVHbZkr59Ju95LX8dTkB/s97ddE50ytdY+XV1dGB0dxcGDBzFx\n4sRQ12WioVwvq47AUe3T2Z7CM8vO8nU9twkp6PFL79sBCCCTEyXnrJYgC7PC8Dtpx63ln1/iaSNv\n6/K3XZPTTz8du3btwmuvvYaRkRHcc889mD9/ftE+8+fPxy9+8QsAwH333YezzjoLRDrWUabclOtl\n1XEK+nUcutlrVRPSig07tZyDsuMzWVEQ4vZzVjNsMEzJXr/29bi1/PNLPDXyecuLbeQAYKTM7SFo\namrCT37yE5xzzjnIZrP4yle+gpNOOgnLly9Hd3c35s+fj69+9av44he/iJkzZ6KjowP33HNPyC/D\nREXYpboKHZOGX7OHmzapmngG05mSbbIVh5+Jq9oaaVAzjd9Ju94bdsRTkFt28I2rTHNKW5cpxCOw\nj5933nk477zziratWrWq8P8tLS249957Q1+HiZ5yvqw6AkdXKHmZgFQTkgq78OrdNoAEEbKakVRx\n1Uj9Ttpxa/nnl3gKcsAU2uzYZGzE5WX10iZVE1KLkcCBoVKt3BJelqYvE+JGkops5NY5de3RtXZP\ng0za9Vz6Or6CnGEkqF7WWhJGXtqkc0KyarYcGMqAgKIwR7vwUmWWJomw5jOnFp1T9x6EigTq6ynL\nqtl+7Vr5TatNTQlyIURdOA45Qai2qLUYYh1t0pqQnGMXQEGYdzqEl0rTzwlR2Mfv9w0cCVSmXA87\n9axh+yW0ICeiqQDuBPAemM/XWiHEj/yep6WlBfv378fEiRNjLcyFENi/fz9aWlqqPZSGQ6V1hwlL\nLIcm70ebVFUclIU1lsPZGzgSiHM9KkoUGvkogO8IIV4goqMBbCWiJ4UQL3sdaKerqwv9/f3Yu3dv\nBEOqLi0tLejqChcKyfjDTesOKoxk57x83XasfGgnrrvgpBLB60fo62qTXmO3X7O91YCRoEB2cBWB\nJ4cy5XowcqJovvwmgDfz//8OEf0eQCcAX4LcMAzMmDEj7HCYBsVN6w4qjFQ25wNDmRLTTLnMN25j\n7902gKX37UAmKwrjSiYI7SkDB9PR1ED3MgMpJ6+2LtOc4iRkrgcjJ9KEICKaDmA2gGfd92SYaHHT\nXP0m63idEyhNPilXASjZ2I0k4fDwKC5ft70gxC2yOQEi4LXV5+OZZWdFYga6cE4nknlzZ5IIH5jW\nhjWPv4rpyx7BFeu2yxOU5i03czuKBh4+14ORE5mzk4iOAnA/gMuFEH+XfL4EwBIAmDZtWlSXZRgA\n7pqr3wgHS8v0clkPDKYxd/VT2JMXZDJUk4GuGcY59vZWA4eOjEqTgyxkIYpBr9+7bQD3bx0ohDRm\nhcAzf3q78Lnzexd8D8vKl+tRK9RSJFQkgpyIDJhC/C4hxHrZPkKItQDWAkB3dzeHdTCBUL08H3v/\nZPxy8xsl+3/s/ZMBeNukrfMODKZLQvxUELxrkMvMN06TSKEWCopNNc7wQwD4e3pUK9mnd9uA60Sl\nawbyapYhozB51XGuR61FQoU2rZAZYnI7gN8LIX4YfkgMI8etLvjTr8id5KrtqvMC+kI8SNna3m0D\nuKKn1CSSyQqsfGhnyXicLeN0MzbdGjT4MQMFSeGPa7aoHypdS92LKGzkcwF8EcBZRLQ9/995Xgcx\njF/cXp4wBbN0tU57cScvcSorAGUJaJUstkwiQbRgJ25Cxc+98iuU66l+iRu1Vk0xiqiVTYBWcxSG\nCYXbyxMmhlrn5XPGbc9d/ZTSrJIkktpLdQV0VMJAdR4/90oWteKGbvXCuFOuAm1BiWcZW6bqVKNt\nllsp0qCRKQDQljJcP5edx+28WSGkpg0dAT139VOe4wHMycJaHUxole+vul9+7pWs1Oz45mTJfsh/\n1ghCHPBftrjc1FSKPlN7qDrNuDU+KJcn3y2mOWjtjd5tA/j7EXWUR3ve0Xj5uu34Ts8OZIUopMa3\npwxl9Igsc1SnquHAYBpGkkoSe+ykjGSR5ut0vAFjjti5q58KVXLX2l9mIqrXkrA61FqtF6pGXZDu\n7m6xZcuWil+3kQkSKqV6Ycc1JaQCrD1lYHg0V7J/lMtte3RJMl+u1VlzxM+5vMwGKSMp/TxlJHHh\nnE7cv3XA9Xj72HSuZzGh1UBrc1NR1MrgkDrJxy3qRuc38Pt81FLoXSNBRFuFEN0l21mQ1z8qgez1\ncrvZgf0QpN2ZG0G+j2wCSPqo2y3DEtKWpq7CPrZre1/EXZvf8HSWEsykHr+ofjO33yDo88FUHpUg\nZxt5AxA0VKrcTjc/2G3y3+nZ4ev7OMML7cktYdgzmMbC2Z34/Iemuu5nH9vTr+zVCm8M6jQLEk1R\niVC6avhUGgm2kTcAQUOlVDbdCa0GjmRKTShejQ/84CwGdejIaMFmrBLAqu8TRTifDOt76cSqW2PT\nmdTC2JuDRFOUO5Su1pJn6hEW5A2A6uVubzUKKeYyO6cqW/L8Wcei+7gOTyco4F8o9W4bwMqHdhZN\nCDop59b3lFGO2N6UkcT0iSm87+pHtTR7a2yq3yJJhJwQaG81IARwxbrtWPP4q/jY+yfj6Vf2atui\ng3TOKXcoXZgywoweLMgbANnLbSQJh46MFoTkwGAaV6zbji2vv43vLTwFgFrTfPqVvfjeQrX9NKgT\nzI8z0IlVSGrGskcwpT1VJAD99LDUYUKrgROPPbqo5ogbdkGqErTXLzLvuVNztU+kKk3W6Xi8cE5n\n2YW/H/yU4mXHaTDY2dkgyJx9Kia0GhjMp4TLkDniongZ/TpXiaDMkiwnne0pvHXwiOs9VHXxAdT3\nSvf7W9q7NWGte353Ucq/kTRbu/m5/1ELU/v5VBNpZ378TudvvTpao7jHHLVSp6jivGUPTBiN144z\nAkIWieHnZbRPMroYCQIIJTVLKoFXnZWgYZEzlj2i5QjVYUKrgW3Lzy7aVinNVzes88I5ncoInqgj\nnapNVJFBKkHOppUYoHoBZU6kpfftKOqWbl+OR+H0cy65e7cNSF9GP63Ugk0uAmXwX2oxxUMjt7Zb\nHYUuX7ddS6jrJAzp4vQreDkcoxTyXs8ZAQXzj9/yv3Gl3H4CFuRVwM9L4/YCyh4OmYbqVVhKBwKk\nY3Wr2+12vSBauJ1MLtBhvkkAsF/Kmsju3fKGto0c0IvU8FvXxAt7VqdXiGGUUSVez5kACjZ8t31m\nr3oCQgAH0xmtpKhaptyRQSzIK4zfUCzVC3j5uu2+rutWWEoH1cvj9iCqaoZEZeIpJ50uZioAeO61\nA77Pmc5ksWLDTuXqyvqtwyYqWej2LdXWFvt6tBpF6JYi6PTYz76qsGcSu74zmmOsNOWODOKEoArj\nN/kiqhm7vdVQtg1rTxme5SutqJZre18s2u72IJLtpF4JPbXKwtmdeGbZWUWt01Zs2Kmsg+LFYDpT\nUk/92t4XpQlLUWDvWypjSntKT1vs6wEeuizfh1OY/z50mbndwdJzTvB8nqwKkc7nURfpO+NjjJWm\n3EW2WJBXGL9LrKhm7ENHRgGgpJLdms+ciu3XnY2bLjrN8xwCwF2b3yjKynN7EAfzGpWzUUKUgqpc\nFPWfdODWZs0v6UwWdz+7u6wT28BgGoeHR0u2p4yk2UFJIXUt88ZpK5/A/vuuADKOZzSTxlvrv1ty\njxbO7vR02maFKKms6JeSd2bjKukYsXFVgLNHi6yKZJSROWxaqTB+l1hR2U0zOYE1j7+qbMirm44t\nYCarWKYdAjCuKYHh0VKj9ZT2FK7tfVGaVBQH0pksvtNT3IKtHJR7YiOUTj4TWg2cP+tYrHtut2sI\n54GhDOYnNqHDOCT9/N1in9TM4WU26cw/7/bKin7DT0vemYP98h1V2yuMV7vBMLBGXmH8LrGsmTxJ\n4Xt3uL0kfl4g4fj/4dEckoni8VmZj3EV4hay2uKq+t+1iCpUsrW5CU+/slfLRHRlUw9Uj98eMVFq\n5nAzm6iedz+mFuk52rrkO6u21xEsyCuM2xJLVVho4exO/MfiU83Y6ZA4l8G92wZw2sonQp83lxMl\n32nzn/07BGsRp6A6f9axFbkuAUgZeq+odd/nvq+jMOkniVwjinT9L1Non3S7EMCNo4sL57OzcHYn\nLpzTWaKAuJkUnO/GhFaj4L9pTxmY0Gq4myXmLQcMh5ZupMztDnSLeMWl2BcnBNUIOgkDvdsGsGLD\nzsIyeXxzEiOjOV+ON/s5o44eIaAoTKz2LeH+0Wm6XGms5Bk/v2d7ysD4cU1aK7FNzZehK1EqzN8W\nR+EDw2uLxmChanbxj2dMK5SAKAsaUSu6yTm1WN63rAlBRPRzAJ8C8DchxMlRnDMotVC3IcgYdELA\nZDY2vzZo+zn9Jgh5hcVZHd/rmVoT4oBZ3Azwl/BFZJoylt67w1MR2Jg7DV+i3xSZV4QAHsqeAUBu\n5pCNxXKWdx/XUb53ctZiz3BD3XDLOBX7isrZeQeAnwC4M6LzBaIWymXKxnD5uu24en0fWoykMqEh\naGEhnRKqTgYG0+jdNuArtPELZ0xD93Ed+M69O5ANGHpnUYtabZyxngE/v+eBoUyhIYbX7zEvsb3E\nRk5kbl/rM79AAKEEYRSKmm7kWLmTeKIkEhu5EOJ3APTT3MpEJQrkBxmDOY4cDuTNDbLQNrc4X2f4\nnv34oA/V1etfRLsPp90vN7+Be7e8gXe1hJv7CUCLpt23ksxPbMKm5svw53EXY1PzZZif2FTtIWlj\nPQN+Q1Wt1ZWAqVXffNFphWgSOyobeVdivzIKKkj9cy9k74Est8ELt3ctyH61QMXeKCJaQkRbiGjL\n3r3+tUgdamEG1b2Wc4Jxi2ZRTVArH9qJRMBolnQmCyHgKyHjmT+9rV0b3P3aFcqv12R+YhNWG7eh\nK7EPCQK6Evuw2ritZoS51yRjCRZpwleCYCS9nxHreZSd401Mkh7zFiYVlBGnU/Bj75+sjA33o0DY\ncTPX+HFC6kaOlTuJJ0oqJsiFEGuFEN1CiO7JkyeX5RpBZtCovdJ+Zmury/mMZY9gzeOv4sI5ndJo\nFtXkcGAoEyoG+WA6E1looy61aFK5sqkHrTRStK2VRnBlU2UyAtsVpQwA70nGLlhkEVFrPnsqLjp9\nqlbCjdW6zjrHgsQmbG75Vl4jLz7DkGjGD0Y+W5KZamnK928dwIff1yG9zqEjo4HeMy9zjS66yTnl\nTuKJksiiVohoOoCHdZyd5Ypa8etlLodX2k/kgG6389mrnohEE3Zi1bVuMRI1pyVXkj+PuxiyyM6c\nILx3+K6yXrvVSODlfztXmQyjihjpz03CRa0/0+ogpJtoUxR5YqW72zIlcwBIAANiEjbmTsO8xHZM\noX14E5NwQ2YxNuTOLDnf4eFRqQM8SJlar+/xlwDNquNGQzRf9ppBndr3yod2atnU/WjtVvysDqrS\nr85rW+n1UZMVAgLlM3UYCfKdPJMykvjCGdOkttowuGm9e4TcdLBHTIx0DDKsey9bxhPc7dNWUwaZ\n78SOjrlvZdPP8bsjFwIr2sz/1v9TSbp7AqYQv3F0MT6b/F1hldBJclPUnsE0DiqimIKYO91quBBK\ncyQaiajCD+8G8A8AJhFRP4DrhBC3R3Fuv6jSYGXRJCrsD1mQSJggkSSyawPmktFvgabO9hSGRkbL\nosXrMj+xCVc29WBKbj/2NE/EjaOlGpuTlJHA8Gi2LNmgB9MZZdr4jaOLsdq4rci8MiSaC8ku5cQy\nxVnPkjMiI/HbqfkiUMUMpY7RrgOvKgth/Uad+clCx/zSSftcTVEbRsZ+YwF1yGoQh+HC2Z3K8sEC\nwMqH5JUlG4FIBLkQ4vNRnKec+ImxtT9kblq76iHx0jbc4rGdD7gfzaWcyT5+sOy61stu2XWRgVSY\np4wkPjCtzVd9b78IAINDIzASVDIxbsidCWRMW/kU2o89Qj3xFCYo2oc9ee3U2m98cxJDeSeyDk7H\nmUwJeX73v+LkF/5fpDA8ttFI4cbMRdp14JeecwKuWLe9aH/nb6SLAAqC38kU2l+yTfacB3UY9m4b\nwAtvHFR+fmAoU9SDdul9O7Biw04cTMezhrkf6sq04oauQLQ/ZL3bBpRabZA63Nb53RyUzgdcV3Nx\nmpGirNHiF7/Ow0ql8x8eycLbiKT+bRYmn1E6Ho0k4fufPkVbiCeJXH03c1c/henLHsHi/9OFq0a+\niv7cJOQEoT83CT3HLsUvDn3Q9RsUTIB9PVj423PwJ1vEy/zEJvzQ+KlvIQ4ACQKyCrHxN5KbqKzv\nG9Zh6DeBLZMVGEyPhfwuvXdH3ZpfGqb6oWp5aaUqy5Zjbp5w3Trczu3XLzpF2RmnPWWUPOA61Q8p\nv9+ax1/FFeu2l3wPv00owqKy68o0ts72FBbO7qzYGGXJTCUrCCpdQaSMJH5w1Hq0pksnqO8234uz\nFnwTC5PPoLv5aqm2bsePA17AHIPdZEF/MJUFtyzagcE0Nj3wX/iUcRuaskfMiYf2YY1xKwiEJgru\nF0kghyHRXDQRpDEOxyz6AehX8qkwJ0RJw26/hA0jzuQEVmzYWZdaecMIcplATBlJrJh/UiATiU4d\n7hLEmC1UNRYn9klFZde3SstaL5Ddjl8N9ohJ6JIIc5nz0ArBrGa2p2oFcZXRgw3DZ6KzPYWbT9yF\n1hfelB5/DPZhYfIZ4KHL0JUwfyPZZACMdR5SPXM6WqeAqRSkjKTrvpfjHjRljxRtG0fhTW3WJOU0\nRW19dBLaUvIolSiSaFTKWJIIR7c0aZWHqNcSEg0jyFXOpCDNcGWas85xXo4tHU1BJfB0ImB0ztue\nMvDO8GioNHy/zsOoGg4HxW0F8ZfV5+dD8a5Tn6CtS9rUwOkAnNBqeIbc6Wqdg0MZ3HTRaYVn6AKJ\n7V71vcJg/Y7OVYI5qDSMJJX4IaJKolEpY9cvMotw1Xr7wHLSMIIc8F/Y3U2LD3Kc07EFjAlzS+g6\nO5q3pQwcHhktNFUW0K9V4iUU2lNGiSNoxfeuw6VNv/Q0D7ghcx7ekrgYW9/1MdBgGomIelJGhdsK\n4vltA1j4W0nnGQurTOr6JdKP7eYkKxEmiBIg26/wPPf1IL3+9oJD1FoNHBBHYSLJG0J4Yf08h9GC\nYdGECXQYb2KiNF7cTiYrMKHVQGuz3FwZBh0FyKofoyJOteT9wGVsPQhapMfrOFUy0oVzOnH/1oFI\nNAsrFlu1qth+3dnFG/t6MHT/N0o06WWZS0teXq9lvRMCCjbSGcseidSMoprYrNhxazk9odXApe96\nHgve/nnRRAVAuoJYlrkUW9/1CTxzZJHiCgAW/cystnfDDCBdGnXTn5uEM0d+XPg7SYT/WHyq8hnq\n3TagVZFwQqtRKMD2JH0drelSs8/b4mi8qylTZF4ZQRIQhGbyzk3YnzsKc0bWFm3T+d3tv3WlcYvW\nMpKENZ9R3/s4UNYytvVM0PZMXsep6qfc/ezuSLRV+wpAe1WxcZVrfLAlMC07r5vd3om9voau1qlL\nU5IAgZLlfIn/o68H6fU/QipRrLkuy1yKZZlLpeGHNJgG3tMljeVG21RTiPf1AMPvlHw8IppKzElW\nxyHApSKnRqCRPcyuZdxb0mMm0CHQgrVF9bl3vO9fsW7LG7hc3INO2qd0zANABx3C/MSmwiRu/933\nuKysqllUyulTskJ9vXwTcYcFeZVQmT3CCHGnoHXWVvZcVSh6G1rmAevcdjuvrl2yYFZIPoOHs9ei\nbdxfA5tunGgv5zeuKo7HxthEdebIj0ttvsgLpXnLS9LVizrPbFwF5EqdaO+IFul3s+chOFduQzYz\nmi57xESpaeivmITN2bnAPzxuXuOvaXz58HNYafxPtGYPetrniIA1xq1ABngodyYG8iZA696qVpW6\n9nCnCXFkNIuhfKbrhFYD112gDkRwo5y9MWuV2AjyWmgYESVuHni/wpzy51PdE+0Hu02uedqjTQYG\n05ix7JHC9axwyj2DabS3GhAC+Mjw09Kkme2PrMWnxE8xIXsEoDGNmDLAvhnz8Zf96aLz+IkwGBzK\n4LoLTpL6HAp4TFSy2jdLzzkBmJWfuFSdZxTnnUCHlePdk68Jr5tt7IbKufyDzGfxyL07kIC5Wpmf\n2ITvZtaOmVU0NP9xlC1y2Moym/2+l73bBrDyoZ1FORrO3/rAUAZL7yt/4+t6IRY28lpsuRQWPzZy\nI0EYFUKabBKk+JCK5zfcipO2XqtlI7fGW/IbuNjZr2zqcS0A5fweKnuxLLvyf7d8DEcyOfdn5KaT\npRNVf24S/iN3EZan7kNb5m/Yk5uI25q/gNPOX+L+fBXaiknMLii1j9tx818EYeyeqDNTVQW4vJAV\nDwv63PnNOI7y+a4HYm0j92q5FEdt3U2b6T6uo2Q7ILd1R1kb+fKXj8ccha1YhrRUgYud3S3MT2Zq\nkt2jb717Gy54/TakbMk7Nxi3YVWuCXdnznAfn8REMiSasSkxB2uM29GUMZ2CXYl9WEG3AsmTACjq\nrUiqA9pxC7ckoFDwKggy5640HNBB0HBEWfx/0OQcv9mZVp5BnN7tahALQe7WMKIW2rsFRWXycDOF\nlHPC2jOYxgC8BYLzmAJ9PUrt1JwY1GF+Vlq58zuV3IubLgMcE0WKRvCN3K9wN4oFecn4LFOIzUTS\nOm85PrdxFXCwOHEGmbS5n6r/oyRuHDDD9gYUtn/7SuLNbZOQbbkYdx8pHbO9MbLM3HPhHLPFn5c2\n71y5BAlHzAHSCSmoQ9PvBEAYW7XE6d2uNLEQ5G4JNnFqkBqWcjtxVGnfbmaAwgttaagKLO3eLVFI\n60VV2bkTpen/ReOzkDXnVcSA4+BuYEW7vBu7YhwCJDWnOMsAdGIflouf4nBitETgD6YzGD+uCTdf\ndBoAYPsja3HpyC8xJbEfR4Fl6SkAACAASURBVFLHoPW9q4CFi13rc8vKDoyKBIRQl5CQQQCeTH4U\nyEWzEvQbsaRT3ZGJSdEst5ZLtdDerR7o3TaAwyOlscVGgsxu615trxQaKlCcDbgscyn2YBJyMAtA\nOe3vXhmpQ6ljpNv35CaW+O60BU5bl8uHAji4G6MP/qs5WXkco6phLisDkHIpJGZNap27H8YKutUs\n1AWB1vSbSK//Jp7fcCuWnnOCsg3cCuPOkus1Uc6XEAcAaptaqPEPmM546zcKUoBK9hwBZgnjVlsv\nV7fEHee7HXWXrzgSC43czZ6simWuxQaptcyax1+Vhr0d1dKkF8ao0lAFioT1k8mPmgWmZnfifygS\ng1ST8LW9L+Kdg5/G9Qqt3p716ituWBZe6KApewRDjy1Hq6WVK+ztKru4n0JiFulMFlO23ghQ8bhS\nGDa3A7ih+TakUFzsa072D5iAYBmdRdfHOCzbewG2PP4qPvb+yUVO+KBmDj+RLqoVh/3djrNpNUpi\nIcgBtVlBJx2e8UYlPO0FwFxNO4rQxXTrsdgqPgGSvLRT2lOY8/cnSyJQtr7rEyXn6d02kG+kcCaE\nS+1wWay7Jw7buYCQRua1pN9SHoO2Lizbe4HSMfwmJqETeoXE7BwrOcbcvh/JF9YUhLhFK43gS8nf\n+Na87QgAB8TRWJH5ovl9BtPKJhaXr9teFFuug66JUOfdbiTTqhuxEeQqwhSgYsbwKvTliSJppvXc\nVXhmllyo3nziLpy8tTQC5aUTpwMwj7Eikuxj84rQCGRWs9nOB5a/Txqmtyc3EV2KYwBg6+qnAMm1\nO9tT2HPilehwNIgYTbbg5tznXIfl5iCegr3SWPBgQjx/UFsXVh6+EHc46p27BSmXSwvWebfZtGoS\ne0EONGYmV9SEXtlINNQSB6GD0//0n9IIlNP/9J8A/jlwl6OwZrXbmr+AKzP/VWK+ua35C1jhcpzb\nPTx99lnA9AlF96dp3nKcmZ2LoyxnpqRQmZuDeFlzD6YoNHb/WIYp4MCQ/4YTQbRgnbBhr3c7tAJS\nJ0TVs/OTAH4EIAngNiHE6ijOy0goJKHoCUtdIlnZyCJC3FDY1a3tfmOOgWjMaqedvwTLHxjF1eIO\ndOTD9Y5gHM6fNcX1OM97KLk/C/t6sJBuBRR1zN3a0CVHCT9o+lmJeSU4pmN3dfPtECOlbfm8qm76\n0YKjsm2zadUkdGYnESUB/AHAJwD0A3gewOeFEC+rjolT9cOaQpaEYqSAC34ciTCvOIpMS7RNBa54\nybNKIgH48Ps6Cqn9UZrVnt9wq7RXZuh77ZyIRw57Vk00ozwE0pnSrj6faf4/+C7dgQk4FMou7nZ9\nawxe8et+fBMqR2aQTM44JgQGpZyZnR8E8EchxJ/zF7oHwAIASkHOBEQW4ueVuFLLeBSjcos5jrSa\nnV24piYA2WGcPiKpkxL2XjsnYkXyFGA2ON7UfBn+PXsRejNzlfvdN/JhXN58DzoS4aNU7Eyh/coI\noLDFsoBobdtsWo1GkHcCsD+R/QA+5NyJiJYAWAIA06ZNi+CyDYiHKSJ2zFoMvLEZ2HoHILIAJYFT\nLy4ISreOMIUXN6ypySlcJdpxEWHutUusvRPKFxX7Af0MOSFcK0SWoxOQlW0r05B1zHBeWnKt2Lbr\nRZuvmLNTCLEWwFrANK1U6rp1hSLEzz2hpYbp6wF2/MoU4oD5745fAdPOAGYtxsLZndjy+tuFGu1J\nIlw4p7NYiDs1XCu71CnMVQLfh3AFEO5eB5gEnO3i7KSMJBYmn0FOJJBA8GbKTuzx8CoN2U0L1rF/\n14Jtu55i0KMQ5AMAptr+7spvY6Lm+LOBLbfLt8cRhanISrzp3TaA+7cOFMr6ZoXA/VsH0H1ch/mi\n6Zqa+nqAB78BZPNOwYO7zb8B/8J15LB5Pg2t36ntPZk6RtrJB6kOoHm8a50aO1bZ4ptP3IXZO9ai\nKRteiA/iaLxLHCqJyw+iIevEdtdC2LDbOKs9Nr9EIcifB3A8Ec2AKcA/B+DiCM7LONn1hL/ttY5C\niLYMvVUQgq4CQdfU9NhVY0LcIjtibletclSk31Zr/TZk2t7y5gux2rituLO9kQLOvcE8l8L5a08a\nKjJ13HQZkD1Ssn8QmlNH4aRDt0WiIevav3Vs2+U0fajGaWnmcdLUQ9daEUKMAvgmgMcB/B5AjxBi\nZ9jzMhLqzUbuUq/EenlltUQKL6DKzOHcrrJ7p982TSyGT63T0vpdkE1C9418GN+jr5lROSDzX3sU\njGQsdjNHykjiW+/ehrdWzETuujYIPxOQBy3pt3DhnE50tqdAMCeMoPX+VVq8X+3emgwHBtMQGBOo\nUdVSUY3Hqidjx6sGULWJpGiWEOJRIcT/JYR4nxDi+1Gck5GgK7iC0NdjaoQr2s1/+yTFnHT28cO8\n5RgSzUWbLMHV/fcnsa3ln/Ej47/MglFk1glfbdyGLx/1XOH4EiFsi3qxiimpImwFgP77rsazmeP9\nN4P2mDxV2t4dhz6IucM/xowjd2Hu8I/Rm7VFpMxabAr2vKAfSh2LG42v46HcmehsT2HljJ244PXV\nOAZ7kSCtBj/a7MlNxP1bB7D0nBPw2urz8cyyswJrn54F1jTxMn2ERTVOVYeuWs4WjUX1QyaPh+AK\njOU0PLgbVlIIHrqsWFDr7ON1DeckMGsxbjS+jv7cJOTEWDVEAFjdfDva8U5JbHQrjeC60ZuBFW2m\naeTUi6Ua7vMbbsXpvR/B/05/Wjkkgjk5nC765EKxbSqGUsdKj1VVYbRQaXtWfW2lhjlrsfl7tnWh\nNf0WVoy/H69dfBjPnLcPi974XqGcQZRYk2dUQnLh7M5CxcQw2n250+9V4+yMaEVRSWLR6o2xUY7M\nTo/EHO193MasSGTqzc4tiV54Ztxl6NQNqUs2AwtuKXFuptd/s6TRsn8IK5q+JU3Xv9H4OlZcu1J5\npGUW+ET2f+G6pjsLGaIHcBRWZL5UFE5YZPeW3atks1lGUtLgOSyjIoFvZ75WMp6gYYVREmXSkB9q\nubWkKiGINfK4MWuxKThXDJr/RpEIpGN7D2Ofd4kucWpFlxz1nL+46OxIqb1646oIhDiAti784tAH\nsSxzacmq4ReOolJOFs7uxJ2nv45/b74VExNm1iUR0EGH8CPjv/DCuCWF2uFFGqbsXmVHyiLEh0Rz\niRAH1CuGctusnURlovGL14qiFuuf10XRLCYkXvHpfT0AJcbivWX7WMhWDB6TQCF6oa8HeOhW/+N3\nnj+A87ekc07eZDXl0RQ2DJZWW1Qtv+2Yxb9K7xkR0IFDhZoqRWV7K+C4FkK+MpBhjxKqdMnYaoYo\nqiJqajX2nAU5454qby31ZULcaZ9XJeikJsgjR5yTgN/knDw5IiTsLdn8hhQCOIxxSKQ60Jp+q8hk\ntTSrkY6uMnd5COVWGsFVRg+eP+ebYxsDjN0vRECL0Le1WyuGapSMrbX0+1qtf86mFaYkWqIoLE4l\nXClZWkBKZUIB9Jy0boKvebw0skQIICFyKHLAdrxXfR4F42kYrc2leo2n487NCawRTTSF9hcLAJlD\nO9kMJNStz4LQ6tJmzonl5IsqrDDO1Gr9c9bIq02ZytJGdm2VdiiyxbbpjavU+6YPAIvWyq9lH4PK\nfAMCRg4XIkss/3wOhCQ5xHsmDfxlk9c3l1yBxsbvSPUv0gr7eoCNlwEP2qoXqrJL5y0Her/uat8m\nm7A3HYmT0H34/8HVzffiPdhnfn782cDOB7zrwPjErc2cncPDo+jdNlATafXVplZqxDjhqJUw6Aph\n1X7VLEvrdm1gbLxeEdY6ERWqyBbZGDR5WxwVeenWEuzj7usxwx21hSmZDmmv41IdwLk3SKN3UkYS\nd57+Ok5/8bpA98iL/twk/I+RHxf9wkaC0NyUwOGR4gnVitoA4pW6HjXVjmhRRa2wIA+KrhB220+l\nxeqE9IVFFU6Y6gBG09EJDreJ6YYZcgFHybxmrm5lUOKclB4fFpsw9jvhyH5DlVA3Ulgh/rmkvRoA\nbG75Fo7B3gBj98AW/ukUzKqG5uUO+4sL1ayYWM565I2JbsEmt/2qmXKvukaUy/e2qWN28JtOLl6R\nuF1L5MwJxWUsSiGeMIDjPgy89r+Cj9vCMnsEccLKCplZPgfn98qkcWnul7gDpYL8PULelzMQ1gRn\n/S6zFmMhSqMtrli3XXp4te3AtUKtOWABFuTB0RXCbvtVsyxtuaMjUh3mv+v/CUWatWV/bnKxKaqi\nXHRoagbe/rP0I2vxqW2OOf5s9crFC1UhM8XzMCVRbK+en9hkOiOjEuI+Vnm1agdm1HDUSlB06564\n7VeulHsZzhT548/2VyyKrMQMh2RRRVQMH7QJQIlDMqigpuTYJCFj5LBSWAoAWe1Hnsza6B5CXGWY\nFKoJXPE8HEkdU0h+mZ/YhNXGbehK7ItGjvt8pqqViMMEhwV5UHSFsNt+bmF/USILkdvxK1udEg+M\nFPDpnwIrDprRJ/bxLrgF+MCXUCLgcwFt1B4mFXz6p2bZ14A0kW7tbqFlTlGd7a+YJP9A8Ty0nruq\nEOa4wrizqCRAUARg3k+fz1RUtVKYysHOzjDhf2GjViqFyqloLbfdzAc2e6qSoOYHJ0bKnFy2/BxS\nXTfVAVz1mvn/358CZCR9NRXkoNBaKAlMPB7Y94qPgRY7YZ2O1yHRjKszl+JHP7hefrjb89DXkzdH\nhcRataQPVOeZY8oCOztl+GkVJmPW4uIXcOMqYP2S0hfHvl+l6etRa7jW8l+V2amryUXhnKWEKcR3\nPQGlweIkWyXDC27WF3iUREIVxSKywMHXfYyzNCKGyCw+lYAodNgpSrt30JudizXDP8aeI2lMaUlh\nafYELLQ+9KhzrkXz+OLII7/PNRM7Gtu04hZR4oewJV5l54uq7vdjV6k/s+y1XiYer/FE4ZwVOWDb\n/3TX7Lfcbq4urFZrbrZy57lVJiRK6EekGCllWGMCAu8dvgtnjvwYTyY/qrQnexaeCjspJpJAclw0\nzzUTGxpbkEcV/hfVhACoJ4WHvx1MuLvZm+32fKuq4qK15t/rl5jXefjb3pOUqmdo83i9MVo427HJ\nsFqt9fWYtnIdh621Qko2l34mfPS7zKRtTt9i/kaTtOzJns0Swk6K49pMc4qMuHaSYjxpDEGu0iij\n6rgTZTy4alLY8vPoNH4LK7vUujc3zADW/3Pxdbbc7j1JqULtUh3Aop/5b6XmRdH17Y8woeSRtjuW\nm48KdLkiQ4+ieNgxi36g1VnHs1ZH2Iil9Nvl7STF1CShBDkRfZaIdhJRjohKDPA1gZvZI6rwvyhf\nHKXwl4Tw6Wj8buaHG2aY3eSte5N+G+o4DJdxuk1kltlGockG5uBus45JkcNTAAnKf2eJiUihqbr5\n+4VQhHJTUn4NDzwLT4W1YVOismGtTE0QViN/CcAiAL+LYCzlwSsDM4rwvyhfHD/CXydS5Nwb5CYF\nwBTcOuYMGfZxek1ksxYDcy4Jdh0VlJTXd8llTZOOrPGGYpxWxIkAAGM8BnF0oYmEEpEL1Nyj7DHa\nIle5sFamZggVtSKE+D0AUFkrF4XEy+wRRUSJdXwUIYayCBIVOlpu0dgiyuR0TlJu9cwtVOYXi1QH\nMPyOXiecZLP7BKT6zT3uLQFAawdm//WGwvpnU/Nl6JJ1LEpN8B6nBGWzhOQzwA1+inJ5UM1IKabi\nVCz8kIiWAFgCANOmTavUZaNNg3eL/w3y4ridz77drZRspZHFletMZG7+gkU/Ky1p61Z1sfkoU+tW\nxr7bflvnPT71Yry1ZQPeI/bKU/UP9helqN84uhj/bqxFM40W7zf8zlj0jE9KanX09ZgmrqCrIzu6\nkTxMXeEpyInoNwBkLcOvEUI8qHshIcRaAGsBMyFIe4Rh0dEWdQgbc+73fPZzujU+dp7TKUyBwKVi\npQStyqiakFId8snQLckofcA0GalqfR/cDaxog6ljEwp2/3xG6+HjFkD85R657butC0v/Yazu9obc\nmbhO3ImJ+ebJBXKZ0gJpQdm4KhohDoTKemXii6cgF0J8vBIDKRtRmT10qx2W43w6k5FTqzu42/y7\n+Sh3IU5JoKlFL0tSpe25TUqAzazjKEtrpEoFT2EycjEDtXWZ9+iNzWZUjRIBmZP4fW/cq/JgAvOW\nY+GsYvPHhITi3oQJ59NdffjBPikyDUVjZHZGYS+MuuSsn/PpTEaPXVWq1WVHvG2uXk0hLJLNam1P\nNSk9dpWjtrlAQZjLTDS6db+tuHUvu7sKpUlKFMZTZP64KeIqlSEaaiiRTYpMwxBKkBPRpwH8J4DJ\nAB4hou1CiHMiGVmtEXXJWb/n85qMAjvJct7L+nwXGwCldcVnLXYxgcjGJNQlVXXrflsCPOgkqmo8\nocr+jMo8ZxGwyXQR3V8170O16vcwNUWo8EMhxANCiC4hxDghxHtqWoiHTXuPOjY3yvOFTQxSQqbA\nsApVqeLx/caIh13dWPsFmUSNlBkK6efeRx3OF0WG5Y5fmeMNEALJ1B+NUf0wqt6YUVcxdLb+sjRf\nt3PKxhBlaKEFJc2SsdZY3CooBrm2zLSiuoZsbCJnhgDqhCwa44HMkLrpsxVKWKlKgbrf04tKtARk\naorGrn4YlaOyHLG5o7ZxWXVE3tgsXzarnIqhlukJSLM551xSXDRLWUHRRYhTAmiSFHCyjrNH6fT1\nmEJZB8ssUhiTurcnjPHANXtKt1u/pZajNuTE7Zw0jgz6P4cMrp3C5GkMQV7N3phuuNVVcbZGc9tf\n2WzYRcBZnxspecSK3ZEYtGqeyJmlabfeIR9fJm2Wot24yuzso+N0lV9I/VFmyP1QXUdt0HBT50QR\naU9Urp3CmDRG0axqFBHSsckrtVlFXRXVxCOycptv91c8ClYJddihTi0VL1Idpi3XK3Hp4O5oBZwd\nr9/YrQl1FBUto3BsyuDaKYyNxhDklS4ipFuf3I+T0Frey7Ccb05n3Kd+aNvuE51aKm5Y97scQszP\nGLx+Y9+VLnf7c5iXa9XHtVMYG40hyINEHYSJctGpT97X4y/F3qtZs1VP3BnFYG1XCfNUh1xrHzk8\n9p1l15VBtsepKRWBlu2zhk+qozhpqUljzKp76prq7qOUcLlWfSzEGRuNIcgBtaCTEbbjj5dN3jq/\nEocAi6JZs0pgnXuDeQ6n4LI3cCi6rtuwbSuM9Nul38OLVEfxd/M0DdkwUmYrOJnz2O13U91TnaYV\nOqaWecvh+z54wfVUGAeNEX7oF7faJjrhXl7Hu9URsRoQlyPZwy18Uvc7q/ajhKLbjiQt/9SLTdu5\nTjioZzNi8g7DDBqmp5VGT6Zy4Hp8hKGhyWZgwS2skTcojR1+6JewUS5emYBu5ymn7dMtfFL5nXeb\nwtsS+qrvprSF5zM5nZPHtDP0QvtmLdYX0OuX+PtuXugU8VKZTqJMw5fdP4axwYJchip9nhJ6pUu9\naqMo0/OnVu8ldSuXe3C3KSTX/5M5RtmKwa827CcmXzdFXrfsQZDELr9p+lFFq1CSk34YTxrHRu4H\nlXNPZPVt5W42edX57Q7GSuPp0LTFtcvSw8sZGaTrG9AZQ1D/h1//RFTRKlF3VmLqEraRq+jrAR74\nmrq4UlgtyZmebxGkdEBU+LHpyu5B1CUMguA1hrD+D13c/CC6zPgo8OUN0YyHqQtUNnIW5G6saIfc\nyeXi4PJDpYSKX3SFUBxtt+X+TS36etSNL3Sp5qTO1CQqQc6mFTfKnRFa7tIBQWPhdePGg4ZnVpNK\nZvmG7WUbJJOUaUhYkLtR7oxQlfCwnKph0LUFy4R9Sdy4hkCSCZ2wpYPLgXSSorFmFVERVfu2atcD\nYmIBm1bsqHpeymyuQezBTrt483hgdES+/A67rNYx2+iW9/UVD52P6z7+7NJY8WSz2XquUuViVTz8\n7eLCZEB0ZoyoY8erbWZjagq2kXvhp2Z5kPrmKpupMpEG4V5iHVuwMrknX+/bKWyV55ThVXkR1bMB\nl8s3EXkLNwIWrWUbOVOAbeRe6NRHCbKv/RiZ5q0S4kC4ZbWOLVilNYospOYYX3ZkDYFfLRtwuXwT\nkVY6JLNEAQtxRgMW5BZ+Xm63LEinPdiyEwfqoqMpOGW2aC/7fl8PfNu+dZ2gfoi6s5EO5XJ4hpkI\nkuNQ+D0oaQrxT/0w3HiYhiGUICeiNUT0ChH1EdEDRNQe1cAqjp+X2/WFt2myD3/b5nD0ia5TVebU\n7P26aYu3mk4ApQksG1dB20xiCSjLCeq3R6crVHknaLmc2FbLuEAIFH4PkTX9C7XgHGZiQViN/EkA\nJwshZgH4A4Crww+pSvh5uXU000za7IwTZKlNSX3bsWw5n8uMOVStphNOx6If7dEeRTNrsdnL07dm\nrtL+hdy8Us6Il6ibKUeBM8KFQw8ZH4QS5EKIJ4QQo/k/NwOIT+8pp6AA9F9upyBQ4VlvnFDyExip\n4qbHXugIZJlQ8GNGcJYmkAnC7q96lLl10f6d3yFsGWEd/JQ11iXqLkcceshoElnUChE9BGCdEOKX\nis+XAFgCANOmTZvz+uuvR3LdQASJOnHDNfrDQ5iHDcnTtr87MheDRFjoRHWsaHM/Pkyp3FoOxevr\nyVdfjDAKrJa/L1MVAketENFviOglyX8LbPtcA2AUwF2q8wgh1gohuoUQ3ZMnTw76PaIhSNSJir4e\ns9iVEyNlFjzyMkFkR8x48qCaoa4D0qmB6zaLsHOw39vkoWp6kOrQN1/VarNsN/z4HGQkm4v/5p6c\njA88y9gKIT7u9jkRXQLgUwDmiWoEpQchKkGh0mpTHWaHGWfdbdWLHkZAOUvmpiYAI4eKba4qoWCV\nktWND09NKP6+ss7y595QGi+fMMbuh32sqhWIbjnaauJMCAscfZMPM9Stz84wEkLVIyeiTwK4EsBH\nhRBD0QypAkQlKFRxw83ji3tmBm1OoIt1DUu4pN8eM+u0TfUWCjqCyEhheDSLcaqVjP37AmqhpFOH\n3G/t70rjnMAP7oZWApQTYzxwwc2l945hfBI2auUnAI4G8CQRbSein0YwpvKjqrdhdcOJqjenznVV\nAspv1EaRgxDqaBXVuBJG8TZK5s0kpjPz+VNWwhj5u/x45/cN60isxagSO9IJPMBitLWjdr4TE2tC\naeRCiJlRDaSiFGmNDm1KZi5Q4Vezn7UYeGOzGZYosqawPPVi7xIAOmNys/vrCAtnpb5Essgccvnq\np7BOTEQX7Ss9thwmDz8dhCpNVElMtWzzZ2JF42Z2Wlpj21SUaFO6jk+/iSV9PWaihxXJokr8COKM\nDWP3l1Xqy46YjTXyK4Luvz+JG0cXY0gUO+WGRHPtmDwqQZQhkLVk82diTeMKcoswAtCvCUBXQAcZ\nU5C0c6/yAbaaK2uabwUALMtciv7cJOQEoT83CTcaX69dzbkcRJWkU0s2fyb2cPPlsI5PPyYA3U71\nQcbk10HoM468GVmsMO7EB4bXYsPImQCAlJHE9eefonV83RDarEIclcJEDmvk5W4eYcetFoc9ezHI\nmKJYHXgwgQ6hsz0FAtDZnsL1i07Bwtmdvs4Re8LWmYkyk5Rh8rBGrhvbHJa+HmD4Hfd9LDOLlc3n\nd0xRrA5cIADPLDvL93F1hWumbhKA2+c01oGJYSKEBTlQLACtWOz1/+Qei+23Q5CqHrkTe6XBcr7w\nSvPNVDNTVVY3RJW12Ui4lV1IJoHZlwB998izfa0CYSzImYipb9NKFLHYQGnRpiBFnXQ14EpFMriZ\nb869oTRlPNlsbm903DTy7Aiw6wngu3vU+3DIIVMG6leQP/xts4iRH2HrZje2R5cECQ/UEdCVjGRw\ns6nPWgwsuKX4swW3sCapE3poCWpVDRsOOWTKQH2aVvp6SpvrAt4JMl7akvV5kPBAWVRJwgDGHV29\nZsRu5ptaTsipFo9dpbGTMFd/subTHHLIlIn6FORulei8YrHdwsssbSpIeGClnKpMdDj9ILr1xg/u\nNoX4qRebphb+vZkyU5+CPEjiDCDXmi3s2lTQok61pOX6ddY2GtLCWD7IpE0hzvXEmQpQnzZypbAm\nH7HYUPe7rGRRp3K0PKtEB564EyDOvgR2bDIVIrIOQX7o7u4WW7ZsKd8FpFmLFL/O5FF3MrKIYwee\nSqNbo73Q4UliduH7yURM4A5BsUSmMS9aGy8hDkTbychOHDvwVJK+HrPhtIxUR/FzNfuL8v3YsclU\nkPq0kQO1ZY8OSrkEbhw68FQLaxUkixc3UsWdjnQ6RDFMBahPjbxeCFLRUIdK1peJG262cWfteJ0O\nUQxTAViQ1zLlEri13oGnmritdnY9obcvm6iYClO/ppV6wBKsj1015kxrcraoC3FuFtylpCao48Wd\nAppNVEyNEEojJ6J/I6K+fL/OJ4hoSlQDqzvChBGO2pbv6bc5VLCa2O87m6iYGiGsaWWNEGKWEOI0\nAA8D4CdYRpi47XJFrjBy0gdcPhTFvxubqJgaIWzzZXtb9fEI1Eq8AVAJ4we+Zhb2csusZDtsZfEq\n0+Cs18MmKqYGCO3sJKLvE9FuAP8IF42ciJYQ0RYi2rJ3796wl40XKqFr64mp1NDLFbnCyJm3HGYL\nDRd4EmVqDE9BTkS/IaKXJP8tAAAhxDVCiKkA7gLwTdV5hBBrhRDdQojuyZMnR/cN4oCO0FWZS9gO\nW3mMVvfPeRJlagxP04oQ4uOa57oLwKMArgs1onrErRiXHZmmx1UTK4dOQ2qeRJkaJJSNnIiOF0Ls\nyv+5AMAr4YdUhziFMSXkmYMqTY/tsJVBleBDSUDkeBJlapawceSriegEADkArwP4Wvgh1SnOvqBB\nyuAy5aOvR+3kFDlgxWBlx8MwPggbtXJhVANpKNhcUlv09QC9X1d/zjZxpsbhzM5qweaS2uGxq4Bc\nRv4Zr5SYGMC1VhjGrYUbJ/gwMYAFOcO4wUKciQEsyBkm1eFvO8PUGCzIGebcG8yWbXaSzeZ2hokB\nLMgZZtZiYMEtxcWveWDGmgAABgxJREFUFtzCZhUmNnDUCsMAHEXExBrWyBmGYWIOC3KGkRGmEQjD\nVBg2rTCME2cJBavMMMDmF6YmYY2cYZxwVyYmZrAgZxgn3JWJiRksyJn6JIyNm7syMTGDBTlTf4Rp\ndg1wVyYmdrAgZ+qPsDbuWYvNYln2BCEunsXUMBy1wtQfUdi4OUGIiRGskTP1h8qWnZpQ2XEwTIVg\nQc7UH/OWAwmjdPvIIU7sYeqSSAQ5EX2HiAQRTYrifAwTilmLgXFHl27PjnAsOFOXhBbkRDQVwNkA\n3gg/HIaJiPQB+XaOBWfqkCg08psAXAlARHAuhokGjgVnGohQgpyIFgAYEELs0Nh3CRFtIaIte/fu\nDXNZhvGGY8GZBsIz/JCIfgPgGMlH1wD4LkyziidCiLUA1gJAd3c3a+9M+ejrGYslpyQgsmYs+Lzl\nHFLI1CWeglwI8XHZdiI6BcAMADuICAC6ALxARB8UQrwV6SgZRhdn5UKRHdPEWYgzdUpg04oQ4kUh\nxLuFENOFENMB9AP4AAtxpqpw5UKmAeE4cqa+4MqFTAMSmSDPa+b7ojofwwSCo1WYBoQ1cqa+4GgV\npgFhQc7UF1y5kGlAuPohU39w5UKmwWCNnGEYJuawIGcYhok5LMgZhmFiDgtyhmGYmMOCnGEYJuaQ\nEJWvX0VEewG8HvDwSQBqMfGIx+UPHpc/eFz+qNdxHSeEmOzcWBVBHgYi2iKE6K72OJzwuPzB4/IH\nj8sfjTYuNq0wDMPEHBbkDMMwMSeOgnxttQeggMflDx6XP3hc/mioccXORs4wDMMUE0eNnGEYhrHB\ngpxhGCbmxFaQE9F3iEgQ0aRqjwUAiOjfiKiPiLYT0RNENKXaYwIAIlpDRK/kx/YAEbVXe0wAQESf\nJaKdRJQjoqqHiRHRJ4noVSL6IxEtq/Z4LIjo50T0NyJ6qdpjsSCiqUT0NBG9nP8Nv1XtMQEAEbUQ\n0XNEtCM/rpXVHpMdIkoS0TYiejjqc8dSkBPRVABnA3ij2mOxsUYIMUsIcRqAhwHUSieDJwGcLISY\nBeAPAK6u8ngsXgKwCMDvqj0QIkoCuAXAuQBOBPB5IjqxuqMqcAeAT1Z7EA5GAXxHCHEigDMAfKNG\n7tcwgLOEEKcCOA3AJ4nojCqPyc63APy+HCeOpSAHcBOAKwHUjKdWCPF325/jUSNjE0I8IYQYzf+5\nGUBN9DwTQvxeCPFqtceR54MA/iiE+LMQYgTAPQAWVHlMAAAhxO8AvF3tcdgRQrwphHgh///vwBRO\nndUdFSBMDuX/NPL/1cR7SERdAM4HcFs5zh87QU5ECwAMCCF2VHssTojo+0S0G8A/onY0cjtfAfBY\ntQdRg3QC2G37ux81IJjiABFNBzAbwLPVHYlJ3nyxHcDfADwphKiJcQG4GabymSvHyWuyQxAR/QbA\nMZKPrgHwXZhmlYrjNi4hxINCiGsAXENEVwP4JoDramFc+X2ugbkkvqsSY9IdFxNfiOgoAPcDuNyx\nIq0aQogsgNPyvqAHiOhkIURV/QtE9CkAfxNCbCWifyjHNWpSkAshPi7bTkSnAJgBYAcRAaaZ4AUi\n+qAQ4q1qjUvCXQAeRYUEude4iOgSAJ8CME9UMHHAx/2qNgMAptr+7spvYxQQkQFTiN8lhFhf7fE4\nEUIMEtHTMP0L1XYUzwUwn4jOA9AC4F1E9EshxBeiukCsTCtCiBeFEO8WQkwXQkyHuQT+QCWEuBdE\ndLztzwUAXqnWWOwQ0SdhLunmCyGGqj2eGuV5AMcT0QwiagbwOQAbqjymmoVMLep2AL8XQvyw2uOx\nIKLJVlQWEaUAfAI18B4KIa4WQnTlZdbnADwVpRAHYibIa5zVRPQSEfXBNP3UREgWgJ8AOBrAk/nQ\nyJ9We0AAQESfJqJ+AP83gEeI6PFqjSXvDP4mgMdhOu56hBA7qzUeO0R0N4D/D8AJRNRPRF+t9phg\naphfBHBW/pnantc2q82xAJ7Ov4PPw7SRRx7qV4twij7DMEzMYY2cYRgm5rAgZxiGiTksyBmGYWIO\nC3KGYZiYw4KcYRgm5rAgZxiGiTksyBmGYWLO/w8sBrq0Z5qn6QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5rBpWAiOeHw",
        "colab_type": "text"
      },
      "source": [
        "### Model Arch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNCwffzrOeHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        #Inputs with linear transformation\n",
        "        self.output = torch.nn.Linear(2, 1)\n",
        "        # Activation function for the output layer - sigmoid\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.output(x)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW_bzzBqOeH1",
        "colab_type": "text"
      },
      "source": [
        "### Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ7hdneCOeH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f683d0bf-9f1e-4653-e184-103dc4206f4c"
      },
      "source": [
        "learningRate = 0.1 \n",
        "epochs = 1000\n",
        "running_loss = []\n",
        "\n",
        "model = LogisticRegression()\n",
        "print(model)\n",
        "\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(\n",
            "  (output): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI6dcqPqOeH5",
        "colab_type": "text"
      },
      "source": [
        "### Define loss & optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOXnB27lOeH6",
        "colab_type": "code",
        "outputId": "64a0570d-1abf-43d9-8677-c73a99e09779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "criterion = torch.nn.BCELoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
        "\n",
        "\n",
        "logits_numpy = model.forward(X).detach().numpy().flatten()\n",
        "plt.figure(figsize=(15,3))\n",
        "plt.title(\"Output probabilities with the untrained model\",fontsize=16)\n",
        "plt.bar([i for i in range(1000)],height=logits_numpy)\n",
        "plt.show()\n",
        "\n",
        "logits = model.forward(X) # Output of the forward pass (logits i.e. probabilities)\n",
        "loss = criterion(logits,y)\n",
        "print(loss.item())\n"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAADUCAYAAADtA9CTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZxZTFoIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.5556984543800354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5QjEey8OeH-",
        "colab_type": "text"
      },
      "source": [
        "### Training: 1. Reset the gradients 2. Forward pass 3. Calc loss 4. Backward pass 5. One step of the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALMlz0WjOeH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a539e27-e7c0-404a-84a1-b2bdd760b281"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model.forward(X)\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, y)\n",
        "    print(loss)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # for print loss vs. epoch\n",
        "    running_loss.append(loss.item())\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, round(loss.item(),5)))\n",
        "\n"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.5557, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 0, loss 0.5557\n",
            "tensor(0.5409, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 1, loss 0.54092\n",
            "tensor(0.5273, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 2, loss 0.52728\n",
            "tensor(0.5147, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 3, loss 0.51467\n",
            "tensor(0.5030, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 4, loss 0.50299\n",
            "tensor(0.4922, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 5, loss 0.49216\n",
            "tensor(0.4821, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 6, loss 0.4821\n",
            "tensor(0.4727, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 7, loss 0.47272\n",
            "tensor(0.4640, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 8, loss 0.46398\n",
            "tensor(0.4558, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 9, loss 0.45582\n",
            "tensor(0.4482, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 10, loss 0.44817\n",
            "tensor(0.4410, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 11, loss 0.441\n",
            "tensor(0.4343, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 12, loss 0.43427\n",
            "tensor(0.4279, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 13, loss 0.42793\n",
            "tensor(0.4220, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 14, loss 0.42196\n",
            "tensor(0.4163, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 15, loss 0.41633\n",
            "tensor(0.4110, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 16, loss 0.41101\n",
            "tensor(0.4060, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 17, loss 0.40597\n",
            "tensor(0.4012, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 18, loss 0.4012\n",
            "tensor(0.3967, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 19, loss 0.39667\n",
            "tensor(0.3924, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 20, loss 0.39236\n",
            "tensor(0.3883, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 21, loss 0.38827\n",
            "tensor(0.3844, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 22, loss 0.38437\n",
            "tensor(0.3807, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 23, loss 0.38066\n",
            "tensor(0.3771, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 24, loss 0.37711\n",
            "tensor(0.3737, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 25, loss 0.37373\n",
            "tensor(0.3705, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 26, loss 0.37049\n",
            "tensor(0.3674, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 27, loss 0.36739\n",
            "tensor(0.3644, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 28, loss 0.36442\n",
            "tensor(0.3616, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 29, loss 0.36158\n",
            "tensor(0.3588, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 30, loss 0.35885\n",
            "tensor(0.3562, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 31, loss 0.35622\n",
            "tensor(0.3537, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 32, loss 0.35371\n",
            "tensor(0.3513, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 33, loss 0.35128\n",
            "tensor(0.3490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 34, loss 0.34895\n",
            "tensor(0.3467, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 35, loss 0.34671\n",
            "tensor(0.3445, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 36, loss 0.34454\n",
            "tensor(0.3425, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 37, loss 0.34246\n",
            "tensor(0.3404, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 38, loss 0.34044\n",
            "tensor(0.3385, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 39, loss 0.3385\n",
            "tensor(0.3366, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 40, loss 0.33662\n",
            "tensor(0.3348, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 41, loss 0.33481\n",
            "tensor(0.3331, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 42, loss 0.33306\n",
            "tensor(0.3314, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 43, loss 0.33136\n",
            "tensor(0.3297, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 44, loss 0.32972\n",
            "tensor(0.3281, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 45, loss 0.32813\n",
            "tensor(0.3266, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 46, loss 0.32658\n",
            "tensor(0.3251, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 47, loss 0.32509\n",
            "tensor(0.3236, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 48, loss 0.32364\n",
            "tensor(0.3222, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 49, loss 0.32224\n",
            "tensor(0.3209, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 50, loss 0.32087\n",
            "tensor(0.3195, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 51, loss 0.31955\n",
            "tensor(0.3183, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 52, loss 0.31826\n",
            "tensor(0.3170, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 53, loss 0.31701\n",
            "tensor(0.3158, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 54, loss 0.31579\n",
            "tensor(0.3146, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 55, loss 0.31461\n",
            "tensor(0.3135, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 56, loss 0.31346\n",
            "tensor(0.3123, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 57, loss 0.31234\n",
            "tensor(0.3113, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 58, loss 0.31126\n",
            "tensor(0.3102, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 59, loss 0.3102\n",
            "tensor(0.3092, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 60, loss 0.30916\n",
            "tensor(0.3082, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 61, loss 0.30816\n",
            "tensor(0.3072, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 62, loss 0.30718\n",
            "tensor(0.3062, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 63, loss 0.30622\n",
            "tensor(0.3053, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 64, loss 0.30529\n",
            "tensor(0.3044, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 65, loss 0.30438\n",
            "tensor(0.3035, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 66, loss 0.30349\n",
            "tensor(0.3026, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 67, loss 0.30263\n",
            "tensor(0.3018, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 68, loss 0.30178\n",
            "tensor(0.3010, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 69, loss 0.30096\n",
            "tensor(0.3002, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 70, loss 0.30015\n",
            "tensor(0.2994, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 71, loss 0.29936\n",
            "tensor(0.2986, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 72, loss 0.29859\n",
            "tensor(0.2978, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 73, loss 0.29784\n",
            "tensor(0.2971, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 74, loss 0.29711\n",
            "tensor(0.2964, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 75, loss 0.29639\n",
            "tensor(0.2957, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 76, loss 0.29568\n",
            "tensor(0.2950, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 77, loss 0.295\n",
            "tensor(0.2943, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 78, loss 0.29432\n",
            "tensor(0.2937, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 79, loss 0.29367\n",
            "tensor(0.2930, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 80, loss 0.29302\n",
            "tensor(0.2924, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 81, loss 0.29239\n",
            "tensor(0.2918, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 82, loss 0.29177\n",
            "tensor(0.2912, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 83, loss 0.29117\n",
            "tensor(0.2906, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 84, loss 0.29057\n",
            "tensor(0.2900, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 85, loss 0.28999\n",
            "tensor(0.2894, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 86, loss 0.28943\n",
            "tensor(0.2889, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 87, loss 0.28887\n",
            "tensor(0.2883, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 88, loss 0.28832\n",
            "tensor(0.2878, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 89, loss 0.28779\n",
            "tensor(0.2873, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 90, loss 0.28726\n",
            "tensor(0.2867, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 91, loss 0.28674\n",
            "tensor(0.2862, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 92, loss 0.28624\n",
            "tensor(0.2857, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 93, loss 0.28574\n",
            "tensor(0.2853, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 94, loss 0.28526\n",
            "tensor(0.2848, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 95, loss 0.28478\n",
            "tensor(0.2843, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 96, loss 0.28431\n",
            "tensor(0.2839, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 97, loss 0.28385\n",
            "tensor(0.2834, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 98, loss 0.2834\n",
            "tensor(0.2830, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 99, loss 0.28296\n",
            "tensor(0.2825, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 100, loss 0.28252\n",
            "tensor(0.2821, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 101, loss 0.2821\n",
            "tensor(0.2817, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 102, loss 0.28168\n",
            "tensor(0.2813, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 103, loss 0.28126\n",
            "tensor(0.2809, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 104, loss 0.28086\n",
            "tensor(0.2805, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 105, loss 0.28046\n",
            "tensor(0.2801, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 106, loss 0.28007\n",
            "tensor(0.2797, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 107, loss 0.27969\n",
            "tensor(0.2793, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 108, loss 0.27931\n",
            "tensor(0.2789, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 109, loss 0.27894\n",
            "tensor(0.2786, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 110, loss 0.27857\n",
            "tensor(0.2782, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 111, loss 0.27821\n",
            "tensor(0.2779, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 112, loss 0.27786\n",
            "tensor(0.2775, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 113, loss 0.27751\n",
            "tensor(0.2772, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 114, loss 0.27717\n",
            "tensor(0.2768, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 115, loss 0.27684\n",
            "tensor(0.2765, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 116, loss 0.27651\n",
            "tensor(0.2762, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 117, loss 0.27618\n",
            "tensor(0.2759, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 118, loss 0.27586\n",
            "tensor(0.2755, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 119, loss 0.27555\n",
            "tensor(0.2752, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 120, loss 0.27524\n",
            "tensor(0.2749, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 121, loss 0.27493\n",
            "tensor(0.2746, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 122, loss 0.27463\n",
            "tensor(0.2743, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 123, loss 0.27433\n",
            "tensor(0.2740, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 124, loss 0.27404\n",
            "tensor(0.2738, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 125, loss 0.27376\n",
            "tensor(0.2735, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 126, loss 0.27348\n",
            "tensor(0.2732, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 127, loss 0.2732\n",
            "tensor(0.2729, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 128, loss 0.27292\n",
            "tensor(0.2727, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 129, loss 0.27265\n",
            "tensor(0.2724, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 130, loss 0.27239\n",
            "tensor(0.2721, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 131, loss 0.27213\n",
            "tensor(0.2719, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 132, loss 0.27187\n",
            "tensor(0.2716, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 133, loss 0.27162\n",
            "tensor(0.2714, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 134, loss 0.27137\n",
            "tensor(0.2711, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 135, loss 0.27112\n",
            "tensor(0.2709, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 136, loss 0.27088\n",
            "tensor(0.2706, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 137, loss 0.27064\n",
            "tensor(0.2704, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 138, loss 0.2704\n",
            "tensor(0.2702, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 139, loss 0.27017\n",
            "tensor(0.2699, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 140, loss 0.26994\n",
            "tensor(0.2697, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 141, loss 0.26971\n",
            "tensor(0.2695, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 142, loss 0.26949\n",
            "tensor(0.2693, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 143, loss 0.26927\n",
            "tensor(0.2691, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 144, loss 0.26906\n",
            "tensor(0.2688, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 145, loss 0.26884\n",
            "tensor(0.2686, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 146, loss 0.26863\n",
            "tensor(0.2684, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 147, loss 0.26842\n",
            "tensor(0.2682, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 148, loss 0.26822\n",
            "tensor(0.2680, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 149, loss 0.26802\n",
            "tensor(0.2678, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 150, loss 0.26782\n",
            "tensor(0.2676, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 151, loss 0.26762\n",
            "tensor(0.2674, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 152, loss 0.26743\n",
            "tensor(0.2672, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 153, loss 0.26723\n",
            "tensor(0.2670, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 154, loss 0.26705\n",
            "tensor(0.2669, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 155, loss 0.26686\n",
            "tensor(0.2667, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 156, loss 0.26667\n",
            "tensor(0.2665, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 157, loss 0.26649\n",
            "tensor(0.2663, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 158, loss 0.26631\n",
            "tensor(0.2661, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 159, loss 0.26614\n",
            "tensor(0.2660, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 160, loss 0.26596\n",
            "tensor(0.2658, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 161, loss 0.26579\n",
            "tensor(0.2656, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 162, loss 0.26562\n",
            "tensor(0.2655, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 163, loss 0.26545\n",
            "tensor(0.2653, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 164, loss 0.26529\n",
            "tensor(0.2651, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 165, loss 0.26512\n",
            "tensor(0.2650, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 166, loss 0.26496\n",
            "tensor(0.2648, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 167, loss 0.2648\n",
            "tensor(0.2646, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 168, loss 0.26464\n",
            "tensor(0.2645, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 169, loss 0.26449\n",
            "tensor(0.2643, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 170, loss 0.26433\n",
            "tensor(0.2642, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 171, loss 0.26418\n",
            "tensor(0.2640, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 172, loss 0.26403\n",
            "tensor(0.2639, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 173, loss 0.26388\n",
            "tensor(0.2637, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 174, loss 0.26374\n",
            "tensor(0.2636, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 175, loss 0.26359\n",
            "tensor(0.2635, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 176, loss 0.26345\n",
            "tensor(0.2633, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 177, loss 0.26331\n",
            "tensor(0.2632, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 178, loss 0.26317\n",
            "tensor(0.2630, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 179, loss 0.26303\n",
            "tensor(0.2629, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 180, loss 0.2629\n",
            "tensor(0.2628, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 181, loss 0.26276\n",
            "tensor(0.2626, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 182, loss 0.26263\n",
            "tensor(0.2625, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 183, loss 0.2625\n",
            "tensor(0.2624, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 184, loss 0.26237\n",
            "tensor(0.2622, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 185, loss 0.26224\n",
            "tensor(0.2621, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 186, loss 0.26211\n",
            "tensor(0.2620, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 187, loss 0.26199\n",
            "tensor(0.2619, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 188, loss 0.26186\n",
            "tensor(0.2617, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 189, loss 0.26174\n",
            "tensor(0.2616, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 190, loss 0.26162\n",
            "tensor(0.2615, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 191, loss 0.2615\n",
            "tensor(0.2614, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 192, loss 0.26138\n",
            "tensor(0.2613, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 193, loss 0.26127\n",
            "tensor(0.2612, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 194, loss 0.26115\n",
            "tensor(0.2610, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 195, loss 0.26104\n",
            "tensor(0.2609, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 196, loss 0.26092\n",
            "tensor(0.2608, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 197, loss 0.26081\n",
            "tensor(0.2607, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 198, loss 0.2607\n",
            "tensor(0.2606, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 199, loss 0.26059\n",
            "tensor(0.2605, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 200, loss 0.26049\n",
            "tensor(0.2604, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 201, loss 0.26038\n",
            "tensor(0.2603, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 202, loss 0.26027\n",
            "tensor(0.2602, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 203, loss 0.26017\n",
            "tensor(0.2601, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 204, loss 0.26007\n",
            "tensor(0.2600, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 205, loss 0.25996\n",
            "tensor(0.2599, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 206, loss 0.25986\n",
            "tensor(0.2598, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 207, loss 0.25976\n",
            "tensor(0.2597, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 208, loss 0.25966\n",
            "tensor(0.2596, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 209, loss 0.25957\n",
            "tensor(0.2595, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 210, loss 0.25947\n",
            "tensor(0.2594, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 211, loss 0.25937\n",
            "tensor(0.2593, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 212, loss 0.25928\n",
            "tensor(0.2592, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 213, loss 0.25919\n",
            "tensor(0.2591, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 214, loss 0.25909\n",
            "tensor(0.2590, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 215, loss 0.259\n",
            "tensor(0.2589, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 216, loss 0.25891\n",
            "tensor(0.2588, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 217, loss 0.25882\n",
            "tensor(0.2587, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 218, loss 0.25873\n",
            "tensor(0.2586, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 219, loss 0.25865\n",
            "tensor(0.2586, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 220, loss 0.25856\n",
            "tensor(0.2585, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 221, loss 0.25847\n",
            "tensor(0.2584, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 222, loss 0.25839\n",
            "tensor(0.2583, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 223, loss 0.2583\n",
            "tensor(0.2582, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 224, loss 0.25822\n",
            "tensor(0.2581, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 225, loss 0.25814\n",
            "tensor(0.2581, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 226, loss 0.25806\n",
            "tensor(0.2580, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 227, loss 0.25798\n",
            "tensor(0.2579, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 228, loss 0.2579\n",
            "tensor(0.2578, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 229, loss 0.25782\n",
            "tensor(0.2577, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 230, loss 0.25774\n",
            "tensor(0.2577, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 231, loss 0.25766\n",
            "tensor(0.2576, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 232, loss 0.25758\n",
            "tensor(0.2575, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 233, loss 0.25751\n",
            "tensor(0.2574, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 234, loss 0.25743\n",
            "tensor(0.2574, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 235, loss 0.25736\n",
            "tensor(0.2573, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 236, loss 0.25728\n",
            "tensor(0.2572, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 237, loss 0.25721\n",
            "tensor(0.2571, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 238, loss 0.25714\n",
            "tensor(0.2571, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 239, loss 0.25707\n",
            "tensor(0.2570, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 240, loss 0.257\n",
            "tensor(0.2569, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 241, loss 0.25693\n",
            "tensor(0.2569, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 242, loss 0.25686\n",
            "tensor(0.2568, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 243, loss 0.25679\n",
            "tensor(0.2567, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 244, loss 0.25672\n",
            "tensor(0.2567, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 245, loss 0.25665\n",
            "tensor(0.2566, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 246, loss 0.25659\n",
            "tensor(0.2565, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 247, loss 0.25652\n",
            "tensor(0.2565, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 248, loss 0.25646\n",
            "tensor(0.2564, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 249, loss 0.25639\n",
            "tensor(0.2563, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 250, loss 0.25633\n",
            "tensor(0.2563, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 251, loss 0.25626\n",
            "tensor(0.2562, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 252, loss 0.2562\n",
            "tensor(0.2561, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 253, loss 0.25614\n",
            "tensor(0.2561, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 254, loss 0.25608\n",
            "tensor(0.2560, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 255, loss 0.25602\n",
            "tensor(0.2560, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 256, loss 0.25595\n",
            "tensor(0.2559, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 257, loss 0.25589\n",
            "tensor(0.2558, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 258, loss 0.25584\n",
            "tensor(0.2558, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 259, loss 0.25578\n",
            "tensor(0.2557, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 260, loss 0.25572\n",
            "tensor(0.2557, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 261, loss 0.25566\n",
            "tensor(0.2556, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 262, loss 0.2556\n",
            "tensor(0.2555, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 263, loss 0.25555\n",
            "tensor(0.2555, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 264, loss 0.25549\n",
            "tensor(0.2554, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 265, loss 0.25544\n",
            "tensor(0.2554, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 266, loss 0.25538\n",
            "tensor(0.2553, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 267, loss 0.25533\n",
            "tensor(0.2553, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 268, loss 0.25527\n",
            "tensor(0.2552, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 269, loss 0.25522\n",
            "tensor(0.2552, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 270, loss 0.25517\n",
            "tensor(0.2551, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 271, loss 0.25511\n",
            "tensor(0.2551, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 272, loss 0.25506\n",
            "tensor(0.2550, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 273, loss 0.25501\n",
            "tensor(0.2550, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 274, loss 0.25496\n",
            "tensor(0.2549, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 275, loss 0.25491\n",
            "tensor(0.2549, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 276, loss 0.25486\n",
            "tensor(0.2548, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 277, loss 0.25481\n",
            "tensor(0.2548, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 278, loss 0.25476\n",
            "tensor(0.2547, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 279, loss 0.25471\n",
            "tensor(0.2547, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 280, loss 0.25466\n",
            "tensor(0.2546, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 281, loss 0.25461\n",
            "tensor(0.2546, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 282, loss 0.25457\n",
            "tensor(0.2545, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 283, loss 0.25452\n",
            "tensor(0.2545, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 284, loss 0.25447\n",
            "tensor(0.2544, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 285, loss 0.25443\n",
            "tensor(0.2544, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 286, loss 0.25438\n",
            "tensor(0.2543, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 287, loss 0.25433\n",
            "tensor(0.2543, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 288, loss 0.25429\n",
            "tensor(0.2542, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 289, loss 0.25424\n",
            "tensor(0.2542, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 290, loss 0.2542\n",
            "tensor(0.2542, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 291, loss 0.25416\n",
            "tensor(0.2541, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 292, loss 0.25411\n",
            "tensor(0.2541, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 293, loss 0.25407\n",
            "tensor(0.2540, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 294, loss 0.25403\n",
            "tensor(0.2540, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 295, loss 0.25399\n",
            "tensor(0.2539, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 296, loss 0.25394\n",
            "tensor(0.2539, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 297, loss 0.2539\n",
            "tensor(0.2539, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 298, loss 0.25386\n",
            "tensor(0.2538, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 299, loss 0.25382\n",
            "tensor(0.2538, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 300, loss 0.25378\n",
            "tensor(0.2537, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 301, loss 0.25374\n",
            "tensor(0.2537, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 302, loss 0.2537\n",
            "tensor(0.2537, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 303, loss 0.25366\n",
            "tensor(0.2536, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 304, loss 0.25362\n",
            "tensor(0.2536, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 305, loss 0.25358\n",
            "tensor(0.2535, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 306, loss 0.25354\n",
            "tensor(0.2535, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 307, loss 0.25351\n",
            "tensor(0.2535, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 308, loss 0.25347\n",
            "tensor(0.2534, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 309, loss 0.25343\n",
            "tensor(0.2534, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 310, loss 0.25339\n",
            "tensor(0.2534, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 311, loss 0.25336\n",
            "tensor(0.2533, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 312, loss 0.25332\n",
            "tensor(0.2533, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 313, loss 0.25328\n",
            "tensor(0.2532, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 314, loss 0.25325\n",
            "tensor(0.2532, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 315, loss 0.25321\n",
            "tensor(0.2532, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 316, loss 0.25318\n",
            "tensor(0.2531, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 317, loss 0.25314\n",
            "tensor(0.2531, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 318, loss 0.25311\n",
            "tensor(0.2531, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 319, loss 0.25307\n",
            "tensor(0.2530, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 320, loss 0.25304\n",
            "tensor(0.2530, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 321, loss 0.25301\n",
            "tensor(0.2530, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 322, loss 0.25297\n",
            "tensor(0.2529, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 323, loss 0.25294\n",
            "tensor(0.2529, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 324, loss 0.25291\n",
            "tensor(0.2529, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 325, loss 0.25287\n",
            "tensor(0.2528, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 326, loss 0.25284\n",
            "tensor(0.2528, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 327, loss 0.25281\n",
            "tensor(0.2528, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 328, loss 0.25278\n",
            "tensor(0.2527, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 329, loss 0.25274\n",
            "tensor(0.2527, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 330, loss 0.25271\n",
            "tensor(0.2527, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 331, loss 0.25268\n",
            "tensor(0.2527, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 332, loss 0.25265\n",
            "tensor(0.2526, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 333, loss 0.25262\n",
            "tensor(0.2526, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 334, loss 0.25259\n",
            "tensor(0.2526, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 335, loss 0.25256\n",
            "tensor(0.2525, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 336, loss 0.25253\n",
            "tensor(0.2525, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 337, loss 0.2525\n",
            "tensor(0.2525, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 338, loss 0.25247\n",
            "tensor(0.2524, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 339, loss 0.25244\n",
            "tensor(0.2524, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 340, loss 0.25241\n",
            "tensor(0.2524, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 341, loss 0.25238\n",
            "tensor(0.2524, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 342, loss 0.25236\n",
            "tensor(0.2523, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 343, loss 0.25233\n",
            "tensor(0.2523, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 344, loss 0.2523\n",
            "tensor(0.2523, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 345, loss 0.25227\n",
            "tensor(0.2522, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 346, loss 0.25225\n",
            "tensor(0.2522, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 347, loss 0.25222\n",
            "tensor(0.2522, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 348, loss 0.25219\n",
            "tensor(0.2522, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 349, loss 0.25216\n",
            "tensor(0.2521, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 350, loss 0.25214\n",
            "tensor(0.2521, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 351, loss 0.25211\n",
            "tensor(0.2521, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 352, loss 0.25208\n",
            "tensor(0.2521, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 353, loss 0.25206\n",
            "tensor(0.2520, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 354, loss 0.25203\n",
            "tensor(0.2520, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 355, loss 0.25201\n",
            "tensor(0.2520, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 356, loss 0.25198\n",
            "tensor(0.2520, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 357, loss 0.25196\n",
            "tensor(0.2519, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 358, loss 0.25193\n",
            "tensor(0.2519, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 359, loss 0.25191\n",
            "tensor(0.2519, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 360, loss 0.25188\n",
            "tensor(0.2519, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 361, loss 0.25186\n",
            "tensor(0.2518, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 362, loss 0.25183\n",
            "tensor(0.2518, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 363, loss 0.25181\n",
            "tensor(0.2518, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 364, loss 0.25179\n",
            "tensor(0.2518, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 365, loss 0.25176\n",
            "tensor(0.2517, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 366, loss 0.25174\n",
            "tensor(0.2517, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 367, loss 0.25172\n",
            "tensor(0.2517, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 368, loss 0.25169\n",
            "tensor(0.2517, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 369, loss 0.25167\n",
            "tensor(0.2516, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 370, loss 0.25165\n",
            "tensor(0.2516, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 371, loss 0.25162\n",
            "tensor(0.2516, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 372, loss 0.2516\n",
            "tensor(0.2516, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 373, loss 0.25158\n",
            "tensor(0.2516, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 374, loss 0.25156\n",
            "tensor(0.2515, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 375, loss 0.25154\n",
            "tensor(0.2515, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 376, loss 0.25151\n",
            "tensor(0.2515, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 377, loss 0.25149\n",
            "tensor(0.2515, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 378, loss 0.25147\n",
            "tensor(0.2515, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 379, loss 0.25145\n",
            "tensor(0.2514, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 380, loss 0.25143\n",
            "tensor(0.2514, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 381, loss 0.25141\n",
            "tensor(0.2514, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 382, loss 0.25139\n",
            "tensor(0.2514, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 383, loss 0.25137\n",
            "tensor(0.2513, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 384, loss 0.25135\n",
            "tensor(0.2513, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 385, loss 0.25133\n",
            "tensor(0.2513, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 386, loss 0.25131\n",
            "tensor(0.2513, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 387, loss 0.25129\n",
            "tensor(0.2513, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 388, loss 0.25127\n",
            "tensor(0.2512, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 389, loss 0.25125\n",
            "tensor(0.2512, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 390, loss 0.25123\n",
            "tensor(0.2512, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 391, loss 0.25121\n",
            "tensor(0.2512, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 392, loss 0.25119\n",
            "tensor(0.2512, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 393, loss 0.25117\n",
            "tensor(0.2512, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 394, loss 0.25115\n",
            "tensor(0.2511, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 395, loss 0.25113\n",
            "tensor(0.2511, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 396, loss 0.25111\n",
            "tensor(0.2511, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 397, loss 0.2511\n",
            "tensor(0.2511, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 398, loss 0.25108\n",
            "tensor(0.2511, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 399, loss 0.25106\n",
            "tensor(0.2510, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 400, loss 0.25104\n",
            "tensor(0.2510, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 401, loss 0.25102\n",
            "tensor(0.2510, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 402, loss 0.25101\n",
            "tensor(0.2510, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 403, loss 0.25099\n",
            "tensor(0.2510, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 404, loss 0.25097\n",
            "tensor(0.2510, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 405, loss 0.25095\n",
            "tensor(0.2509, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 406, loss 0.25094\n",
            "tensor(0.2509, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 407, loss 0.25092\n",
            "tensor(0.2509, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 408, loss 0.2509\n",
            "tensor(0.2509, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 409, loss 0.25088\n",
            "tensor(0.2509, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 410, loss 0.25087\n",
            "tensor(0.2509, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 411, loss 0.25085\n",
            "tensor(0.2508, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 412, loss 0.25083\n",
            "tensor(0.2508, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 413, loss 0.25082\n",
            "tensor(0.2508, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 414, loss 0.2508\n",
            "tensor(0.2508, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 415, loss 0.25079\n",
            "tensor(0.2508, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 416, loss 0.25077\n",
            "tensor(0.2508, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 417, loss 0.25075\n",
            "tensor(0.2507, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 418, loss 0.25074\n",
            "tensor(0.2507, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 419, loss 0.25072\n",
            "tensor(0.2507, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 420, loss 0.25071\n",
            "tensor(0.2507, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 421, loss 0.25069\n",
            "tensor(0.2507, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 422, loss 0.25068\n",
            "tensor(0.2507, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 423, loss 0.25066\n",
            "tensor(0.2506, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 424, loss 0.25065\n",
            "tensor(0.2506, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 425, loss 0.25063\n",
            "tensor(0.2506, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 426, loss 0.25062\n",
            "tensor(0.2506, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 427, loss 0.2506\n",
            "tensor(0.2506, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 428, loss 0.25059\n",
            "tensor(0.2506, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 429, loss 0.25057\n",
            "tensor(0.2506, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 430, loss 0.25056\n",
            "tensor(0.2505, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 431, loss 0.25054\n",
            "tensor(0.2505, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 432, loss 0.25053\n",
            "tensor(0.2505, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 433, loss 0.25051\n",
            "tensor(0.2505, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 434, loss 0.2505\n",
            "tensor(0.2505, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 435, loss 0.25049\n",
            "tensor(0.2505, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 436, loss 0.25047\n",
            "tensor(0.2505, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 437, loss 0.25046\n",
            "tensor(0.2504, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 438, loss 0.25045\n",
            "tensor(0.2504, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 439, loss 0.25043\n",
            "tensor(0.2504, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 440, loss 0.25042\n",
            "tensor(0.2504, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 441, loss 0.25041\n",
            "tensor(0.2504, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 442, loss 0.25039\n",
            "tensor(0.2504, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 443, loss 0.25038\n",
            "tensor(0.2504, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 444, loss 0.25037\n",
            "tensor(0.2504, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 445, loss 0.25035\n",
            "tensor(0.2503, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 446, loss 0.25034\n",
            "tensor(0.2503, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 447, loss 0.25033\n",
            "tensor(0.2503, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 448, loss 0.25031\n",
            "tensor(0.2503, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 449, loss 0.2503\n",
            "tensor(0.2503, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 450, loss 0.25029\n",
            "tensor(0.2503, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 451, loss 0.25028\n",
            "tensor(0.2503, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 452, loss 0.25026\n",
            "tensor(0.2503, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 453, loss 0.25025\n",
            "tensor(0.2502, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 454, loss 0.25024\n",
            "tensor(0.2502, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 455, loss 0.25023\n",
            "tensor(0.2502, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 456, loss 0.25022\n",
            "tensor(0.2502, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 457, loss 0.2502\n",
            "tensor(0.2502, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 458, loss 0.25019\n",
            "tensor(0.2502, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 459, loss 0.25018\n",
            "tensor(0.2502, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 460, loss 0.25017\n",
            "tensor(0.2502, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 461, loss 0.25016\n",
            "tensor(0.2501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 462, loss 0.25015\n",
            "tensor(0.2501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 463, loss 0.25013\n",
            "tensor(0.2501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 464, loss 0.25012\n",
            "tensor(0.2501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 465, loss 0.25011\n",
            "tensor(0.2501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 466, loss 0.2501\n",
            "tensor(0.2501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 467, loss 0.25009\n",
            "tensor(0.2501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 468, loss 0.25008\n",
            "tensor(0.2501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 469, loss 0.25007\n",
            "tensor(0.2501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 470, loss 0.25006\n",
            "tensor(0.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 471, loss 0.25005\n",
            "tensor(0.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 472, loss 0.25004\n",
            "tensor(0.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 473, loss 0.25002\n",
            "tensor(0.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 474, loss 0.25001\n",
            "tensor(0.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 475, loss 0.25\n",
            "tensor(0.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 476, loss 0.24999\n",
            "tensor(0.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 477, loss 0.24998\n",
            "tensor(0.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 478, loss 0.24997\n",
            "tensor(0.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 479, loss 0.24996\n",
            "tensor(0.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 480, loss 0.24995\n",
            "tensor(0.2499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 481, loss 0.24994\n",
            "tensor(0.2499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 482, loss 0.24993\n",
            "tensor(0.2499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 483, loss 0.24992\n",
            "tensor(0.2499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 484, loss 0.24991\n",
            "tensor(0.2499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 485, loss 0.2499\n",
            "tensor(0.2499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 486, loss 0.24989\n",
            "tensor(0.2499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 487, loss 0.24988\n",
            "tensor(0.2499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 488, loss 0.24987\n",
            "tensor(0.2499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 489, loss 0.24986\n",
            "tensor(0.2499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 490, loss 0.24985\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 491, loss 0.24985\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 492, loss 0.24984\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 493, loss 0.24983\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 494, loss 0.24982\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 495, loss 0.24981\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 496, loss 0.2498\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 497, loss 0.24979\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 498, loss 0.24978\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 499, loss 0.24977\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 500, loss 0.24976\n",
            "tensor(0.2498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 501, loss 0.24975\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 502, loss 0.24975\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 503, loss 0.24974\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 504, loss 0.24973\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 505, loss 0.24972\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 506, loss 0.24971\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 507, loss 0.2497\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 508, loss 0.24969\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 509, loss 0.24969\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 510, loss 0.24968\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 511, loss 0.24967\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 512, loss 0.24966\n",
            "tensor(0.2497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 513, loss 0.24965\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 514, loss 0.24965\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 515, loss 0.24964\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 516, loss 0.24963\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 517, loss 0.24962\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 518, loss 0.24961\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 519, loss 0.24961\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 520, loss 0.2496\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 521, loss 0.24959\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 522, loss 0.24958\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 523, loss 0.24958\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 524, loss 0.24957\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 525, loss 0.24956\n",
            "tensor(0.2496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 526, loss 0.24955\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 527, loss 0.24955\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 528, loss 0.24954\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 529, loss 0.24953\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 530, loss 0.24952\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 531, loss 0.24952\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 532, loss 0.24951\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 533, loss 0.2495\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 534, loss 0.2495\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 535, loss 0.24949\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 536, loss 0.24948\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 537, loss 0.24948\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 538, loss 0.24947\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 539, loss 0.24946\n",
            "tensor(0.2495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 540, loss 0.24945\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 541, loss 0.24945\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 542, loss 0.24944\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 543, loss 0.24943\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 544, loss 0.24943\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 545, loss 0.24942\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 546, loss 0.24942\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 547, loss 0.24941\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 548, loss 0.2494\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 549, loss 0.2494\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 550, loss 0.24939\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 551, loss 0.24938\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 552, loss 0.24938\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 553, loss 0.24937\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 554, loss 0.24936\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 555, loss 0.24936\n",
            "tensor(0.2494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 556, loss 0.24935\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 557, loss 0.24935\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 558, loss 0.24934\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 559, loss 0.24933\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 560, loss 0.24933\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 561, loss 0.24932\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 562, loss 0.24932\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 563, loss 0.24931\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 564, loss 0.24931\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 565, loss 0.2493\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 566, loss 0.24929\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 567, loss 0.24929\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 568, loss 0.24928\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 569, loss 0.24928\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 570, loss 0.24927\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 571, loss 0.24927\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 572, loss 0.24926\n",
            "tensor(0.2493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 573, loss 0.24926\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 574, loss 0.24925\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 575, loss 0.24924\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 576, loss 0.24924\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 577, loss 0.24923\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 578, loss 0.24923\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 579, loss 0.24922\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 580, loss 0.24922\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 581, loss 0.24921\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 582, loss 0.24921\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 583, loss 0.2492\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 584, loss 0.2492\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 585, loss 0.24919\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 586, loss 0.24919\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 587, loss 0.24918\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 588, loss 0.24918\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 589, loss 0.24917\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 590, loss 0.24917\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 591, loss 0.24916\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 592, loss 0.24916\n",
            "tensor(0.2492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 593, loss 0.24915\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 594, loss 0.24915\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 595, loss 0.24914\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 596, loss 0.24914\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 597, loss 0.24913\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 598, loss 0.24913\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 599, loss 0.24913\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 600, loss 0.24912\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 601, loss 0.24912\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 602, loss 0.24911\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 603, loss 0.24911\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 604, loss 0.2491\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 605, loss 0.2491\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 606, loss 0.24909\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 607, loss 0.24909\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 608, loss 0.24909\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 609, loss 0.24908\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 610, loss 0.24908\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 611, loss 0.24907\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 612, loss 0.24907\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 613, loss 0.24906\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 614, loss 0.24906\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 615, loss 0.24906\n",
            "tensor(0.2491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 616, loss 0.24905\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 617, loss 0.24905\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 618, loss 0.24904\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 619, loss 0.24904\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 620, loss 0.24903\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 621, loss 0.24903\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 622, loss 0.24903\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 623, loss 0.24902\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 624, loss 0.24902\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 625, loss 0.24902\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 626, loss 0.24901\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 627, loss 0.24901\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 628, loss 0.249\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 629, loss 0.249\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 630, loss 0.249\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 631, loss 0.24899\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 632, loss 0.24899\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 633, loss 0.24899\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 634, loss 0.24898\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 635, loss 0.24898\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 636, loss 0.24897\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 637, loss 0.24897\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 638, loss 0.24897\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 639, loss 0.24896\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 640, loss 0.24896\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 641, loss 0.24896\n",
            "tensor(0.2490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 642, loss 0.24895\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 643, loss 0.24895\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 644, loss 0.24895\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 645, loss 0.24894\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 646, loss 0.24894\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 647, loss 0.24894\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 648, loss 0.24893\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 649, loss 0.24893\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 650, loss 0.24893\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 651, loss 0.24892\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 652, loss 0.24892\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 653, loss 0.24892\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 654, loss 0.24891\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 655, loss 0.24891\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 656, loss 0.24891\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 657, loss 0.2489\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 658, loss 0.2489\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 659, loss 0.2489\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 660, loss 0.24889\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 661, loss 0.24889\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 662, loss 0.24889\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 663, loss 0.24888\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 664, loss 0.24888\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 665, loss 0.24888\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 666, loss 0.24887\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 667, loss 0.24887\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 668, loss 0.24887\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 669, loss 0.24887\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 670, loss 0.24886\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 671, loss 0.24886\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 672, loss 0.24886\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 673, loss 0.24885\n",
            "tensor(0.2489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 674, loss 0.24885\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 675, loss 0.24885\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 676, loss 0.24885\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 677, loss 0.24884\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 678, loss 0.24884\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 679, loss 0.24884\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 680, loss 0.24883\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 681, loss 0.24883\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 682, loss 0.24883\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 683, loss 0.24883\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 684, loss 0.24882\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 685, loss 0.24882\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 686, loss 0.24882\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 687, loss 0.24882\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 688, loss 0.24881\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 689, loss 0.24881\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 690, loss 0.24881\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 691, loss 0.2488\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 692, loss 0.2488\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 693, loss 0.2488\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 694, loss 0.2488\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 695, loss 0.24879\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 696, loss 0.24879\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 697, loss 0.24879\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 698, loss 0.24879\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 699, loss 0.24878\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 700, loss 0.24878\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 701, loss 0.24878\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 702, loss 0.24878\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 703, loss 0.24877\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 704, loss 0.24877\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 705, loss 0.24877\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 706, loss 0.24877\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 707, loss 0.24876\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 708, loss 0.24876\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 709, loss 0.24876\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 710, loss 0.24876\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 711, loss 0.24876\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 712, loss 0.24875\n",
            "tensor(0.2488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 713, loss 0.24875\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 714, loss 0.24875\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 715, loss 0.24875\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 716, loss 0.24874\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 717, loss 0.24874\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 718, loss 0.24874\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 719, loss 0.24874\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 720, loss 0.24874\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 721, loss 0.24873\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 722, loss 0.24873\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 723, loss 0.24873\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 724, loss 0.24873\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 725, loss 0.24872\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 726, loss 0.24872\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 727, loss 0.24872\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 728, loss 0.24872\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 729, loss 0.24872\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 730, loss 0.24871\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 731, loss 0.24871\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 732, loss 0.24871\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 733, loss 0.24871\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 734, loss 0.24871\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 735, loss 0.2487\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 736, loss 0.2487\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 737, loss 0.2487\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 738, loss 0.2487\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 739, loss 0.2487\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 740, loss 0.24869\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 741, loss 0.24869\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 742, loss 0.24869\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 743, loss 0.24869\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 744, loss 0.24869\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 745, loss 0.24869\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 746, loss 0.24868\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 747, loss 0.24868\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 748, loss 0.24868\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 749, loss 0.24868\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 750, loss 0.24868\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 751, loss 0.24867\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 752, loss 0.24867\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 753, loss 0.24867\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 754, loss 0.24867\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 755, loss 0.24867\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 756, loss 0.24867\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 757, loss 0.24866\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 758, loss 0.24866\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 759, loss 0.24866\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 760, loss 0.24866\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 761, loss 0.24866\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 762, loss 0.24865\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 763, loss 0.24865\n",
            "tensor(0.2487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 764, loss 0.24865\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 765, loss 0.24865\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 766, loss 0.24865\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 767, loss 0.24865\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 768, loss 0.24865\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 769, loss 0.24864\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 770, loss 0.24864\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 771, loss 0.24864\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 772, loss 0.24864\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 773, loss 0.24864\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 774, loss 0.24864\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 775, loss 0.24863\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 776, loss 0.24863\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 777, loss 0.24863\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 778, loss 0.24863\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 779, loss 0.24863\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 780, loss 0.24863\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 781, loss 0.24862\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 782, loss 0.24862\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 783, loss 0.24862\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 784, loss 0.24862\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 785, loss 0.24862\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 786, loss 0.24862\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 787, loss 0.24862\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 788, loss 0.24861\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 789, loss 0.24861\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 790, loss 0.24861\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 791, loss 0.24861\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 792, loss 0.24861\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 793, loss 0.24861\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 794, loss 0.24861\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 795, loss 0.2486\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 796, loss 0.2486\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 797, loss 0.2486\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 798, loss 0.2486\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 799, loss 0.2486\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 800, loss 0.2486\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 801, loss 0.2486\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 802, loss 0.24859\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 803, loss 0.24859\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 804, loss 0.24859\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 805, loss 0.24859\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 806, loss 0.24859\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 807, loss 0.24859\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 808, loss 0.24859\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 809, loss 0.24859\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 810, loss 0.24858\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 811, loss 0.24858\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 812, loss 0.24858\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 813, loss 0.24858\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 814, loss 0.24858\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 815, loss 0.24858\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 816, loss 0.24858\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 817, loss 0.24858\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 818, loss 0.24857\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 819, loss 0.24857\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 820, loss 0.24857\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 821, loss 0.24857\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 822, loss 0.24857\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 823, loss 0.24857\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 824, loss 0.24857\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 825, loss 0.24857\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 826, loss 0.24856\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 827, loss 0.24856\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 828, loss 0.24856\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 829, loss 0.24856\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 830, loss 0.24856\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 831, loss 0.24856\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 832, loss 0.24856\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 833, loss 0.24856\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 834, loss 0.24856\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 835, loss 0.24855\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 836, loss 0.24855\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 837, loss 0.24855\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 838, loss 0.24855\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 839, loss 0.24855\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 840, loss 0.24855\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 841, loss 0.24855\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 842, loss 0.24855\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 843, loss 0.24855\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 844, loss 0.24854\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 845, loss 0.24854\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 846, loss 0.24854\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 847, loss 0.24854\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 848, loss 0.24854\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 849, loss 0.24854\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 850, loss 0.24854\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 851, loss 0.24854\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 852, loss 0.24854\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 853, loss 0.24854\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 854, loss 0.24853\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 855, loss 0.24853\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 856, loss 0.24853\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 857, loss 0.24853\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 858, loss 0.24853\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 859, loss 0.24853\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 860, loss 0.24853\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 861, loss 0.24853\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 862, loss 0.24853\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 863, loss 0.24853\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 864, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 865, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 866, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 867, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 868, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 869, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 870, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 871, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 872, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 873, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 874, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 875, loss 0.24852\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 876, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 877, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 878, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 879, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 880, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 881, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 882, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 883, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 884, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 885, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 886, loss 0.24851\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 887, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 888, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 889, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 890, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 891, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 892, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 893, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 894, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 895, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 896, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 897, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 898, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 899, loss 0.2485\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 900, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 901, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 902, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 903, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 904, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 905, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 906, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 907, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 908, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 909, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 910, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 911, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 912, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 913, loss 0.24849\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 914, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 915, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 916, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 917, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 918, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 919, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 920, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 921, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 922, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 923, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 924, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 925, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 926, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 927, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 928, loss 0.24848\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 929, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 930, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 931, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 932, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 933, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 934, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 935, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 936, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 937, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 938, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 939, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 940, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 941, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 942, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 943, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 944, loss 0.24847\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 945, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 946, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 947, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 948, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 949, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 950, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 951, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 952, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 953, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 954, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 955, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 956, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 957, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 958, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 959, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 960, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 961, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 962, loss 0.24846\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 963, loss 0.24845\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 964, loss 0.24845\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 965, loss 0.24845\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 966, loss 0.24845\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 967, loss 0.24845\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 968, loss 0.24845\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 969, loss 0.24845\n",
            "tensor(0.2485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 970, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 971, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 972, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 973, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 974, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 975, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 976, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 977, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 978, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 979, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 980, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 981, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 982, loss 0.24845\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 983, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 984, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 985, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 986, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 987, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 988, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 989, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 990, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 991, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 992, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 993, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 994, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 995, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 996, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 997, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 998, loss 0.24844\n",
            "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 999, loss 0.24844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMyRqiklxM2f",
        "colab_type": "code",
        "outputId": "9e3c4af0-597e-49c8-da79-a026e06c5595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "plt.title(\"Loss over epochs\",fontsize=16)\n",
        "#plt.plot([e for e in range(epochs)],running_loss)\n",
        "plt.plot(running_loss)\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"Epochs\",fontsize=15)\n",
        "plt.ylabel(\"Training loss\",fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEdCAYAAABzBQKpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nExGRnKLEJyIiOUWJT0REcsr/B6fRFkFQvT6LAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 504x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylt9eCLUOeID",
        "colab_type": "text"
      },
      "source": [
        "### Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-3jwmfMsSsr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "588753a5-4034-4d43-a009-e489388c99f9"
      },
      "source": [
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_pred_torch=model.forward(X)\n",
        "\n",
        "y_pred_numpy = y_pred_torch.detach().numpy()\n",
        "y_pred=list(y_pred_numpy > 0.5)\n",
        "\n",
        "print(classification_report(y, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.89      0.90       500\n",
            "         1.0       0.89      0.91      0.90       500\n",
            "\n",
            "    accuracy                           0.90      1000\n",
            "   macro avg       0.90      0.90      0.90      1000\n",
            "weighted avg       0.90      0.90      0.90      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}