{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Logistic PyTorch.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwFZsMuvOeHd",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch Logistic Regression example\n",
        "\n",
        "### Moshe Wasserblat <br><br> March 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytNZMY1Tfau6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification,make_regression\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40rq_w-iOeHp",
        "colab_type": "text"
      },
      "source": [
        "### Input feature and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI_8q9A85Fq8",
        "colab_type": "code",
        "outputId": "db533c4d-281e-43bf-a1f9-7ff7a1be500e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,\n",
        "\tn_clusters_per_class=1, weights=[0.50], flip_y=0, random_state=4)\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "\n",
        "# scatter plot of examples by class label\n",
        "for label, _ in counter.items():\n",
        "\trow_ix = np.where(y == label)[0]\n",
        "\tplt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "X = torch.from_numpy(X)\n",
        "y = torch.from_numpy(y)\n",
        "\n",
        "X.shape\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0.0: 500, 1.0: 500})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE0JIwaGQE0JIwaGQE0JIwfn/8Cm62GEfAt8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5rBpWAiOeHw",
        "colab_type": "text"
      },
      "source": [
        "### Model Arch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNCwffzrOeHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        #Inputs with linear transformation\n",
        "        self.output = torch.nn.Linear(2, 1)\n",
        "        # Activation function for the output layer - sigmoid\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.output(x)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW_bzzBqOeH1",
        "colab_type": "text"
      },
      "source": [
        "### Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ7hdneCOeH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "092b48b0-2be3-42f6-b5df-5a2dedd234db"
      },
      "source": [
        "learningRate = 0.1 \n",
        "epochs = 1000\n",
        "running_loss = []\n",
        "\n",
        "model = LogisticRegression()\n",
        "print(model)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(\n",
            "  (output): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI6dcqPqOeH5",
        "colab_type": "text"
      },
      "source": [
        "### Define loss & optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOXnB27lOeH6",
        "colab_type": "code",
        "outputId": "acc09a9a-efe4-4bd3-a865-d2379b4f0237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "criterion = torch.nn.BCELoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
        "\n",
        "\n",
        "logits_numpy = model.forward(X).detach().numpy().flatten()\n",
        "plt.figure(figsize=(15,3))\n",
        "plt.title(\"Output probabilities with the untrained model\",fontsize=16)\n",
        "plt.bar([i for i in range(1000)],height=logits_numpy)\n",
        "plt.show()\n",
        "\n",
        "logits = model.forward(X) # Output of the forward pass (logits i.e. probabilities)\n",
        "loss = criterion(logits,y)\n",
        "print(loss.item())\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAADUCAYAAADtA9CTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nljFQkyRJkqSW+f+2YxZ156YBNAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.7748286128044128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5QjEey8OeH-",
        "colab_type": "text"
      },
      "source": [
        "### Training: 1. Reset the gradients 2. Forward pass 3. Calc loss 4. Backward pass 5. One step of the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALMlz0WjOeH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b21f15f-bea2-4c80-c7ff-1bbba21d1388"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    inputs = Variable(X)\n",
        "    labels = Variable(y)\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model.forward(inputs)\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, labels)\n",
        "    print(loss)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # for print loss vs. epoch\n",
        "    running_loss.append(loss.item())\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, round(loss.item(),5)))\n",
        "\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7748, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 0, loss 0.77483\n",
            "tensor(0.7472, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 1, loss 0.7472\n",
            "tensor(0.7210, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 2, loss 0.721\n",
            "tensor(0.6962, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 3, loss 0.69617\n",
            "tensor(0.6727, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 4, loss 0.67269\n",
            "tensor(0.6505, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 5, loss 0.65049\n",
            "tensor(0.6295, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 6, loss 0.62953\n",
            "tensor(0.6097, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 7, loss 0.60974\n",
            "tensor(0.5911, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 8, loss 0.59108\n",
            "tensor(0.5735, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 9, loss 0.57347\n",
            "tensor(0.5569, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 10, loss 0.55686\n",
            "tensor(0.5412, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 11, loss 0.54119\n",
            "tensor(0.5264, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 12, loss 0.52641\n",
            "tensor(0.5124, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 13, loss 0.51244\n",
            "tensor(0.4992, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 14, loss 0.49924\n",
            "tensor(0.4868, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 15, loss 0.48677\n",
            "tensor(0.4750, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 16, loss 0.47496\n",
            "tensor(0.4638, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 17, loss 0.46377\n",
            "tensor(0.4532, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 18, loss 0.45316\n",
            "tensor(0.4431, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 19, loss 0.44309\n",
            "tensor(0.4335, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 20, loss 0.43353\n",
            "tensor(0.4244, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 21, loss 0.42443\n",
            "tensor(0.4158, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 22, loss 0.41576\n",
            "tensor(0.4075, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 23, loss 0.4075\n",
            "tensor(0.3996, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 24, loss 0.39962\n",
            "tensor(0.3921, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 25, loss 0.39209\n",
            "tensor(0.3849, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 26, loss 0.38489\n",
            "tensor(0.3780, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 27, loss 0.378\n",
            "tensor(0.3714, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 28, loss 0.3714\n",
            "tensor(0.3651, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 29, loss 0.36507\n",
            "tensor(0.3590, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 30, loss 0.35899\n",
            "tensor(0.3531, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 31, loss 0.35315\n",
            "tensor(0.3475, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 32, loss 0.34753\n",
            "tensor(0.3421, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 33, loss 0.34212\n",
            "tensor(0.3369, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 34, loss 0.33691\n",
            "tensor(0.3319, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 35, loss 0.33189\n",
            "tensor(0.3270, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 36, loss 0.32705\n",
            "tensor(0.3224, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 37, loss 0.32237\n",
            "tensor(0.3179, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 38, loss 0.31785\n",
            "tensor(0.3135, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 39, loss 0.31348\n",
            "tensor(0.3093, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 40, loss 0.30925\n",
            "tensor(0.3052, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 41, loss 0.30516\n",
            "tensor(0.3012, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 42, loss 0.3012\n",
            "tensor(0.2974, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 43, loss 0.29736\n",
            "tensor(0.2936, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 44, loss 0.29363\n",
            "tensor(0.2900, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 45, loss 0.29001\n",
            "tensor(0.2865, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 46, loss 0.2865\n",
            "tensor(0.2831, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 47, loss 0.28309\n",
            "tensor(0.2798, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 48, loss 0.27978\n",
            "tensor(0.2766, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 49, loss 0.27656\n",
            "tensor(0.2734, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 50, loss 0.27343\n",
            "tensor(0.2704, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 51, loss 0.27038\n",
            "tensor(0.2674, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 52, loss 0.26741\n",
            "tensor(0.2645, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 53, loss 0.26452\n",
            "tensor(0.2617, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 54, loss 0.2617\n",
            "tensor(0.2590, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 55, loss 0.25895\n",
            "tensor(0.2563, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 56, loss 0.25627\n",
            "tensor(0.2537, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 57, loss 0.25366\n",
            "tensor(0.2511, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 58, loss 0.25111\n",
            "tensor(0.2486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 59, loss 0.24862\n",
            "tensor(0.2462, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 60, loss 0.24619\n",
            "tensor(0.2438, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 61, loss 0.24382\n",
            "tensor(0.2415, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 62, loss 0.2415\n",
            "tensor(0.2392, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 63, loss 0.23923\n",
            "tensor(0.2370, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 64, loss 0.23701\n",
            "tensor(0.2348, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 65, loss 0.23484\n",
            "tensor(0.2327, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 66, loss 0.23272\n",
            "tensor(0.2306, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 67, loss 0.23064\n",
            "tensor(0.2286, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 68, loss 0.22861\n",
            "tensor(0.2266, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 69, loss 0.22662\n",
            "tensor(0.2247, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 70, loss 0.22467\n",
            "tensor(0.2228, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 71, loss 0.22276\n",
            "tensor(0.2209, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 72, loss 0.22089\n",
            "tensor(0.2191, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 73, loss 0.21905\n",
            "tensor(0.2173, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 74, loss 0.21726\n",
            "tensor(0.2155, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 75, loss 0.21549\n",
            "tensor(0.2138, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 76, loss 0.21376\n",
            "tensor(0.2121, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 77, loss 0.21207\n",
            "tensor(0.2104, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 78, loss 0.2104\n",
            "tensor(0.2088, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 79, loss 0.20877\n",
            "tensor(0.2072, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 80, loss 0.20716\n",
            "tensor(0.2056, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 81, loss 0.20559\n",
            "tensor(0.2040, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 82, loss 0.20404\n",
            "tensor(0.2025, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 83, loss 0.20252\n",
            "tensor(0.2010, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 84, loss 0.20103\n",
            "tensor(0.1996, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 85, loss 0.19956\n",
            "tensor(0.1981, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 86, loss 0.19812\n",
            "tensor(0.1967, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 87, loss 0.19671\n",
            "tensor(0.1953, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 88, loss 0.19531\n",
            "tensor(0.1939, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 89, loss 0.19394\n",
            "tensor(0.1926, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 90, loss 0.1926\n",
            "tensor(0.1913, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 91, loss 0.19127\n",
            "tensor(0.1900, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 92, loss 0.18997\n",
            "tensor(0.1887, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 93, loss 0.18869\n",
            "tensor(0.1874, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 94, loss 0.18743\n",
            "tensor(0.1862, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 95, loss 0.18618\n",
            "tensor(0.1850, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 96, loss 0.18496\n",
            "tensor(0.1838, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 97, loss 0.18376\n",
            "tensor(0.1826, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 98, loss 0.18257\n",
            "tensor(0.1814, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 99, loss 0.1814\n",
            "tensor(0.1803, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 100, loss 0.18026\n",
            "tensor(0.1791, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 101, loss 0.17912\n",
            "tensor(0.1780, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 102, loss 0.17801\n",
            "tensor(0.1769, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 103, loss 0.17691\n",
            "tensor(0.1758, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 104, loss 0.17583\n",
            "tensor(0.1748, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 105, loss 0.17476\n",
            "tensor(0.1737, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 106, loss 0.17371\n",
            "tensor(0.1727, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 107, loss 0.17267\n",
            "tensor(0.1716, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 108, loss 0.17164\n",
            "tensor(0.1706, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 109, loss 0.17064\n",
            "tensor(0.1696, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 110, loss 0.16964\n",
            "tensor(0.1687, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 111, loss 0.16866\n",
            "tensor(0.1677, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 112, loss 0.16769\n",
            "tensor(0.1667, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 113, loss 0.16674\n",
            "tensor(0.1658, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 114, loss 0.1658\n",
            "tensor(0.1649, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 115, loss 0.16487\n",
            "tensor(0.1640, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 116, loss 0.16395\n",
            "tensor(0.1630, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 117, loss 0.16305\n",
            "tensor(0.1622, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 118, loss 0.16215\n",
            "tensor(0.1613, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 119, loss 0.16127\n",
            "tensor(0.1604, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 120, loss 0.1604\n",
            "tensor(0.1595, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 121, loss 0.15954\n",
            "tensor(0.1587, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 122, loss 0.15869\n",
            "tensor(0.1579, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 123, loss 0.15786\n",
            "tensor(0.1570, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 124, loss 0.15703\n",
            "tensor(0.1562, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 125, loss 0.15621\n",
            "tensor(0.1554, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 126, loss 0.1554\n",
            "tensor(0.1546, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 127, loss 0.15461\n",
            "tensor(0.1538, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 128, loss 0.15382\n",
            "tensor(0.1530, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 129, loss 0.15304\n",
            "tensor(0.1523, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 130, loss 0.15227\n",
            "tensor(0.1515, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 131, loss 0.15151\n",
            "tensor(0.1508, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 132, loss 0.15076\n",
            "tensor(0.1500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 133, loss 0.15002\n",
            "tensor(0.1493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 134, loss 0.14928\n",
            "tensor(0.1486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 135, loss 0.14856\n",
            "tensor(0.1478, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 136, loss 0.14784\n",
            "tensor(0.1471, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 137, loss 0.14713\n",
            "tensor(0.1464, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 138, loss 0.14643\n",
            "tensor(0.1457, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 139, loss 0.14574\n",
            "tensor(0.1451, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 140, loss 0.14505\n",
            "tensor(0.1444, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 141, loss 0.14438\n",
            "tensor(0.1437, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 142, loss 0.14371\n",
            "tensor(0.1430, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 143, loss 0.14304\n",
            "tensor(0.1424, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 144, loss 0.14239\n",
            "tensor(0.1417, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 145, loss 0.14174\n",
            "tensor(0.1411, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 146, loss 0.1411\n",
            "tensor(0.1405, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 147, loss 0.14046\n",
            "tensor(0.1398, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 148, loss 0.13983\n",
            "tensor(0.1392, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 149, loss 0.13921\n",
            "tensor(0.1386, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 150, loss 0.1386\n",
            "tensor(0.1380, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 151, loss 0.13799\n",
            "tensor(0.1374, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 152, loss 0.13739\n",
            "tensor(0.1368, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 153, loss 0.13679\n",
            "tensor(0.1362, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 154, loss 0.1362\n",
            "tensor(0.1356, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 155, loss 0.13562\n",
            "tensor(0.1350, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 156, loss 0.13504\n",
            "tensor(0.1345, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 157, loss 0.13447\n",
            "tensor(0.1339, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 158, loss 0.1339\n",
            "tensor(0.1333, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 159, loss 0.13334\n",
            "tensor(0.1328, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 160, loss 0.13278\n",
            "tensor(0.1322, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 161, loss 0.13223\n",
            "tensor(0.1317, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 162, loss 0.13169\n",
            "tensor(0.1311, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 163, loss 0.13115\n",
            "tensor(0.1306, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 164, loss 0.13062\n",
            "tensor(0.1301, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 165, loss 0.13009\n",
            "tensor(0.1296, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 166, loss 0.12956\n",
            "tensor(0.1290, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 167, loss 0.12904\n",
            "tensor(0.1285, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 168, loss 0.12853\n",
            "tensor(0.1280, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 169, loss 0.12802\n",
            "tensor(0.1275, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 170, loss 0.12752\n",
            "tensor(0.1270, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 171, loss 0.12702\n",
            "tensor(0.1265, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 172, loss 0.12652\n",
            "tensor(0.1260, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 173, loss 0.12603\n",
            "tensor(0.1255, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 174, loss 0.12554\n",
            "tensor(0.1251, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 175, loss 0.12506\n",
            "tensor(0.1246, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 176, loss 0.12458\n",
            "tensor(0.1241, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 177, loss 0.12411\n",
            "tensor(0.1236, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 178, loss 0.12364\n",
            "tensor(0.1232, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 179, loss 0.12318\n",
            "tensor(0.1227, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 180, loss 0.12272\n",
            "tensor(0.1223, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 181, loss 0.12226\n",
            "tensor(0.1218, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 182, loss 0.12181\n",
            "tensor(0.1214, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 183, loss 0.12136\n",
            "tensor(0.1209, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 184, loss 0.12091\n",
            "tensor(0.1205, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 185, loss 0.12047\n",
            "tensor(0.1200, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 186, loss 0.12004\n",
            "tensor(0.1196, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 187, loss 0.1196\n",
            "tensor(0.1192, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 188, loss 0.11917\n",
            "tensor(0.1187, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 189, loss 0.11875\n",
            "tensor(0.1183, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 190, loss 0.11833\n",
            "tensor(0.1179, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 191, loss 0.11791\n",
            "tensor(0.1175, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 192, loss 0.11749\n",
            "tensor(0.1171, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 193, loss 0.11708\n",
            "tensor(0.1167, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 194, loss 0.11667\n",
            "tensor(0.1163, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 195, loss 0.11627\n",
            "tensor(0.1159, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 196, loss 0.11586\n",
            "tensor(0.1155, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 197, loss 0.11547\n",
            "tensor(0.1151, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 198, loss 0.11507\n",
            "tensor(0.1147, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 199, loss 0.11468\n",
            "tensor(0.1143, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 200, loss 0.11429\n",
            "tensor(0.1139, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 201, loss 0.1139\n",
            "tensor(0.1135, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 202, loss 0.11352\n",
            "tensor(0.1131, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 203, loss 0.11314\n",
            "tensor(0.1128, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 204, loss 0.11277\n",
            "tensor(0.1124, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 205, loss 0.11239\n",
            "tensor(0.1120, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 206, loss 0.11202\n",
            "tensor(0.1117, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 207, loss 0.11165\n",
            "tensor(0.1113, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 208, loss 0.11129\n",
            "tensor(0.1109, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 209, loss 0.11093\n",
            "tensor(0.1106, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 210, loss 0.11057\n",
            "tensor(0.1102, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 211, loss 0.11021\n",
            "tensor(0.1099, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 212, loss 0.10986\n",
            "tensor(0.1095, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 213, loss 0.10951\n",
            "tensor(0.1092, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 214, loss 0.10916\n",
            "tensor(0.1088, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 215, loss 0.10881\n",
            "tensor(0.1085, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 216, loss 0.10847\n",
            "tensor(0.1081, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 217, loss 0.10813\n",
            "tensor(0.1078, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 218, loss 0.10779\n",
            "tensor(0.1075, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 219, loss 0.10745\n",
            "tensor(0.1071, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 220, loss 0.10712\n",
            "tensor(0.1068, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 221, loss 0.10679\n",
            "tensor(0.1065, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 222, loss 0.10646\n",
            "tensor(0.1061, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 223, loss 0.10614\n",
            "tensor(0.1058, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 224, loss 0.10581\n",
            "tensor(0.1055, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 225, loss 0.10549\n",
            "tensor(0.1052, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 226, loss 0.10517\n",
            "tensor(0.1049, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 227, loss 0.10486\n",
            "tensor(0.1045, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 228, loss 0.10454\n",
            "tensor(0.1042, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 229, loss 0.10423\n",
            "tensor(0.1039, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 230, loss 0.10392\n",
            "tensor(0.1036, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 231, loss 0.10361\n",
            "tensor(0.1033, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 232, loss 0.10331\n",
            "tensor(0.1030, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 233, loss 0.103\n",
            "tensor(0.1027, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 234, loss 0.1027\n",
            "tensor(0.1024, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 235, loss 0.1024\n",
            "tensor(0.1021, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 236, loss 0.10211\n",
            "tensor(0.1018, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 237, loss 0.10181\n",
            "tensor(0.1015, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 238, loss 0.10152\n",
            "tensor(0.1012, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 239, loss 0.10123\n",
            "tensor(0.1009, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 240, loss 0.10094\n",
            "tensor(0.1007, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 241, loss 0.10065\n",
            "tensor(0.1004, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 242, loss 0.10037\n",
            "tensor(0.1001, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 243, loss 0.10008\n",
            "tensor(0.0998, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 244, loss 0.0998\n",
            "tensor(0.0995, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 245, loss 0.09952\n",
            "tensor(0.0992, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 246, loss 0.09925\n",
            "tensor(0.0990, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 247, loss 0.09897\n",
            "tensor(0.0987, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 248, loss 0.0987\n",
            "tensor(0.0984, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 249, loss 0.09842\n",
            "tensor(0.0982, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 250, loss 0.09815\n",
            "tensor(0.0979, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 251, loss 0.09789\n",
            "tensor(0.0976, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 252, loss 0.09762\n",
            "tensor(0.0974, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 253, loss 0.09735\n",
            "tensor(0.0971, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 254, loss 0.09709\n",
            "tensor(0.0968, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 255, loss 0.09683\n",
            "tensor(0.0966, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 256, loss 0.09657\n",
            "tensor(0.0963, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 257, loss 0.09631\n",
            "tensor(0.0961, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 258, loss 0.09606\n",
            "tensor(0.0958, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 259, loss 0.0958\n",
            "tensor(0.0955, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 260, loss 0.09555\n",
            "tensor(0.0953, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 261, loss 0.0953\n",
            "tensor(0.0950, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 262, loss 0.09505\n",
            "tensor(0.0948, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 263, loss 0.0948\n",
            "tensor(0.0946, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 264, loss 0.09455\n",
            "tensor(0.0943, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 265, loss 0.09431\n",
            "tensor(0.0941, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 266, loss 0.09406\n",
            "tensor(0.0938, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 267, loss 0.09382\n",
            "tensor(0.0936, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 268, loss 0.09358\n",
            "tensor(0.0933, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 269, loss 0.09334\n",
            "tensor(0.0931, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 270, loss 0.0931\n",
            "tensor(0.0929, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 271, loss 0.09287\n",
            "tensor(0.0926, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 272, loss 0.09263\n",
            "tensor(0.0924, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 273, loss 0.0924\n",
            "tensor(0.0922, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 274, loss 0.09216\n",
            "tensor(0.0919, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 275, loss 0.09193\n",
            "tensor(0.0917, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 276, loss 0.0917\n",
            "tensor(0.0915, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 277, loss 0.09148\n",
            "tensor(0.0913, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 278, loss 0.09125\n",
            "tensor(0.0910, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 279, loss 0.09102\n",
            "tensor(0.0908, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 280, loss 0.0908\n",
            "tensor(0.0906, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 281, loss 0.09058\n",
            "tensor(0.0904, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 282, loss 0.09036\n",
            "tensor(0.0901, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 283, loss 0.09014\n",
            "tensor(0.0899, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 284, loss 0.08992\n",
            "tensor(0.0897, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 285, loss 0.0897\n",
            "tensor(0.0895, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 286, loss 0.08949\n",
            "tensor(0.0893, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 287, loss 0.08927\n",
            "tensor(0.0891, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 288, loss 0.08906\n",
            "tensor(0.0888, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 289, loss 0.08884\n",
            "tensor(0.0886, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 290, loss 0.08863\n",
            "tensor(0.0884, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 291, loss 0.08842\n",
            "tensor(0.0882, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 292, loss 0.08821\n",
            "tensor(0.0880, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 293, loss 0.08801\n",
            "tensor(0.0878, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 294, loss 0.0878\n",
            "tensor(0.0876, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 295, loss 0.0876\n",
            "tensor(0.0874, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 296, loss 0.08739\n",
            "tensor(0.0872, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 297, loss 0.08719\n",
            "tensor(0.0870, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 298, loss 0.08699\n",
            "tensor(0.0868, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 299, loss 0.08679\n",
            "tensor(0.0866, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 300, loss 0.08659\n",
            "tensor(0.0864, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 301, loss 0.08639\n",
            "tensor(0.0862, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 302, loss 0.08619\n",
            "tensor(0.0860, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 303, loss 0.086\n",
            "tensor(0.0858, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 304, loss 0.0858\n",
            "tensor(0.0856, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 305, loss 0.08561\n",
            "tensor(0.0854, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 306, loss 0.08541\n",
            "tensor(0.0852, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 307, loss 0.08522\n",
            "tensor(0.0850, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 308, loss 0.08503\n",
            "tensor(0.0848, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 309, loss 0.08484\n",
            "tensor(0.0847, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 310, loss 0.08465\n",
            "tensor(0.0845, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 311, loss 0.08446\n",
            "tensor(0.0843, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 312, loss 0.08428\n",
            "tensor(0.0841, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 313, loss 0.08409\n",
            "tensor(0.0839, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 314, loss 0.08391\n",
            "tensor(0.0837, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 315, loss 0.08372\n",
            "tensor(0.0835, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 316, loss 0.08354\n",
            "tensor(0.0834, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 317, loss 0.08336\n",
            "tensor(0.0832, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 318, loss 0.08318\n",
            "tensor(0.0830, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 319, loss 0.083\n",
            "tensor(0.0828, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 320, loss 0.08282\n",
            "tensor(0.0826, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 321, loss 0.08264\n",
            "tensor(0.0825, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 322, loss 0.08246\n",
            "tensor(0.0823, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 323, loss 0.08229\n",
            "tensor(0.0821, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 324, loss 0.08211\n",
            "tensor(0.0819, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 325, loss 0.08194\n",
            "tensor(0.0818, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 326, loss 0.08177\n",
            "tensor(0.0816, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 327, loss 0.08159\n",
            "tensor(0.0814, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 328, loss 0.08142\n",
            "tensor(0.0813, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 329, loss 0.08125\n",
            "tensor(0.0811, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 330, loss 0.08108\n",
            "tensor(0.0809, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 331, loss 0.08091\n",
            "tensor(0.0807, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 332, loss 0.08074\n",
            "tensor(0.0806, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 333, loss 0.08058\n",
            "tensor(0.0804, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 334, loss 0.08041\n",
            "tensor(0.0802, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 335, loss 0.08024\n",
            "tensor(0.0801, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 336, loss 0.08008\n",
            "tensor(0.0799, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 337, loss 0.07992\n",
            "tensor(0.0798, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 338, loss 0.07975\n",
            "tensor(0.0796, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 339, loss 0.07959\n",
            "tensor(0.0794, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 340, loss 0.07943\n",
            "tensor(0.0793, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 341, loss 0.07927\n",
            "tensor(0.0791, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 342, loss 0.07911\n",
            "tensor(0.0789, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 343, loss 0.07895\n",
            "tensor(0.0788, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 344, loss 0.07879\n",
            "tensor(0.0786, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 345, loss 0.07863\n",
            "tensor(0.0785, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 346, loss 0.07848\n",
            "tensor(0.0783, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 347, loss 0.07832\n",
            "tensor(0.0782, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 348, loss 0.07817\n",
            "tensor(0.0780, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 349, loss 0.07801\n",
            "tensor(0.0779, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 350, loss 0.07786\n",
            "tensor(0.0777, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 351, loss 0.0777\n",
            "tensor(0.0776, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 352, loss 0.07755\n",
            "tensor(0.0774, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 353, loss 0.0774\n",
            "tensor(0.0772, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 354, loss 0.07725\n",
            "tensor(0.0771, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 355, loss 0.0771\n",
            "tensor(0.0770, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 356, loss 0.07695\n",
            "tensor(0.0768, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 357, loss 0.0768\n",
            "tensor(0.0767, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 358, loss 0.07665\n",
            "tensor(0.0765, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 359, loss 0.07651\n",
            "tensor(0.0764, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 360, loss 0.07636\n",
            "tensor(0.0762, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 361, loss 0.07621\n",
            "tensor(0.0761, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 362, loss 0.07607\n",
            "tensor(0.0759, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 363, loss 0.07592\n",
            "tensor(0.0758, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 364, loss 0.07578\n",
            "tensor(0.0756, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 365, loss 0.07564\n",
            "tensor(0.0755, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 366, loss 0.0755\n",
            "tensor(0.0754, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 367, loss 0.07535\n",
            "tensor(0.0752, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 368, loss 0.07521\n",
            "tensor(0.0751, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 369, loss 0.07507\n",
            "tensor(0.0749, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 370, loss 0.07493\n",
            "tensor(0.0748, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 371, loss 0.07479\n",
            "tensor(0.0747, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 372, loss 0.07465\n",
            "tensor(0.0745, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 373, loss 0.07452\n",
            "tensor(0.0744, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 374, loss 0.07438\n",
            "tensor(0.0742, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 375, loss 0.07424\n",
            "tensor(0.0741, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 376, loss 0.07411\n",
            "tensor(0.0740, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 377, loss 0.07397\n",
            "tensor(0.0738, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 378, loss 0.07384\n",
            "tensor(0.0737, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 379, loss 0.0737\n",
            "tensor(0.0736, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 380, loss 0.07357\n",
            "tensor(0.0734, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 381, loss 0.07344\n",
            "tensor(0.0733, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 382, loss 0.0733\n",
            "tensor(0.0732, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 383, loss 0.07317\n",
            "tensor(0.0730, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 384, loss 0.07304\n",
            "tensor(0.0729, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 385, loss 0.07291\n",
            "tensor(0.0728, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 386, loss 0.07278\n",
            "tensor(0.0727, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 387, loss 0.07265\n",
            "tensor(0.0725, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 388, loss 0.07252\n",
            "tensor(0.0724, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 389, loss 0.07239\n",
            "tensor(0.0723, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 390, loss 0.07227\n",
            "tensor(0.0721, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 391, loss 0.07214\n",
            "tensor(0.0720, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 392, loss 0.07201\n",
            "tensor(0.0719, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 393, loss 0.07189\n",
            "tensor(0.0718, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 394, loss 0.07176\n",
            "tensor(0.0716, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 395, loss 0.07164\n",
            "tensor(0.0715, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 396, loss 0.07151\n",
            "tensor(0.0714, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 397, loss 0.07139\n",
            "tensor(0.0713, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 398, loss 0.07126\n",
            "tensor(0.0711, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 399, loss 0.07114\n",
            "tensor(0.0710, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 400, loss 0.07102\n",
            "tensor(0.0709, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 401, loss 0.0709\n",
            "tensor(0.0708, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 402, loss 0.07078\n",
            "tensor(0.0707, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 403, loss 0.07065\n",
            "tensor(0.0705, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 404, loss 0.07053\n",
            "tensor(0.0704, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 405, loss 0.07041\n",
            "tensor(0.0703, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 406, loss 0.0703\n",
            "tensor(0.0702, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 407, loss 0.07018\n",
            "tensor(0.0701, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 408, loss 0.07006\n",
            "tensor(0.0699, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 409, loss 0.06994\n",
            "tensor(0.0698, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 410, loss 0.06982\n",
            "tensor(0.0697, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 411, loss 0.06971\n",
            "tensor(0.0696, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 412, loss 0.06959\n",
            "tensor(0.0695, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 413, loss 0.06947\n",
            "tensor(0.0694, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 414, loss 0.06936\n",
            "tensor(0.0692, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 415, loss 0.06924\n",
            "tensor(0.0691, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 416, loss 0.06913\n",
            "tensor(0.0690, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 417, loss 0.06902\n",
            "tensor(0.0689, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 418, loss 0.0689\n",
            "tensor(0.0688, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 419, loss 0.06879\n",
            "tensor(0.0687, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 420, loss 0.06868\n",
            "tensor(0.0686, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 421, loss 0.06857\n",
            "tensor(0.0685, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 422, loss 0.06845\n",
            "tensor(0.0683, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 423, loss 0.06834\n",
            "tensor(0.0682, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 424, loss 0.06823\n",
            "tensor(0.0681, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 425, loss 0.06812\n",
            "tensor(0.0680, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 426, loss 0.06801\n",
            "tensor(0.0679, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 427, loss 0.0679\n",
            "tensor(0.0678, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 428, loss 0.06779\n",
            "tensor(0.0677, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 429, loss 0.06769\n",
            "tensor(0.0676, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 430, loss 0.06758\n",
            "tensor(0.0675, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 431, loss 0.06747\n",
            "tensor(0.0674, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 432, loss 0.06736\n",
            "tensor(0.0673, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 433, loss 0.06726\n",
            "tensor(0.0672, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 434, loss 0.06715\n",
            "tensor(0.0670, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 435, loss 0.06704\n",
            "tensor(0.0669, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 436, loss 0.06694\n",
            "tensor(0.0668, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 437, loss 0.06683\n",
            "tensor(0.0667, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 438, loss 0.06673\n",
            "tensor(0.0666, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 439, loss 0.06663\n",
            "tensor(0.0665, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 440, loss 0.06652\n",
            "tensor(0.0664, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 441, loss 0.06642\n",
            "tensor(0.0663, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 442, loss 0.06632\n",
            "tensor(0.0662, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 443, loss 0.06621\n",
            "tensor(0.0661, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 444, loss 0.06611\n",
            "tensor(0.0660, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 445, loss 0.06601\n",
            "tensor(0.0659, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 446, loss 0.06591\n",
            "tensor(0.0658, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 447, loss 0.06581\n",
            "tensor(0.0657, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 448, loss 0.06571\n",
            "tensor(0.0656, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 449, loss 0.06561\n",
            "tensor(0.0655, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 450, loss 0.06551\n",
            "tensor(0.0654, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 451, loss 0.06541\n",
            "tensor(0.0653, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 452, loss 0.06531\n",
            "tensor(0.0652, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 453, loss 0.06521\n",
            "tensor(0.0651, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 454, loss 0.06511\n",
            "tensor(0.0650, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 455, loss 0.06501\n",
            "tensor(0.0649, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 456, loss 0.06492\n",
            "tensor(0.0648, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 457, loss 0.06482\n",
            "tensor(0.0647, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 458, loss 0.06472\n",
            "tensor(0.0646, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 459, loss 0.06463\n",
            "tensor(0.0645, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 460, loss 0.06453\n",
            "tensor(0.0644, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 461, loss 0.06443\n",
            "tensor(0.0643, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 462, loss 0.06434\n",
            "tensor(0.0642, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 463, loss 0.06424\n",
            "tensor(0.0641, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 464, loss 0.06415\n",
            "tensor(0.0641, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 465, loss 0.06405\n",
            "tensor(0.0640, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 466, loss 0.06396\n",
            "tensor(0.0639, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 467, loss 0.06387\n",
            "tensor(0.0638, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 468, loss 0.06377\n",
            "tensor(0.0637, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 469, loss 0.06368\n",
            "tensor(0.0636, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 470, loss 0.06359\n",
            "tensor(0.0635, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 471, loss 0.0635\n",
            "tensor(0.0634, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 472, loss 0.0634\n",
            "tensor(0.0633, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 473, loss 0.06331\n",
            "tensor(0.0632, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 474, loss 0.06322\n",
            "tensor(0.0631, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 475, loss 0.06313\n",
            "tensor(0.0630, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 476, loss 0.06304\n",
            "tensor(0.0630, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 477, loss 0.06295\n",
            "tensor(0.0629, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 478, loss 0.06286\n",
            "tensor(0.0628, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 479, loss 0.06277\n",
            "tensor(0.0627, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 480, loss 0.06268\n",
            "tensor(0.0626, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 481, loss 0.06259\n",
            "tensor(0.0625, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 482, loss 0.0625\n",
            "tensor(0.0624, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 483, loss 0.06242\n",
            "tensor(0.0623, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 484, loss 0.06233\n",
            "tensor(0.0622, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 485, loss 0.06224\n",
            "tensor(0.0622, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 486, loss 0.06215\n",
            "tensor(0.0621, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 487, loss 0.06207\n",
            "tensor(0.0620, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 488, loss 0.06198\n",
            "tensor(0.0619, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 489, loss 0.06189\n",
            "tensor(0.0618, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 490, loss 0.06181\n",
            "tensor(0.0617, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 491, loss 0.06172\n",
            "tensor(0.0616, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 492, loss 0.06164\n",
            "tensor(0.0616, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 493, loss 0.06155\n",
            "tensor(0.0615, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 494, loss 0.06147\n",
            "tensor(0.0614, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 495, loss 0.06138\n",
            "tensor(0.0613, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 496, loss 0.0613\n",
            "tensor(0.0612, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 497, loss 0.06121\n",
            "tensor(0.0611, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 498, loss 0.06113\n",
            "tensor(0.0610, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 499, loss 0.06105\n",
            "tensor(0.0610, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 500, loss 0.06096\n",
            "tensor(0.0609, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 501, loss 0.06088\n",
            "tensor(0.0608, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 502, loss 0.0608\n",
            "tensor(0.0607, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 503, loss 0.06072\n",
            "tensor(0.0606, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 504, loss 0.06063\n",
            "tensor(0.0606, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 505, loss 0.06055\n",
            "tensor(0.0605, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 506, loss 0.06047\n",
            "tensor(0.0604, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 507, loss 0.06039\n",
            "tensor(0.0603, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 508, loss 0.06031\n",
            "tensor(0.0602, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 509, loss 0.06023\n",
            "tensor(0.0601, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 510, loss 0.06015\n",
            "tensor(0.0601, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 511, loss 0.06007\n",
            "tensor(0.0600, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 512, loss 0.05999\n",
            "tensor(0.0599, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 513, loss 0.05991\n",
            "tensor(0.0598, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 514, loss 0.05983\n",
            "tensor(0.0598, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 515, loss 0.05975\n",
            "tensor(0.0597, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 516, loss 0.05967\n",
            "tensor(0.0596, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 517, loss 0.0596\n",
            "tensor(0.0595, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 518, loss 0.05952\n",
            "tensor(0.0594, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 519, loss 0.05944\n",
            "tensor(0.0594, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 520, loss 0.05936\n",
            "tensor(0.0593, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 521, loss 0.05928\n",
            "tensor(0.0592, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 522, loss 0.05921\n",
            "tensor(0.0591, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 523, loss 0.05913\n",
            "tensor(0.0591, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 524, loss 0.05905\n",
            "tensor(0.0590, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 525, loss 0.05898\n",
            "tensor(0.0589, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 526, loss 0.0589\n",
            "tensor(0.0588, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 527, loss 0.05883\n",
            "tensor(0.0588, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 528, loss 0.05875\n",
            "tensor(0.0587, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 529, loss 0.05868\n",
            "tensor(0.0586, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 530, loss 0.0586\n",
            "tensor(0.0585, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 531, loss 0.05853\n",
            "tensor(0.0585, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 532, loss 0.05845\n",
            "tensor(0.0584, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 533, loss 0.05838\n",
            "tensor(0.0583, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 534, loss 0.0583\n",
            "tensor(0.0582, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 535, loss 0.05823\n",
            "tensor(0.0582, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 536, loss 0.05816\n",
            "tensor(0.0581, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 537, loss 0.05808\n",
            "tensor(0.0580, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 538, loss 0.05801\n",
            "tensor(0.0579, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 539, loss 0.05794\n",
            "tensor(0.0579, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 540, loss 0.05786\n",
            "tensor(0.0578, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 541, loss 0.05779\n",
            "tensor(0.0577, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 542, loss 0.05772\n",
            "tensor(0.0576, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 543, loss 0.05765\n",
            "tensor(0.0576, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 544, loss 0.05758\n",
            "tensor(0.0575, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 545, loss 0.0575\n",
            "tensor(0.0574, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 546, loss 0.05743\n",
            "tensor(0.0574, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 547, loss 0.05736\n",
            "tensor(0.0573, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 548, loss 0.05729\n",
            "tensor(0.0572, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 549, loss 0.05722\n",
            "tensor(0.0572, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 550, loss 0.05715\n",
            "tensor(0.0571, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 551, loss 0.05708\n",
            "tensor(0.0570, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 552, loss 0.05701\n",
            "tensor(0.0569, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 553, loss 0.05694\n",
            "tensor(0.0569, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 554, loss 0.05687\n",
            "tensor(0.0568, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 555, loss 0.0568\n",
            "tensor(0.0567, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 556, loss 0.05673\n",
            "tensor(0.0567, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 557, loss 0.05667\n",
            "tensor(0.0566, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 558, loss 0.0566\n",
            "tensor(0.0565, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 559, loss 0.05653\n",
            "tensor(0.0565, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 560, loss 0.05646\n",
            "tensor(0.0564, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 561, loss 0.05639\n",
            "tensor(0.0563, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 562, loss 0.05632\n",
            "tensor(0.0563, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 563, loss 0.05626\n",
            "tensor(0.0562, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 564, loss 0.05619\n",
            "tensor(0.0561, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 565, loss 0.05612\n",
            "tensor(0.0561, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 566, loss 0.05606\n",
            "tensor(0.0560, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 567, loss 0.05599\n",
            "tensor(0.0559, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 568, loss 0.05592\n",
            "tensor(0.0559, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 569, loss 0.05586\n",
            "tensor(0.0558, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 570, loss 0.05579\n",
            "tensor(0.0557, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 571, loss 0.05573\n",
            "tensor(0.0557, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 572, loss 0.05566\n",
            "tensor(0.0556, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 573, loss 0.05559\n",
            "tensor(0.0555, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 574, loss 0.05553\n",
            "tensor(0.0555, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 575, loss 0.05546\n",
            "tensor(0.0554, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 576, loss 0.0554\n",
            "tensor(0.0553, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 577, loss 0.05533\n",
            "tensor(0.0553, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 578, loss 0.05527\n",
            "tensor(0.0552, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 579, loss 0.05521\n",
            "tensor(0.0551, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 580, loss 0.05514\n",
            "tensor(0.0551, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 581, loss 0.05508\n",
            "tensor(0.0550, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 582, loss 0.05501\n",
            "tensor(0.0550, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 583, loss 0.05495\n",
            "tensor(0.0549, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 584, loss 0.05489\n",
            "tensor(0.0548, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 585, loss 0.05483\n",
            "tensor(0.0548, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 586, loss 0.05476\n",
            "tensor(0.0547, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 587, loss 0.0547\n",
            "tensor(0.0546, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 588, loss 0.05464\n",
            "tensor(0.0546, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 589, loss 0.05457\n",
            "tensor(0.0545, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 590, loss 0.05451\n",
            "tensor(0.0545, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 591, loss 0.05445\n",
            "tensor(0.0544, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 592, loss 0.05439\n",
            "tensor(0.0543, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 593, loss 0.05433\n",
            "tensor(0.0543, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 594, loss 0.05427\n",
            "tensor(0.0542, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 595, loss 0.05421\n",
            "tensor(0.0541, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 596, loss 0.05414\n",
            "tensor(0.0541, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 597, loss 0.05408\n",
            "tensor(0.0540, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 598, loss 0.05402\n",
            "tensor(0.0540, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 599, loss 0.05396\n",
            "tensor(0.0539, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 600, loss 0.0539\n",
            "tensor(0.0538, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 601, loss 0.05384\n",
            "tensor(0.0538, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 602, loss 0.05378\n",
            "tensor(0.0537, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 603, loss 0.05372\n",
            "tensor(0.0537, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 604, loss 0.05366\n",
            "tensor(0.0536, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 605, loss 0.0536\n",
            "tensor(0.0535, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 606, loss 0.05354\n",
            "tensor(0.0535, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 607, loss 0.05348\n",
            "tensor(0.0534, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 608, loss 0.05343\n",
            "tensor(0.0534, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 609, loss 0.05337\n",
            "tensor(0.0533, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 610, loss 0.05331\n",
            "tensor(0.0533, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 611, loss 0.05325\n",
            "tensor(0.0532, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 612, loss 0.05319\n",
            "tensor(0.0531, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 613, loss 0.05313\n",
            "tensor(0.0531, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 614, loss 0.05308\n",
            "tensor(0.0530, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 615, loss 0.05302\n",
            "tensor(0.0530, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 616, loss 0.05296\n",
            "tensor(0.0529, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 617, loss 0.0529\n",
            "tensor(0.0528, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 618, loss 0.05285\n",
            "tensor(0.0528, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 619, loss 0.05279\n",
            "tensor(0.0527, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 620, loss 0.05273\n",
            "tensor(0.0527, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 621, loss 0.05268\n",
            "tensor(0.0526, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 622, loss 0.05262\n",
            "tensor(0.0526, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 623, loss 0.05256\n",
            "tensor(0.0525, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 624, loss 0.05251\n",
            "tensor(0.0525, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 625, loss 0.05245\n",
            "tensor(0.0524, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 626, loss 0.05239\n",
            "tensor(0.0523, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 627, loss 0.05234\n",
            "tensor(0.0523, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 628, loss 0.05228\n",
            "tensor(0.0522, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 629, loss 0.05223\n",
            "tensor(0.0522, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 630, loss 0.05217\n",
            "tensor(0.0521, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 631, loss 0.05212\n",
            "tensor(0.0521, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 632, loss 0.05206\n",
            "tensor(0.0520, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 633, loss 0.05201\n",
            "tensor(0.0520, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 634, loss 0.05195\n",
            "tensor(0.0519, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 635, loss 0.0519\n",
            "tensor(0.0518, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 636, loss 0.05184\n",
            "tensor(0.0518, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 637, loss 0.05179\n",
            "tensor(0.0517, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 638, loss 0.05173\n",
            "tensor(0.0517, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 639, loss 0.05168\n",
            "tensor(0.0516, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 640, loss 0.05163\n",
            "tensor(0.0516, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 641, loss 0.05157\n",
            "tensor(0.0515, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 642, loss 0.05152\n",
            "tensor(0.0515, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 643, loss 0.05147\n",
            "tensor(0.0514, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 644, loss 0.05141\n",
            "tensor(0.0514, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 645, loss 0.05136\n",
            "tensor(0.0513, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 646, loss 0.05131\n",
            "tensor(0.0513, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 647, loss 0.05125\n",
            "tensor(0.0512, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 648, loss 0.0512\n",
            "tensor(0.0511, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 649, loss 0.05115\n",
            "tensor(0.0511, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 650, loss 0.0511\n",
            "tensor(0.0510, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 651, loss 0.05105\n",
            "tensor(0.0510, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 652, loss 0.05099\n",
            "tensor(0.0509, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 653, loss 0.05094\n",
            "tensor(0.0509, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 654, loss 0.05089\n",
            "tensor(0.0508, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 655, loss 0.05084\n",
            "tensor(0.0508, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 656, loss 0.05079\n",
            "tensor(0.0507, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 657, loss 0.05074\n",
            "tensor(0.0507, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 658, loss 0.05068\n",
            "tensor(0.0506, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 659, loss 0.05063\n",
            "tensor(0.0506, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 660, loss 0.05058\n",
            "tensor(0.0505, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 661, loss 0.05053\n",
            "tensor(0.0505, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 662, loss 0.05048\n",
            "tensor(0.0504, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 663, loss 0.05043\n",
            "tensor(0.0504, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 664, loss 0.05038\n",
            "tensor(0.0503, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 665, loss 0.05033\n",
            "tensor(0.0503, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 666, loss 0.05028\n",
            "tensor(0.0502, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 667, loss 0.05023\n",
            "tensor(0.0502, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 668, loss 0.05018\n",
            "tensor(0.0501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 669, loss 0.05013\n",
            "tensor(0.0501, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 670, loss 0.05008\n",
            "tensor(0.0500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 671, loss 0.05003\n",
            "tensor(0.0500, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 672, loss 0.04998\n",
            "tensor(0.0499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 673, loss 0.04993\n",
            "tensor(0.0499, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 674, loss 0.04988\n",
            "tensor(0.0498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 675, loss 0.04983\n",
            "tensor(0.0498, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 676, loss 0.04978\n",
            "tensor(0.0497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 677, loss 0.04974\n",
            "tensor(0.0497, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 678, loss 0.04969\n",
            "tensor(0.0496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 679, loss 0.04964\n",
            "tensor(0.0496, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 680, loss 0.04959\n",
            "tensor(0.0495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 681, loss 0.04954\n",
            "tensor(0.0495, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 682, loss 0.04949\n",
            "tensor(0.0494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 683, loss 0.04945\n",
            "tensor(0.0494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 684, loss 0.0494\n",
            "tensor(0.0494, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 685, loss 0.04935\n",
            "tensor(0.0493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 686, loss 0.0493\n",
            "tensor(0.0493, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 687, loss 0.04926\n",
            "tensor(0.0492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 688, loss 0.04921\n",
            "tensor(0.0492, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 689, loss 0.04916\n",
            "tensor(0.0491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 690, loss 0.04911\n",
            "tensor(0.0491, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 691, loss 0.04907\n",
            "tensor(0.0490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 692, loss 0.04902\n",
            "tensor(0.0490, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 693, loss 0.04897\n",
            "tensor(0.0489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 694, loss 0.04893\n",
            "tensor(0.0489, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 695, loss 0.04888\n",
            "tensor(0.0488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 696, loss 0.04883\n",
            "tensor(0.0488, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 697, loss 0.04879\n",
            "tensor(0.0487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 698, loss 0.04874\n",
            "tensor(0.0487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 699, loss 0.0487\n",
            "tensor(0.0487, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 700, loss 0.04865\n",
            "tensor(0.0486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 701, loss 0.0486\n",
            "tensor(0.0486, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 702, loss 0.04856\n",
            "tensor(0.0485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 703, loss 0.04851\n",
            "tensor(0.0485, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 704, loss 0.04847\n",
            "tensor(0.0484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 705, loss 0.04842\n",
            "tensor(0.0484, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 706, loss 0.04838\n",
            "tensor(0.0483, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 707, loss 0.04833\n",
            "tensor(0.0483, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 708, loss 0.04829\n",
            "tensor(0.0482, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 709, loss 0.04824\n",
            "tensor(0.0482, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 710, loss 0.0482\n",
            "tensor(0.0482, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 711, loss 0.04815\n",
            "tensor(0.0481, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 712, loss 0.04811\n",
            "tensor(0.0481, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 713, loss 0.04806\n",
            "tensor(0.0480, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 714, loss 0.04802\n",
            "tensor(0.0480, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 715, loss 0.04798\n",
            "tensor(0.0479, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 716, loss 0.04793\n",
            "tensor(0.0479, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 717, loss 0.04789\n",
            "tensor(0.0478, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 718, loss 0.04784\n",
            "tensor(0.0478, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 719, loss 0.0478\n",
            "tensor(0.0478, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 720, loss 0.04776\n",
            "tensor(0.0477, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 721, loss 0.04771\n",
            "tensor(0.0477, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 722, loss 0.04767\n",
            "tensor(0.0476, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 723, loss 0.04763\n",
            "tensor(0.0476, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 724, loss 0.04758\n",
            "tensor(0.0475, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 725, loss 0.04754\n",
            "tensor(0.0475, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 726, loss 0.0475\n",
            "tensor(0.0475, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 727, loss 0.04745\n",
            "tensor(0.0474, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 728, loss 0.04741\n",
            "tensor(0.0474, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 729, loss 0.04737\n",
            "tensor(0.0473, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 730, loss 0.04733\n",
            "tensor(0.0473, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 731, loss 0.04728\n",
            "tensor(0.0472, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 732, loss 0.04724\n",
            "tensor(0.0472, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 733, loss 0.0472\n",
            "tensor(0.0472, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 734, loss 0.04716\n",
            "tensor(0.0471, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 735, loss 0.04711\n",
            "tensor(0.0471, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 736, loss 0.04707\n",
            "tensor(0.0470, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 737, loss 0.04703\n",
            "tensor(0.0470, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 738, loss 0.04699\n",
            "tensor(0.0469, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 739, loss 0.04695\n",
            "tensor(0.0469, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 740, loss 0.04691\n",
            "tensor(0.0469, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 741, loss 0.04686\n",
            "tensor(0.0468, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 742, loss 0.04682\n",
            "tensor(0.0468, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 743, loss 0.04678\n",
            "tensor(0.0467, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 744, loss 0.04674\n",
            "tensor(0.0467, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 745, loss 0.0467\n",
            "tensor(0.0467, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 746, loss 0.04666\n",
            "tensor(0.0466, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 747, loss 0.04662\n",
            "tensor(0.0466, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 748, loss 0.04658\n",
            "tensor(0.0465, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 749, loss 0.04654\n",
            "tensor(0.0465, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 750, loss 0.04649\n",
            "tensor(0.0465, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 751, loss 0.04645\n",
            "tensor(0.0464, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 752, loss 0.04641\n",
            "tensor(0.0464, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 753, loss 0.04637\n",
            "tensor(0.0463, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 754, loss 0.04633\n",
            "tensor(0.0463, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 755, loss 0.04629\n",
            "tensor(0.0463, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 756, loss 0.04625\n",
            "tensor(0.0462, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 757, loss 0.04621\n",
            "tensor(0.0462, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 758, loss 0.04617\n",
            "tensor(0.0461, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 759, loss 0.04613\n",
            "tensor(0.0461, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 760, loss 0.04609\n",
            "tensor(0.0461, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 761, loss 0.04605\n",
            "tensor(0.0460, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 762, loss 0.04601\n",
            "tensor(0.0460, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 763, loss 0.04598\n",
            "tensor(0.0459, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 764, loss 0.04594\n",
            "tensor(0.0459, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 765, loss 0.0459\n",
            "tensor(0.0459, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 766, loss 0.04586\n",
            "tensor(0.0458, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 767, loss 0.04582\n",
            "tensor(0.0458, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 768, loss 0.04578\n",
            "tensor(0.0457, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 769, loss 0.04574\n",
            "tensor(0.0457, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 770, loss 0.0457\n",
            "tensor(0.0457, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 771, loss 0.04566\n",
            "tensor(0.0456, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 772, loss 0.04562\n",
            "tensor(0.0456, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 773, loss 0.04559\n",
            "tensor(0.0455, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 774, loss 0.04555\n",
            "tensor(0.0455, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 775, loss 0.04551\n",
            "tensor(0.0455, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 776, loss 0.04547\n",
            "tensor(0.0454, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 777, loss 0.04543\n",
            "tensor(0.0454, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 778, loss 0.0454\n",
            "tensor(0.0454, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 779, loss 0.04536\n",
            "tensor(0.0453, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 780, loss 0.04532\n",
            "tensor(0.0453, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 781, loss 0.04528\n",
            "tensor(0.0452, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 782, loss 0.04524\n",
            "tensor(0.0452, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 783, loss 0.04521\n",
            "tensor(0.0452, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 784, loss 0.04517\n",
            "tensor(0.0451, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 785, loss 0.04513\n",
            "tensor(0.0451, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 786, loss 0.04509\n",
            "tensor(0.0451, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 787, loss 0.04506\n",
            "tensor(0.0450, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 788, loss 0.04502\n",
            "tensor(0.0450, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 789, loss 0.04498\n",
            "tensor(0.0449, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 790, loss 0.04495\n",
            "tensor(0.0449, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 791, loss 0.04491\n",
            "tensor(0.0449, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 792, loss 0.04487\n",
            "tensor(0.0448, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 793, loss 0.04483\n",
            "tensor(0.0448, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 794, loss 0.0448\n",
            "tensor(0.0448, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 795, loss 0.04476\n",
            "tensor(0.0447, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 796, loss 0.04473\n",
            "tensor(0.0447, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 797, loss 0.04469\n",
            "tensor(0.0447, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 798, loss 0.04465\n",
            "tensor(0.0446, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 799, loss 0.04462\n",
            "tensor(0.0446, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 800, loss 0.04458\n",
            "tensor(0.0445, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 801, loss 0.04454\n",
            "tensor(0.0445, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 802, loss 0.04451\n",
            "tensor(0.0445, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 803, loss 0.04447\n",
            "tensor(0.0444, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 804, loss 0.04444\n",
            "tensor(0.0444, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 805, loss 0.0444\n",
            "tensor(0.0444, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 806, loss 0.04436\n",
            "tensor(0.0443, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 807, loss 0.04433\n",
            "tensor(0.0443, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 808, loss 0.04429\n",
            "tensor(0.0443, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 809, loss 0.04426\n",
            "tensor(0.0442, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 810, loss 0.04422\n",
            "tensor(0.0442, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 811, loss 0.04419\n",
            "tensor(0.0442, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 812, loss 0.04415\n",
            "tensor(0.0441, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 813, loss 0.04412\n",
            "tensor(0.0441, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 814, loss 0.04408\n",
            "tensor(0.0440, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 815, loss 0.04405\n",
            "tensor(0.0440, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 816, loss 0.04401\n",
            "tensor(0.0440, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 817, loss 0.04398\n",
            "tensor(0.0439, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 818, loss 0.04394\n",
            "tensor(0.0439, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 819, loss 0.04391\n",
            "tensor(0.0439, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 820, loss 0.04387\n",
            "tensor(0.0438, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 821, loss 0.04384\n",
            "tensor(0.0438, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 822, loss 0.0438\n",
            "tensor(0.0438, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 823, loss 0.04377\n",
            "tensor(0.0437, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 824, loss 0.04373\n",
            "tensor(0.0437, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 825, loss 0.0437\n",
            "tensor(0.0437, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 826, loss 0.04367\n",
            "tensor(0.0436, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 827, loss 0.04363\n",
            "tensor(0.0436, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 828, loss 0.0436\n",
            "tensor(0.0436, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 829, loss 0.04356\n",
            "tensor(0.0435, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 830, loss 0.04353\n",
            "tensor(0.0435, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 831, loss 0.0435\n",
            "tensor(0.0435, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 832, loss 0.04346\n",
            "tensor(0.0434, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 833, loss 0.04343\n",
            "tensor(0.0434, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 834, loss 0.04339\n",
            "tensor(0.0434, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 835, loss 0.04336\n",
            "tensor(0.0433, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 836, loss 0.04333\n",
            "tensor(0.0433, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 837, loss 0.04329\n",
            "tensor(0.0433, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 838, loss 0.04326\n",
            "tensor(0.0432, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 839, loss 0.04323\n",
            "tensor(0.0432, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 840, loss 0.04319\n",
            "tensor(0.0432, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 841, loss 0.04316\n",
            "tensor(0.0431, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 842, loss 0.04313\n",
            "tensor(0.0431, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 843, loss 0.0431\n",
            "tensor(0.0431, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 844, loss 0.04306\n",
            "tensor(0.0430, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 845, loss 0.04303\n",
            "tensor(0.0430, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 846, loss 0.043\n",
            "tensor(0.0430, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 847, loss 0.04296\n",
            "tensor(0.0429, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 848, loss 0.04293\n",
            "tensor(0.0429, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 849, loss 0.0429\n",
            "tensor(0.0429, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 850, loss 0.04287\n",
            "tensor(0.0428, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 851, loss 0.04283\n",
            "tensor(0.0428, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 852, loss 0.0428\n",
            "tensor(0.0428, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 853, loss 0.04277\n",
            "tensor(0.0427, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 854, loss 0.04274\n",
            "tensor(0.0427, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 855, loss 0.04271\n",
            "tensor(0.0427, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 856, loss 0.04267\n",
            "tensor(0.0426, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 857, loss 0.04264\n",
            "tensor(0.0426, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 858, loss 0.04261\n",
            "tensor(0.0426, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 859, loss 0.04258\n",
            "tensor(0.0425, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 860, loss 0.04255\n",
            "tensor(0.0425, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 861, loss 0.04251\n",
            "tensor(0.0425, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 862, loss 0.04248\n",
            "tensor(0.0425, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 863, loss 0.04245\n",
            "tensor(0.0424, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 864, loss 0.04242\n",
            "tensor(0.0424, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 865, loss 0.04239\n",
            "tensor(0.0424, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 866, loss 0.04236\n",
            "tensor(0.0423, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 867, loss 0.04232\n",
            "tensor(0.0423, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 868, loss 0.04229\n",
            "tensor(0.0423, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 869, loss 0.04226\n",
            "tensor(0.0422, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 870, loss 0.04223\n",
            "tensor(0.0422, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 871, loss 0.0422\n",
            "tensor(0.0422, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 872, loss 0.04217\n",
            "tensor(0.0421, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 873, loss 0.04214\n",
            "tensor(0.0421, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 874, loss 0.04211\n",
            "tensor(0.0421, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 875, loss 0.04208\n",
            "tensor(0.0420, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 876, loss 0.04204\n",
            "tensor(0.0420, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 877, loss 0.04201\n",
            "tensor(0.0420, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 878, loss 0.04198\n",
            "tensor(0.0420, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 879, loss 0.04195\n",
            "tensor(0.0419, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 880, loss 0.04192\n",
            "tensor(0.0419, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 881, loss 0.04189\n",
            "tensor(0.0419, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 882, loss 0.04186\n",
            "tensor(0.0418, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 883, loss 0.04183\n",
            "tensor(0.0418, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 884, loss 0.0418\n",
            "tensor(0.0418, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 885, loss 0.04177\n",
            "tensor(0.0417, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 886, loss 0.04174\n",
            "tensor(0.0417, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 887, loss 0.04171\n",
            "tensor(0.0417, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 888, loss 0.04168\n",
            "tensor(0.0416, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 889, loss 0.04165\n",
            "tensor(0.0416, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 890, loss 0.04162\n",
            "tensor(0.0416, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 891, loss 0.04159\n",
            "tensor(0.0416, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 892, loss 0.04156\n",
            "tensor(0.0415, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 893, loss 0.04153\n",
            "tensor(0.0415, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 894, loss 0.0415\n",
            "tensor(0.0415, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 895, loss 0.04147\n",
            "tensor(0.0414, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 896, loss 0.04144\n",
            "tensor(0.0414, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 897, loss 0.04141\n",
            "tensor(0.0414, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 898, loss 0.04138\n",
            "tensor(0.0414, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 899, loss 0.04135\n",
            "tensor(0.0413, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 900, loss 0.04132\n",
            "tensor(0.0413, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 901, loss 0.04129\n",
            "tensor(0.0413, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 902, loss 0.04127\n",
            "tensor(0.0412, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 903, loss 0.04124\n",
            "tensor(0.0412, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 904, loss 0.04121\n",
            "tensor(0.0412, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 905, loss 0.04118\n",
            "tensor(0.0411, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 906, loss 0.04115\n",
            "tensor(0.0411, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 907, loss 0.04112\n",
            "tensor(0.0411, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 908, loss 0.04109\n",
            "tensor(0.0411, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 909, loss 0.04106\n",
            "tensor(0.0410, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 910, loss 0.04103\n",
            "tensor(0.0410, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 911, loss 0.041\n",
            "tensor(0.0410, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 912, loss 0.04098\n",
            "tensor(0.0409, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 913, loss 0.04095\n",
            "tensor(0.0409, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 914, loss 0.04092\n",
            "tensor(0.0409, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 915, loss 0.04089\n",
            "tensor(0.0409, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 916, loss 0.04086\n",
            "tensor(0.0408, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 917, loss 0.04083\n",
            "tensor(0.0408, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 918, loss 0.0408\n",
            "tensor(0.0408, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 919, loss 0.04078\n",
            "tensor(0.0407, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 920, loss 0.04075\n",
            "tensor(0.0407, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 921, loss 0.04072\n",
            "tensor(0.0407, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 922, loss 0.04069\n",
            "tensor(0.0407, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 923, loss 0.04066\n",
            "tensor(0.0406, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 924, loss 0.04064\n",
            "tensor(0.0406, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 925, loss 0.04061\n",
            "tensor(0.0406, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 926, loss 0.04058\n",
            "tensor(0.0406, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 927, loss 0.04055\n",
            "tensor(0.0405, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 928, loss 0.04052\n",
            "tensor(0.0405, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 929, loss 0.0405\n",
            "tensor(0.0405, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 930, loss 0.04047\n",
            "tensor(0.0404, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 931, loss 0.04044\n",
            "tensor(0.0404, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 932, loss 0.04041\n",
            "tensor(0.0404, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 933, loss 0.04039\n",
            "tensor(0.0404, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 934, loss 0.04036\n",
            "tensor(0.0403, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 935, loss 0.04033\n",
            "tensor(0.0403, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 936, loss 0.0403\n",
            "tensor(0.0403, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 937, loss 0.04028\n",
            "tensor(0.0402, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 938, loss 0.04025\n",
            "tensor(0.0402, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 939, loss 0.04022\n",
            "tensor(0.0402, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 940, loss 0.0402\n",
            "tensor(0.0402, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 941, loss 0.04017\n",
            "tensor(0.0401, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 942, loss 0.04014\n",
            "tensor(0.0401, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 943, loss 0.04011\n",
            "tensor(0.0401, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 944, loss 0.04009\n",
            "tensor(0.0401, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 945, loss 0.04006\n",
            "tensor(0.0400, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 946, loss 0.04003\n",
            "tensor(0.0400, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 947, loss 0.04001\n",
            "tensor(0.0400, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 948, loss 0.03998\n",
            "tensor(0.0400, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 949, loss 0.03995\n",
            "tensor(0.0399, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 950, loss 0.03993\n",
            "tensor(0.0399, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 951, loss 0.0399\n",
            "tensor(0.0399, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 952, loss 0.03987\n",
            "tensor(0.0398, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 953, loss 0.03985\n",
            "tensor(0.0398, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 954, loss 0.03982\n",
            "tensor(0.0398, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 955, loss 0.03979\n",
            "tensor(0.0398, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 956, loss 0.03977\n",
            "tensor(0.0397, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 957, loss 0.03974\n",
            "tensor(0.0397, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 958, loss 0.03971\n",
            "tensor(0.0397, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 959, loss 0.03969\n",
            "tensor(0.0397, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 960, loss 0.03966\n",
            "tensor(0.0396, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 961, loss 0.03964\n",
            "tensor(0.0396, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 962, loss 0.03961\n",
            "tensor(0.0396, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 963, loss 0.03958\n",
            "tensor(0.0396, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 964, loss 0.03956\n",
            "tensor(0.0395, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 965, loss 0.03953\n",
            "tensor(0.0395, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 966, loss 0.03951\n",
            "tensor(0.0395, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 967, loss 0.03948\n",
            "tensor(0.0395, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 968, loss 0.03945\n",
            "tensor(0.0394, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 969, loss 0.03943\n",
            "tensor(0.0394, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 970, loss 0.0394\n",
            "tensor(0.0394, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 971, loss 0.03938\n",
            "tensor(0.0394, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 972, loss 0.03935\n",
            "tensor(0.0393, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 973, loss 0.03933\n",
            "tensor(0.0393, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 974, loss 0.0393\n",
            "tensor(0.0393, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 975, loss 0.03928\n",
            "tensor(0.0393, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 976, loss 0.03925\n",
            "tensor(0.0392, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 977, loss 0.03922\n",
            "tensor(0.0392, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 978, loss 0.0392\n",
            "tensor(0.0392, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 979, loss 0.03917\n",
            "tensor(0.0391, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 980, loss 0.03915\n",
            "tensor(0.0391, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 981, loss 0.03912\n",
            "tensor(0.0391, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 982, loss 0.0391\n",
            "tensor(0.0391, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 983, loss 0.03907\n",
            "tensor(0.0390, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 984, loss 0.03905\n",
            "tensor(0.0390, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 985, loss 0.03902\n",
            "tensor(0.0390, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 986, loss 0.039\n",
            "tensor(0.0390, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 987, loss 0.03897\n",
            "tensor(0.0389, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 988, loss 0.03895\n",
            "tensor(0.0389, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 989, loss 0.03892\n",
            "tensor(0.0389, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 990, loss 0.0389\n",
            "tensor(0.0389, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 991, loss 0.03887\n",
            "tensor(0.0388, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 992, loss 0.03885\n",
            "tensor(0.0388, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 993, loss 0.03882\n",
            "tensor(0.0388, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 994, loss 0.0388\n",
            "tensor(0.0388, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 995, loss 0.03878\n",
            "tensor(0.0388, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 996, loss 0.03875\n",
            "tensor(0.0387, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 997, loss 0.03873\n",
            "tensor(0.0387, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 998, loss 0.0387\n",
            "tensor(0.0387, grad_fn=<BinaryCrossEntropyBackward>)\n",
            "epoch 999, loss 0.03868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMyRqiklxM2f",
        "colab_type": "code",
        "outputId": "9e3c4af0-597e-49c8-da79-a026e06c5595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "plt.title(\"Loss over epochs\",fontsize=16)\n",
        "#plt.plot([e for e in range(epochs)],running_loss)\n",
        "plt.plot(running_loss)\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"Epochs\",fontsize=15)\n",
        "plt.ylabel(\"Training loss\",fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEdCAYAAABzBQKpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nExGRnKLEJyIiOUWJT0REcsr/B6fRFkFQvT6LAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 504x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylt9eCLUOeID",
        "colab_type": "text"
      },
      "source": [
        "### Test\n"
      ]
    }
  ]
}